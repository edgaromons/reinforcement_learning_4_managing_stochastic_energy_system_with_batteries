# -*- coding: utf-8 -*-
"""Managing future high penetration stochastic renewable energy-based power systems with batteries using reinforcement learning method - 7.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1N96L2-HaCPHry2c-OxoK2hYd_aBh16W0

#**Managing future high penetration stochastic renewable energy-based power systems with batteries using reinforcement learning method**

###**Mount the Google Drive**
"""

# Mount the Google Drive
from google.colab import drive
print(drive.mount('/content/gdrive'))

# Check if GPU is connected and display its Details
gpu_info = !nvidia-smi
gpu_info = '\n'.join(gpu_info)
if gpu_info.find('failed') >= 0:
  print('Not connected to a GPU')
else:
  print(gpu_info)

"""###**Install and Import Needed Packages**"""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from IPython.display import display, HTML

import shutil
import sys
import os.path
from matplotlib.lines import Line2D
import copy

if not shutil.which("pyomo"):
    #!pip install -q pyomo
    assert(shutil.which("pyomo"))

if not (shutil.which("cbc") or os.path.isfile("cbc")):
    if "google.colab" in sys.modules:
        #!apt-get install -y -qq coinor-cbc
    else:
        try:
            #!conda install -c conda-forge coincbc
        except:
            pass

assert(shutil.which("cbc") or os.path.isfile("cbc"))
import pyomo.environ as pyo
import pyomo.gdp as gdp
import powerplantmatching as pm

import sys
print("To install it, either uncomment the cell bellow, or type, in a command prompt:\n{}".format(("\t{} -m pip install -m pip install -U git+https://github.com/rte-france/grid2viz --user".format(sys.executable))))
import sys
if "google.colab" in sys.modules:
    #!wget "https://raw.githubusercontent.com/ndcbe/CBE60499/main/notebooks/helper.py"
    import helper
    helper.install_idaes()
    helper.install_ipopt()
from grid2op.Chronics import ChangeNothing
from chronix2grid.GeneratorBackend import GeneratorBackend
import chronix2grid.generation.generation_utils as gu
from chronix2grid.kpi.Generator_parameter_checker import EnergyMix_AprioriChecker
from chronix2grid.kpi.Generator_parameter_checker import Ramps_Pmax_Pmin_APrioriCheckers
from chronix2grid.kpi.Generator_parameter_checker import Aposteriori_renewableCapacityFactor_Checkers
import chronix2grid.constants as cst
from chronix2grid.generation.dispatch.PypsaDispatchBackend import PypsaDispatcher
import chronix2grid.kpi.main as kpis
from chronix2grid.main import create_directory_tree
from chronix2grid import default_backend
import gymnasium as gym
from gymnasium import Env
import gym
from gym import spaces
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import pickle
import torch
import os
import numpy as np
import numpy.random as rd
import pandas as pd
import pyomo.environ as pyo
#import pyomo.kernel as pmo
from omlt import OmltBlock

from gurobipy import *
from omlt.neuralnet import NetworkDefinition, FullSpaceNNFormulation,ReluBigMFormulation
from omlt.io.onnx import write_onnx_model_with_bounds,load_onnx_neural_network_with_bounds
import tempfile
import torch.onnx
import torch.nn as nn
from copy import deepcopy
import wandb
import datetime
import pydantic
import requests
from pathlib import Path
from collections import defaultdict
import powerplantmatching as pm
import cartopy.crs as ccrs
import hvplot
import hvplot.pandas  # noqa
import hvplot.xarray
import holoviews as hv
import geoviews as gv
import geoviews.tile_sources as gvts

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from IPython.display import display, HTML
import shutil
import sys
import os.path
from matplotlib.lines import Line2D
import copy
import pyomo.environ as pyo
import pyomo.gdp as gdp
import powerplantmatching as pm
import sys
from grid2op.Chronics import ChangeNothing
from chronix2grid.GeneratorBackend import GeneratorBackend
import chronix2grid.generation.generation_utils as gu
from chronix2grid.kpi.Generator_parameter_checker import EnergyMix_AprioriChecker
from chronix2grid.kpi.Generator_parameter_checker import Ramps_Pmax_Pmin_APrioriCheckers
from chronix2grid.kpi.Generator_parameter_checker import Aposteriori_renewableCapacityFactor_Checkers
import chronix2grid.constants as cst
from chronix2grid.generation.dispatch.PypsaDispatchBackend import PypsaDispatcher
import chronix2grid.kpi.main as kpis
from chronix2grid.main import create_directory_tree
from chronix2grid import default_backend
import gymnasium as gym
from gymnasium import Env
import gym
from gym import spaces
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import pickle
import torch
import os
import numpy as np
import numpy.random as rd
import pandas as pd
import pyomo.environ as pyo
#import pyomo.kernel as pmo
from omlt import OmltBlock

from gurobipy import *
from omlt.neuralnet import NetworkDefinition, FullSpaceNNFormulation,ReluBigMFormulation
from omlt.io.onnx import write_onnx_model_with_bounds,load_onnx_neural_network_with_bounds
import tempfile
import torch.onnx
import torch.nn as nn
from copy import deepcopy
import wandb
import datetime
import pydantic
import requests
from pathlib import Path
from collections import defaultdict
import powerplantmatching as pm
import cartopy.crs as ccrs
import hvplot
import hvplot.pandas  # noqa
import hvplot.xarray
import holoviews as hv
import geoviews as gv
import geoviews.tile_sources as gvts
from pyomo.core.base.config import default_pyomo_config
from pyomo.core.base.piecewise import Bound
from pyomo.environ import *
from pyomo.opt import SolverFactory
import gurobipy as gp
from gurobipy import GRB
from gurobipy import *
from pyomo.environ import ConcreteModel, Var, Binary, NonNegativeReals
import pickle
import seaborn as sns
from re import escape
from datetime import datetime, timedelta
import math

"""**Create the Plotting Function**"""

#!sudo apt install cm-super dvipng texlive-latex-extra texlive-latex-recommended
import matplotlib.pyplot as plt
import matplotlib
matplotlib.rc('text', usetex=True)
pd.options.display.notebook_repr_html=False
def make_dir(directory, feature_change):
    cwd = f'{directory}/DRL_{feature_change}_plots'
    os.makedirs(cwd, exist_ok=True)
    return cwd
class PlotArgs():
    def __init__(self) -> None:
        self.cwd = None
        self.feature_change = None
        self.plot_on = None
def smooth(data, sm = 5):
        if sm > 1:
            smooth_data = []
            for n, d in enumerate(data):
                if n - sm + 1 >= 0:
                    y = np.mean(data[n - sm + 1: n])
                else:
                    y = np.mean(data[: n])
                smooth_data.append(y)
        return smooth_data

"""#**Optimal Renewable Energy-Based System Scheduling Using A Mixed-Integer Optimization with Constraint Reinforcement Learning Algorithm**

**Highlights:**

Deep reinforcement learning (DRL) algorithm is used to solve the problem of operational scheduling in highly penetrated renewable-based systems.

A DRL algorithm (namely MIP-DQN) is proposed, capable of enforcing all operational constraints in the action space, ensuring the feasibility of the defined schedule in real-time operation.

Recent optimization advances for deep neural networks (DNNs) allow their representation as a mixed-integer linear (MIP) formulation, enabling further consideration of any action space constraints.

##**Data Collection**

By **National Grid Electricity System Operator - ESO**

The ESO is the electricity system operator for Great Britain.

**Historic Generation Mix & Carbon Intensity:**

This dataset contains data from the 1st of Jan 2009. It has seasonal decomposition applied to correct missing or irregular data points. The carbon intensity of electricity is a measure of how much Carbon dioxide emissions are produced per kilowatt hour of electricity consumed.

**Elexon BMRS System Electricity Price.**

#**Case Studies With Real-World U.K. Data**

###**Download the Datasets**

Download the systemSellPrice and	systemBuyPrice by Scraping the Elexon API, Upload other datasets, and Resample to Hourly timesteps.
"""

'''client = api.Client('86zln0egepotid1')
start_date = '2021-01-01'
end_date = '2021-01-31 23:30'
df = client.get_DERSYSDATA(start_date, end_date)
df.to_csv("/content/ESO_Enery_Price_2021_1.csv")
df'''

df_All_2022 = pd.read_csv('/content/gdrive/MyDrive/Energy_System_RL_Modeling_Data/ESO_Historic_Generation_Mix_GB_Demand_System_Prices_2022.csv')
# Create the Solar and Wind Renewable only and its percentage penetration
df_All_2022['SOLAR_AND_WIND_RENEWABLE'] = df_All_2022['SOLAR'] + df_All_2022['WIND']
df_All_2022['SOLAR_AND_WIND_RENEWABLE_perc'] = (df_All_2022['SOLAR_AND_WIND_RENEWABLE']/df_All_2022['GENERATION']) * 100
# Create the Distributed Generation Operational Costs by multiplying the DG values
# with the System Selling price every hour.
df_All_2022['GAS_op_cost'] = df_All_2022['GAS'] * df_All_2022['systemSellPrice']
df_All_2022['COAL_op_cost'] = df_All_2022['COAL'] * df_All_2022['systemSellPrice']
df_All_2022['NUCLEAR_op_cost'] = df_All_2022['NUCLEAR'] * df_All_2022['systemSellPrice']
df_All_2022['HYDRO_op_cost'] = df_All_2022['HYDRO'] * df_All_2022['systemSellPrice']
df_All_2022['BIOMASS_op_cost'] = df_All_2022['BIOMASS'] * df_All_2022['systemSellPrice']
df_All_2022['SOLAR_op_cost'] = df_All_2022['SOLAR'] * df_All_2022['systemSellPrice']
df_All_2022['WIND_op_cost'] = df_All_2022['WIND'] * df_All_2022['systemSellPrice']
df_All_2022['STORAGE_op_cost'] = df_All_2022['STORAGE'] * df_All_2022['systemSellPrice']
df_All_2022['IMPORTS_op_cost'] = df_All_2022['IMPORTS'] * df_All_2022['systemSellPrice']
df_All_2022['OTHER_op_cost'] = df_All_2022['OTHER'] * df_All_2022['systemSellPrice']

df_All_2022['DATETIME'] = pd.to_datetime(df_All_2022['DATETIME'])
df_All_2022.set_index('DATETIME', inplace=True)
df_All_2022['systemSellPrice'].fillna(method='ffill', inplace=True)
df_All_2022['systemSellPrice'].fillna(method='bfill', inplace=True)
df_All_2022['systemBuyPrice'].fillna(method='ffill', inplace=True)
df_All_2022['systemBuyPrice'].fillna(method='bfill', inplace=True)
df_All_2022_hourly = df_All_2022.resample('H').mean()
df_All_2022_hourly.reset_index(inplace=True)
df_All_2022_hourly[['DATETIME', 'systemSellPrice', 'systemBuyPrice']].head(20)

"""**Visualize the Energy Mix**"""

EnergyMix2022 = pd.DataFrame(columns=['Energy_Mix_2022'],
                            index=['solar','wind', 'gas', 'coal', 'biomass', 'nuclear',
                                   'hydro', 'imports', 'storage', 'other'])
EnergyMix2022.at["solar", "Energy_Mix_2022"] = df_All_2022_hourly[['SOLAR']].max()
EnergyMix2022.at["wind", "Energy_Mix_2022"] = df_All_2022_hourly[['WIND']].max()
EnergyMix2022.at["gas", "Energy_Mix_2022"] = df_All_2022_hourly[['GAS']].max()
EnergyMix2022.at["coal", "Energy_Mix_2022"] = df_All_2022_hourly[['COAL']].max()
EnergyMix2022.at["biomass", "Energy_Mix_2022"] = df_All_2022_hourly[['BIOMASS']].max()
EnergyMix2022.at["nuclear", "Energy_Mix_2022"] = df_All_2022_hourly[['NUCLEAR']].max()
EnergyMix2022.at["hydro", "Energy_Mix_2022"] = df_All_2022_hourly[['HYDRO']].max()
EnergyMix2022.at["imports", "Energy_Mix_2022"] = df_All_2022_hourly[['IMPORTS']].max()
EnergyMix2022.at["storage", "Energy_Mix_2022"] = df_All_2022_hourly[['STORAGE']].max()
EnergyMix2022.at["other", "Energy_Mix_2022"] = df_All_2022_hourly[['OTHER']].max()
plt.figure(figsize=(12, 8))
plt.gca().set_facecolor("white")
explode = (0.0, 0.0, 0.0, 0.1, 0.2, 0.0, 0.3, 0.0, 0.2, 0.2)
colors = ['#ff9999', '#66b3ff', '#ffcc99', '#c2c2f0', 'maroon', 'sienna',
          'chocolate', 'purple', 'seagreen', 'darkgray']
plt.pie(EnergyMix2022['Energy_Mix_2022'], labels=EnergyMix2022.index,
        autopct='%1.2f%%', startangle=140, explode=explode,
        textprops={'fontsize': 15, 'fontweight': 'bold'},
        colors=colors, shadow=True, rotatelabels=False)
plt.title("Energy Mix In Great Britain", y=1.1, size=30, fontweight='bold')
plt.axis('equal')
total_capacity_text = 'Total capacity of this Mix is: \n {:.2f} MW'.format(df_All_2022_hourly['GENERATION'].max())
df_wind_solar = df_All_2022['SOLAR_AND_WIND_RENEWABLE_perc'].mean()
percentage_wind_solar = 'Percentage of Wind and Solar in the Mix is: \n {:.2f} %'.format(df_wind_solar)
plt.text(0.0, -1.3, total_capacity_text, ha='center', va='center',
         fontsize=15, color='blue')
plt.text(0.0, -1.6, percentage_wind_solar, ha='center', va='center',
         fontsize=15, color='red')
# Save the plot to a directory in Google Drive
output_dir = '/content/gdrive/MyDrive/Energy_System_RL_Modelling_Plots'
plt.savefig(f'{output_dir}/Energy_Mix_UK.png')
plt.tight_layout()
plt.show()

"""**Visualize the Price and Load Demand Data - 2022**"""

plt.figure(figsize=(15, 9))
plt.gca().set_facecolor("white")
plt.gca().spines[['top', 'right']].set_visible(False)
df_All_2022_hourly['systemSellPrice'].plot(
    kind='line', color='gold', label='Price in £/MWh', linewidth=6.5, linestyle='-.')
df_All_2022_hourly['ND'].plot(
    kind='line', color='maroon', label='Load Demand in MWh', linewidth=2.5)
plt.title('Energy System Selling Price and Load Demand - 2022', size=28, fontweight='bold')
plt.xlabel('Time (Every Hour)', size=25, fontweight='bold')
plt.ylabel('', size=25, fontweight='bold')
plt.xticks(rotation=0, size=15, fontweight='bold')
plt.yticks(rotation=0, ha='right', size=15, fontweight='bold')
#plt.yscale('log')
ax = plt.gca()
# Set linewidth and edge color for each spine
for spine in ax.spines.values():
    spine.set_linewidth(2)
    spine.set_edgecolor('black')
ax.xaxis.grid(False)  # Remove x-axis grid lines
ax.yaxis.grid(False)
plt.legend(loc='best', ncol=1, frameon=True, fontsize='20',
           fancybox=True, framealpha=1, shadow=True, borderpad=2)
# Save the plot to a directory in Google Drive
output_dir = '/content/gdrive/MyDrive/Energy_System_RL_Modelling_Plots'
plt.savefig(f'{output_dir}/Energy_System_Selling_Price_and_Load_Demand_2022.png')
plt.tight_layout()
plt.show()

"""**Visualize Solar and Wind Renewable Data**"""

plt.figure(figsize=(15, 9))
plt.gca().set_facecolor("white")
plt.gca().spines[['top', 'right']].set_visible(False)
df_All_2022_hourly['SOLAR'].plot(
    kind='line', color='blue', label='Eveley Farm Solar', linewidth=3.5,
    linestyle='--')
df_All_2022_hourly['WIND'].plot(
    kind='line', color='red', label='Laceby Solar Farm', linewidth=3.5)
plt.title('Solar and Wind Input Time Series', size=35, fontweight='bold')
plt.xlabel('Every Hour (Year 2022)', size=25, fontweight='bold')
plt.ylabel('Power (MW)', size=25, fontweight='bold')
plt.xticks(rotation=30, size=15, fontweight='bold')
plt.yticks(rotation=0, ha='right', size=15, fontweight='bold')
ax = plt.gca()
# Set linewidth and edge color for each spine
for spine in ax.spines.values():
    spine.set_linewidth(2)
    spine.set_edgecolor('black')
ax.xaxis.grid(False)  # Remove x-axis grid lines
ax.yaxis.grid(True)
plt.legend(loc='best', ncol=1, frameon=True, fontsize='15',
           fancybox=True, framealpha=1, shadow=True, borderpad=2)
# Save the plot to a directory in Google Drive
output_dir = '/content/gdrive/MyDrive/Energy_System_RL_Modelling_Plots'
plt.savefig(f'{output_dir}/Solar_and_Wind_Input_Data_2022.png')
plt.tight_layout()
plt.show()

"""**Visualize Other Energy Sources Data**"""

plt.figure(figsize=(15, 9))
plt.gca().set_facecolor("white")
plt.gca().spines[['top', 'right']].set_visible(False)
df_All_2022_hourly['HYDRO'].plot(
    kind='line', color='gold', label='Hydro (MWh)', linewidth=2.5, linestyle='-.')
df_All_2022_hourly['NUCLEAR'].plot(
    kind='line', color='navy', label='Nuclear (MWh)', linewidth=2.5, linestyle='--')
df_All_2022_hourly['COAL'].plot(
    kind='line', color='maroon', label='Coal (MWh)', linewidth=2.5, linestyle='dashed')
df_All_2022_hourly['GAS'].plot(
    kind='line', color='purple', label='Gas (MWh)', linewidth=2.5, linestyle='dashdot')
df_All_2022_hourly['BIOMASS'].plot(
    kind='line', color='green', label='Biomass (MWh)', linewidth=2.5, linestyle=':')
df_All_2022_hourly['OTHER'].plot(
    kind='line', color='gray', label='Other (MWh)', linewidth=2.5, linestyle='solid')
df_All_2022_hourly['IMPORTS'].plot(
    kind='line', color='chocolate', label='Imports (MWh)', linewidth=2.5, linestyle='dotted')
plt.title('Hydro, Imports, Low and Zero-Carbon, and Fossil Energy Sources - 2022',
          size=28, fontweight='bold')
plt.xlabel('Time (Every Hour)', size=25, fontweight='bold')
plt.ylabel('', size=25, fontweight='bold')
plt.xticks(rotation=0, size=15, fontweight='bold')
plt.yticks(rotation=0, ha='right', size=15, fontweight='bold')
#plt.yscale('log')
ax = plt.gca()
# Set linewidth and edge color for each spine
for spine in ax.spines.values():
    spine.set_linewidth(2)
    spine.set_edgecolor('black')
ax.xaxis.grid(False)  # Remove x-axis grid lines
ax.yaxis.grid(False)
plt.legend(loc='best', ncol=4, frameon=True, fontsize='13',
           fancybox=True, framealpha=1, shadow=True, borderpad=2)
# Save the plot to a directory in Google Drive
output_dir = '/content/gdrive/MyDrive/Energy_System_RL_Modelling_Plots'
plt.savefig(f'{output_dir}/Energy_System_other_sources_2022.png')
plt.tight_layout()
plt.show()

"""**Create a DF of the Generators, Imports, Storage, and Others' Fueltype, Maximum Capacity, and Average Operating Cost.**"""

data_scenario_df = pd.DataFrame(columns=['Name', 'Fueltype', 'Capacity', 'Operating_Cost'],
                            index=['solar','wind', 'gas', 'coal', 'biomass', 'nuclear',
                                   'hydro', 'imports', 'storage', 'other'])
data_scenario_df.at["solar", "Name"] = 'SOLAR GENs OUTPUTS'
data_scenario_df.at["wind", "Name"] = 'WIND GENs OUTPUTS'
data_scenario_df.at["gas", "Name"] = 'GAS GENs OUTPUTS'
data_scenario_df.at["coal", "Name"] = 'COAL GENs OUTPUTS'
data_scenario_df.at["biomass", "Name"] = 'BIOMASS GENs OUTPUTS'
data_scenario_df.at["nuclear", "Name"] = 'NUCLEAR GENs OUTPUTS'
data_scenario_df.at["hydro", "Name"] = 'HYDRO GENs OUTPUTS'
data_scenario_df.at["imports", "Name"] = 'IMPORTS GENs OUTPUTS'
data_scenario_df.at["storage", "Name"] = 'STORAGE SYSs OUTPUTS'
data_scenario_df.at["other", "Name"] = 'OTHER GENs OUTPUTS'
data_scenario_df.at["solar", "Capacity"] = df_All_2022_hourly['SOLAR'].max()
data_scenario_df.at["wind", "Capacity"] = df_All_2022_hourly['WIND'].max()
data_scenario_df.at["gas", "Capacity"] = df_All_2022_hourly['GAS'].max()
data_scenario_df.at["coal", "Capacity"] = df_All_2022_hourly['COAL'].max()
data_scenario_df.at["biomass", "Capacity"] = df_All_2022_hourly['BIOMASS'].max()
data_scenario_df.at["nuclear", "Capacity"] = df_All_2022_hourly['NUCLEAR'].max()
data_scenario_df.at["hydro", "Capacity"] = df_All_2022_hourly['HYDRO'].max()
data_scenario_df.at["imports", "Capacity"] = df_All_2022_hourly['IMPORTS'].max()
data_scenario_df.at["storage", "Capacity"] = df_All_2022_hourly['STORAGE'].max()
data_scenario_df.at["other", "Capacity"] = df_All_2022_hourly['OTHER'].max()
data_scenario_df.at["solar", "Fueltype"] = 'SOLAR'
data_scenario_df.at["wind", "Fueltype"] = 'WIND'
data_scenario_df.at["gas", "Fueltype"] = 'GAS'
data_scenario_df.at["coal", "Fueltype"] = 'COAL'
data_scenario_df.at["biomass", "Fueltype"] = 'BIOMASS'
data_scenario_df.at["nuclear", "Fueltype"] = 'NUCLEAR'
data_scenario_df.at["hydro", "Fueltype"] = 'HYDRO'
data_scenario_df.at["imports", "Fueltype"] = 'IMPORTS'
data_scenario_df.at["storage", "Fueltype"] = 'STORAGE'
data_scenario_df.at["other", "Fueltype"] = 'OTHER'
data_scenario_df.at["solar", "Operating_Cost"] = df_All_2022_hourly['SOLAR_op_cost'].mean()
data_scenario_df.at["wind", "Operating_Cost"] = df_All_2022_hourly['WIND_op_cost'].mean()
data_scenario_df.at["gas", "Operating_Cost"] = df_All_2022_hourly['GAS_op_cost'].mean()
data_scenario_df.at["coal", "Operating_Cost"] = df_All_2022_hourly['COAL_op_cost'].mean()
data_scenario_df.at["biomass", "Operating_Cost"] = df_All_2022_hourly['BIOMASS_op_cost'].mean()
data_scenario_df.at["nuclear", "Operating_Cost"] = df_All_2022_hourly['NUCLEAR_op_cost'].mean()
data_scenario_df.at["hydro", "Operating_Cost"] = df_All_2022_hourly['HYDRO_op_cost'].mean()
data_scenario_df.at["imports", "Operating_Cost"] = df_All_2022_hourly['IMPORTS_op_cost'].mean()
data_scenario_df.at["storage", "Operating_Cost"] = df_All_2022_hourly['STORAGE_op_cost'].mean()
data_scenario_df.at["other", "Operating_Cost"] = df_All_2022_hourly['OTHER_op_cost'].mean()
data_scenario_df

"""**Visualize the Operational Cost of the DGs, Renewable Generators, Storage, Imports, and Others**"""

plt.figure(figsize=(15, 9))
plt.gca().set_facecolor("white")
plt.gca().spines[['top', 'right']].set_visible(False)
df_operating_cost = data_scenario_df[['Fueltype', 'Operating_Cost']]
Operatingcost_plot = sns.barplot(x='Fueltype', y='Operating_Cost', data=df_operating_cost)
Operatingcost_plot.set_xticklabels(Operatingcost_plot.get_xticklabels(), rotation=40,
                                   size=15, fontweight='bold')
Operatingcost_plot.set_xlabel('Plant', size=25, weight='bold')
Operatingcost_plot.set_ylabel('Operating cost per MWh (£)', size=22, weight='bold')
plt.title('Operational Cost of the DGs, Renewable Generators, Storage, Imports, and Others',
          size=28, fontweight='bold')
plt.xticks(size=15, fontweight='bold')
plt.yticks(ha='right', size=15, fontweight='bold')
ax = plt.gca()
# Set linewidth and edge color for each spine
for spine in ax.spines.values():
    spine.set_linewidth(2)
    spine.set_edgecolor('black')
ax.xaxis.grid(False)  # Remove x-axis grid lines
ax.yaxis.grid(False)
#plt.legend(loc='best', ncol=4, frameon=True, fontsize='13',
           #fancybox=True, framealpha=1, shadow=True, borderpad=2)
# Save the plot to a directory in Google Drive
output_dir = '/content/gdrive/MyDrive/Energy_System_RL_Modelling_Plots'
plt.savefig(f'{output_dir}/Energy_System_Operational_Cost.png')
plt.tight_layout()
plt.show()

"""#**Scenario Modelling**

The **Energy Systems Optimal Scheduling problem** can be modeled by Model-Based approaches such that they consider the uncertainties introduced by variable renewable energy sources using a probability distribution function or by leveraging a set of representative scenarios, constructing a complex mathematical formulation considering the system’s operational constraints.

This formulation leads to a stochastic multi-stage mathematical programming model that can be solved using
commercial **mixed-integer nonlinear programming (MINLP) Model**. MINLP is capable of providing good quality solutions, existing model-based
approaches are not adequate for real-time operation due to the large computational time required. Coupled with a large number of distributed generation (DG) units and the increasing level of uncertainty, innovative approaches are urgently needed, capable of providing in realtime good-quality solutions while still enforcing operational and technical constraints.

To overcome this, model-free approaches have been introduced as an alternative solution. The most promising model-free approach is based on the use of reinforcement learning
(RL)

**Solving the mixed-integer nonlinear programming (MINLP) problem With Gurobi Solver**

We model this decision-making problem using mathematical optimization. Three components of a mathematical optimization model are used: the objective function to be optimized, the decision variables, and the constraints. In our power generation problem, the objective is to **minimize the overall costs.** The decision variables model the power generation schedule. The constraints capture basic requirements such as ensuring that the power supply meets the demand, as well as practical limitations such as the minimum and maximum production levels for each power plant. By finding the optimally cost-efficient schedule, this model helps power plant operators get the best output from their facilities while minimizing the overall costs.

##**Data Manager**

**Power Plant Data**
"""

df_plant_info = data_scenario_df[['Name', 'Fueltype', 'Capacity']]
desired_fuels = ['GAS', 'COAL', 'HYDRO', 'NUCLEAR', 'BIOMASS',
                 'STORAGE', 'IMPORTS', 'OTHER']
df_plant_info = df_plant_info[df_plant_info['Fueltype'].isin(desired_fuels)]
P = set(df_plant_info['Name'].unique())# set of all power plants
plant_type = df_plant_info.set_index('Name').Fueltype.to_dict() # plant type for each plant
fuel_type = df_plant_info.set_index('Name').Fueltype.to_dict() # fuel type for each plant
P_NG = set([i for i in P if plant_type[i]=='GAS'])
P_HC = set([i for i in P if plant_type[i]=='COAL'])
P_hyd = set([i for i in P if plant_type[i]=='HYDRO'])
P_Nuc = set([i for i in P if plant_type[i]=='NUCLEAR'])
P_Bio = set([i for i in P if plant_type[i]=='BIOMASS'])
P_W = set([i for i in P if plant_type[i]=='WIND'])
P_S = set([i for i in P if plant_type[i]=='SOLAR'])
P_Stg = set([i for i in P if plant_type[i]=='STORAGE'])
P_Imp = set([i for i in P if plant_type[i]=='IMPORTS'])
P_Oth = set([i for i in P if plant_type[i]=='OTHER'])
fuel_type

"""**Plant Capacities and Limits**

The overall goal is to determine the amount of power to generate from each plant. This amount must be within the plant's minimum and maximum production limits (in MWh). See below for a visualization of the maximum production limits for the plants.
"""

df_plant_info['capacity'] = df_plant_info['Capacity']
c = df_plant_info.set_index('Name').capacity.to_dict() # generation capacity
fig, ax = plt.subplots(figsize=(30, 15))
plt.gca().set_facecolor("white")
plt.gca().spines[['top', 'right']].set_visible(False)
capacity_plot = sns.barplot(x=list(c.keys()), y=[c[k] for k in c])
capacity_plot.set_xticklabels(capacity_plot.get_xticklabels(), rotation=90);
capacity_plot.set(xlabel='Plant', ylabel='Capacity (MWh)');
plt.title('Maximum Production Limits for the Power Plants', size=45, fontweight='bold')
plt.ylabel('Capacity (MWh)', size=40, fontweight='bold')
plt.xlabel('Plant', size=40, fontweight='bold')
plt.xticks(rotation=30, size=22, fontweight='bold')
plt.yticks(rotation=0, ha='right', size=25, fontweight='bold')
ax = plt.gca()
# Set linewidth and edge color for each spine
for spine in ax.spines.values():
    spine.set_linewidth(2)
    spine.set_edgecolor('black')
ax.xaxis.grid(False)  # Remove x-axis grid lines
ax.yaxis.grid(True)
# Save the plot to a directory in Google Drive
output_dir = '/content/gdrive/MyDrive/Energy_System_RL_Modelling_Plots'
plt.savefig(f'{output_dir}/maximum_production_limits_for_the_plants_scenario.png')
plt.tight_layout()
plt.show()

"""**For all the plants, we set the minimum production limit to 1%.**"""

min_p = {i: 0.01 if i in P_NG else 0.01 if i in P_HC else 0.01 if i in P_hyd else 0.01 if i in P_Bio else 0.01 if i in P_Nuc else 0.01 if i in P_Stg else 0.01 if i in P_Imp else 0.01 if i in P_Oth else 1 for i in fuel_type} # min % generation when on
min_p

"""**Ramp Up and Ramp Down**

Additionally, we also have a limit on how much the power generation schedule can **ramp up or ramp down** the power generation between successive hours. This limit ensures that the plants are not forced to make drastic changes to their generation schedule.

We set this speed limit 20% for nuclear plants, and 25% for coal plants. In other words, a coal plant cannot be ramped up or ramped down within an hour by more than 25% of its total capacity. We set the speed limit to be 100% for all other plants thereby imposing no limit on the speed.
"""

r = {i: 1 if i in P_NG else 0.20 if i in P_Nuc else 0.25 if i in P_HC else 1 for i in fuel_type} # ramp up/down speed
r

"""**Operating Cost Data**"""

df_Op_Cost = data_scenario_df[['Name', 'Fueltype', 'Operating_Cost']]
desired_fuels = ['GAS', 'COAL', 'HYDRO', 'NUCLEAR', 'BIOMASS', 'STORAGE',
                 'IMPORTS', 'OTHER']
df_Op_Cost = df_Op_Cost[df_Op_Cost['Fueltype'].isin(desired_fuels)]
df_Op_Cost = df_Op_Cost.set_index('Name').Operating_Cost.to_dict()
df_Op_Cost

class Constant:
	MONTHS_LEN = [31, 28, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31]
	MAX_STEP_HOURS = 24 * 30
class DataManager():
    def __init__(self) -> None:
        self.PV_Generation = []
        self.Wind_Generation = []
        self.Prices = []
        self.Electricity_Consumption = []
    def add_pv_element(self, element):self.PV_Generation.append(element)
    def add_wind_element(self, element):self.Wind_Generation.append(element)
    def add_price_element(self, element):self.Prices.append(element)
    def add_electricity_element(
        self, element):self.Electricity_Consumption.append(element)
    def get_pv_data(self, month, day, day_time):return self.PV_Generation[(
        sum(Constant.MONTHS_LEN[:month-1]) + day-1) * 24 + day_time]
    def get_wind_data(self, month, day, day_time):return self.Wind_Generation[(
        sum(Constant.MONTHS_LEN[:month-1]) + day-1) * 24 + day_time]
    def get_price_data(self, month, day, day_time):return self.Prices[(
        sum(Constant.MONTHS_LEN[:month-1]) + day-1) * 24 + day_time]
    def get_electricity_cons_data(self, month, day, day_time):return self.Electricity_Consumption[(
        sum(Constant.MONTHS_LEN[:month-1]) + day-1) * 24 + day_time]
    def get_series_pv_data(self, month, day):return self.PV_Generation[(
        sum(Constant.MONTHS_LEN[:month-1]) + day-1) * 24:(
            sum(Constant.MONTHS_LEN[:month-1]) + day-1) * 24 + 24]
    def get_series_wind_data(self, month, day):return self.Wind_Generation[(
        sum(Constant.MONTHS_LEN[:month-1]) + day-1) * 24:(
            sum(Constant.MONTHS_LEN[:month-1]) + day-1) * 24 + 24]
    def get_series_price_data(self, month, day):return self.Prices[(
        sum(Constant.MONTHS_LEN[:month-1]) + day-1) * 24:(
            sum(Constant.MONTHS_LEN[:month-1]) + day-1) * 24 + 24]
    def get_series_electricity_cons_data(self, month, day):return self.Electricity_Consumption[(
        sum(Constant.MONTHS_LEN[:month-1]) + day-1) * 24:(
            sum(Constant.MONTHS_LEN[:month-1]) + day-1) * 24 + 24]

"""##**Distributed Generators Module**"""

class DG():
    def __init__(self, parameters):
        self.name = parameters.keys()
        self.DG_Cost = parameters['dg_cost']
        self.power_output_max = parameters['power_output_max']
        self.power_output_min = parameters['power_output_min']
        self.ramping_up = parameters['ramping_up']
        self.ramping_down = parameters['ramping_down']
        self.last_step_output = None
    def step(self, action_gen):
        output_change = action_gen * self.ramping_up # constrain the output_change with ramping up boundary
        output = self.current_output + output_change
        if output > 0:
            output = max(self.power_output_min,
                         min(self.power_output_max, output))# meet the constrain
        else:
            output=0
        self.current_output = output
    def _get_cost(self, output):
        if output <= 0:
            cost = 0
        else:
            cost = self.DG_Cost
        print(cost)
        return cost
    def reset(self):
        self.current_output = 0

"""###**Battery Simulation Module**"""

class Battery():
    '''simulate the battery here'''
    def __init__(self, parameters):
        self.capacity = parameters['capacity']#
        self.max_soc = parameters['max_soc']# max soc 0.8
        self.initial_capacity = parameters['initial_capacity']# initial soc 0.4
        self.min_soc = parameters['min_soc']# 0.2
        self.degradation = parameters['degradation']# degradation cost 0，
        self.max_charge = parameters['max_charge']# max charge ability
        self.max_discharge = parameters['max_discharge']# max discharge ability
        self.efficiency = parameters['efficiency']# charge and discharge efficiency
    def step(self, action_battery):
        energy = action_battery * self.max_charge
        updated_capacity = max(self.min_soc, min(self.max_soc,
         (self.current_capacity * self.capacity + energy) / self.capacity))
        self.energy_change = (updated_capacity-self.current_capacity) * self.capacity# if charge, positive, if discharge, negative
        self.current_capacity = updated_capacity # update capacity to current codition
    def _get_cost(self, energy):# calculate the cost depends on the energy change
        cost = energy**2*self.degradation
        return cost
    def SOC(self):
        return self.current_capacity
    def reset(self):
        self.current_capacity = np.random.uniform(0.2, 0.8)

"""###**Grid Module**"""

class Grid():
    def __init__(self):
        self.on = True
        if self.on:
            self.exchange_ability = 30 # assume that the network’s maximum export
                                       # /import limit is defined as 30 MW
        else:
            self.exchange_ability = 0
    def _get_cost(self, current_price, energy_exchange):#energy if charge, will be positive, if discharge will be negative
        return current_price * energy_exchange
    def retrive_past_price(self):
        result = []
        if self.day < 1:
            past_price = self.past_price # self.past price is fixed as the last days price
        else:
            past_price = self.price[24 * (self.day-1):24 * self.day] # get the price data of previous day
        for item in past_price[(self.time-24)::]:# here if current time_step is 10, then the 10th data of past price is extrated to the result as the  first value
            result.append(item)
        for item in self.price[24*self.day:(24*self.day+self.time)]:# continue to retrive data from the past and attend it to the result. as past price is change everytime.
            result.append(item)
        return result

"""###**The Distributed Generation and Battery Parameters**"""

battery_parameters = {
'capacity':48, # MW
'max_charge':12, # MW
'max_discharge':12, #kw
'efficiency':0.95,
'degradation':0, #euro/kw
'max_soc':0.8,
'min_soc':0.2,
'initial_capacity':0.2}

dg_parameters_RL = {
'GAS GENs OUTPUTS':{'dg_cost':2826882.66
, 'power_output_max':25905.0,'power_output_min':259.05, 'ramping_up':25905.0,'ramping_down':25905.0},
'COAL GENs OUTPUTS':{'dg_cost':127601.88
,'power_output_max':3020.5,'power_output_min':30.205, 'ramping_up':755.125,'ramping_down':755.125},
'BIOMASS GENs OUTPUTS':{'dg_cost':373739.75
,'power_output_max':3170.5,'power_output_min':31.705, 'ramping_up':3170.5,'ramping_down':3170.5},
'NUCLEAR GENs OUTPUTS':{'dg_cost':1011696.42
,'power_output_max':6823.0,'power_output_min':68.23, 'ramping_up':1364.6,'ramping_down':1364.6},
'HYDRO GENs OUTPUTS':{'dg_cost':76704.61
,'power_output_max':1177.5,'power_output_min':11.775, 'ramping_up':1177.5,'ramping_down':1177.5},
'IMPORTS GENs OUTPUTS':{'dg_cost':395243.68
,'power_output_max':6638.0,'power_output_min':66.38, 'ramping_up':6638.0,'ramping_down':6638.0},
'STORAGE SYSs OUTPUTS':{'dg_cost':57432.49
,'power_output_max':2314.0,'power_output_min':23.14, 'ramping_up':2314.0,'ramping_down':2314.0},
'OTHER GENs OUTPUTS':{'dg_cost':71927.85
,'power_output_max':1721.5,'power_output_min':17.215, 'ramping_up':1721.5,'ramping_down':1721.5}}

"""#**Build the Mixed Integer Programming - Deep Q-Network (MIP-DQN)**

**The Energy Storage System (ESS) Environment Module**

The agent learns to charge with low price and then discharge at high price, in this way, it could get benefits.
"""

class ESSEnv(gym.Env):
    def __init__(self, **kwargs):
        super(ESSEnv, self).__init__()
        #parameters
        self.data_manager = DataManager()
        self._load_year_data()
        self.episode_length = kwargs.get('episode_length', 24)
        self.month = None
        self.day = None
        # Control training set and validation set with reset function
        self.TRAIN = True
        self.current_time = None
        self.battery_parameters = kwargs.get('battery_parameters', battery_parameters)
        self.dg_parameters = kwargs.get('dg_parameters', dg_parameters_RL)
        self.penalty_coefficient = 20 #control soft penalty constrain
        self.sell_coefficient = 0.5 # control sell benefits
        # instantiate the components of the environment
        self.grid = Grid()
        # Use Only 1 Battery for now
        self.battery1 = Battery(self.battery_parameters)
        self.dgGAS = DG(self.dg_parameters['GAS GENs OUTPUTS'])
        self.dgCOAL = DG(self.dg_parameters['COAL GENs OUTPUTS'])
        self.dgBIOMASS = DG(self.dg_parameters['BIOMASS GENs OUTPUTS'])
        self.dgNUCLEAR = DG(self.dg_parameters['NUCLEAR GENs OUTPUTS'])
        self.dgHYDRO = DG(self.dg_parameters['HYDRO GENs OUTPUTS'])
        self.dgIMPORTS = DG(self.dg_parameters['IMPORTS GENs OUTPUTS'])
        self.dgSTORAGE = DG(self.dg_parameters['STORAGE SYSs OUTPUTS'])
        self.dgOTHER = DG(self.dg_parameters['OTHER GENs OUTPUTS'])
        # Define Normalized Action Space
        #action space here is [output of gen1, outputof gen2, output of gen3,
        # charge/discharge of battery]
        self.action_space = spaces.Box(low=-1, high=1, shape=(9,), dtype=np.float32)# seems here doesn't used
        # state is [time_step, netload, dg_output_last_step] # this time no prive
        self.state_space = spaces.Box(low=0, high=1, shape=(12,), dtype=np.float32)
        # set state related normalization reference
        self.Length_max = 24
        self.Price_max = max(self.data_manager.Prices)
        # self.Netload_max=max(self.data_manager.Electricity_Consumption)-max(self.data_manager.PV_Generation)
        self.Netload_max = max(self.data_manager.Electricity_Consumption)
        self.SOC_max = self.battery1.max_soc
        self.DGGAS_max = self.dgGAS.power_output_max
        self.DGCOAL_max = self.dgCOAL.power_output_max
        self.DGBIOMASS_max = self.dgBIOMASS.power_output_max
        self.DGNUCLEAR_max = self.dgNUCLEAR.power_output_max
        self.DGHYDRO_max = self.dgHYDRO.power_output_max
        self.DGIMPORTS_max = self.dgIMPORTS.power_output_max
        self.DGSTORAGE_max = self.dgSTORAGE.power_output_max
        self.DGOTHER_max = self.dgOTHER.power_output_max

    def reset(self):
        '''reset is used to initialize the environment, decide the day of month.'''
        '''We Train with the First Day of December, 2022 and Test with the 31st Day of December, 2022'''
        #self.month = np.random.randint(1, 13)# here we choose 12 month
        self.month = 12
        if self.TRAIN:
            #self.day = np.random.randint(1, 21) # Use the first 3 weeks of each Month - Train
            self.day = 1
        else:
            self.day = np.random.randint(21, Constant.MONTHS_LEN[self.month-1]) # Test Data
            self.day = 31
        self.current_time = 0
        self.battery1.reset()
        self.dgGAS.reset()
        self.dgCOAL.reset()
        self.dgBIOMASS.reset()
        self.dgNUCLEAR.reset()
        self.dgHYDRO.reset()
        self.dgIMPORTS.reset()
        self.dgSTORAGE.reset()
        self.dgOTHER.reset()
        return self._build_state()

    def _build_state(self):
        #we put all original information into state and then transfer it into normalized state
        soc = self.battery1.SOC() / self.SOC_max
        dgGAS_output = self.dgGAS.current_output / self.DGGAS_max
        dgCOAL_output = self.dgCOAL.current_output / self.DGCOAL_max
        dgBIOMASS_output = self.dgBIOMASS.current_output / self.DGBIOMASS_max
        dgNUCLEAR_output = self.dgNUCLEAR.current_output / self.DGNUCLEAR_max
        dgHYDRO_output = self.dgHYDRO.current_output / self.DGHYDRO_max
        dgIMPORTS_output = self.dgIMPORTS.current_output / self.DGIMPORTS_max
        dgSTORAGE_output = self.dgSTORAGE.current_output / self.DGSTORAGE_max
        dgOTHER_output = self.dgOTHER.current_output / self.DGOTHER_max
        time_step = self.current_time / (self.Length_max-1)
        electricity_demand = self.data_manager.get_electricity_cons_data(
            self.month, self.day, self.current_time)
        pv_generation = self.data_manager.get_pv_data(self.month, self.day,
                                                      self.current_time)
        wind_generation = self.data_manager.get_wind_data(self.month, self.day,
                                                      self.current_time)
        price = self.data_manager.get_price_data(self.month, self.day,
                                                 self.current_time) / self.Price_max
        #gen_sum = pv_generation + wind_generation
        net_load = (electricity_demand - pv_generation - wind_generation) / self.Netload_max
        obs = np.concatenate((np.float32(time_step), np.float32(price),
                              np.float32(soc), np.float32(net_load),
                              np.float32(dgGAS_output), np.float32(dgCOAL_output),
                              np.float32(dgBIOMASS_output), np.float32(dgNUCLEAR_output),
                              np.float32(dgHYDRO_output), np.float32(dgIMPORTS_output),
                              np.float32(dgSTORAGE_output), np.float32(dgOTHER_output)), axis=None)
        return obs

    def step(self, action):# state transition here current_obs--take_action--get reward-- get_finish--next_obs
        ## here we want to put take action into each components
        current_obs = self._build_state()
        self.battery1.step(action[0])# here execute the state-transition part, battery.current_capacity also changed
        self.dgGAS.step(action[1])
        self.dgCOAL.step(action[2])
        self.dgBIOMASS.step(action[3])
        self.dgNUCLEAR.step(action[4])
        self.dgHYDRO.step(action[5])
        self.dgIMPORTS.step(action[6])
        self.dgSTORAGE.step(action[7])
        self.dgOTHER.step(action[8])
        current_output = np.array((self.dgGAS.current_output, self.dgCOAL.current_output,
                                   self.dgBIOMASS.current_output, self.dgNUCLEAR.current_output,
                                   self.dgHYDRO.current_output, self.dgIMPORTS.current_output,
                                   self.dgSTORAGE.current_output, self.dgOTHER.current_output,
                                   -self.battery1.energy_change))
                                    #truely corresonding to the result
        self.current_output = current_output
        actual_production = sum(current_output)
        # transfer to normal_state
        netload = current_obs[3] * self.Netload_max
        price = current_obs[1] * self.Price_max
        self.netload = netload
        self.price = price
        unbalance = actual_production - netload
        reward = 0
        excess_penalty = 0
        deficient_penalty = 0
        sell_benefit = 0
        buy_cost = 0
        self.excess = 0
        self.shedding = 0
        # logic here is: if unbalance > 0 then it is production excess, so the
        # excessed output should be sold to power grid to get benefits
        if unbalance >= 0: # it is now in excess condition
            if unbalance <= self.grid.exchange_ability:
                sell_benefit = self.grid._get_cost(
                    price, unbalance) * self.sell_coefficient
                    #sell money to grid is little [0.029, 0.1]
            else:
                sell_benefit = self.grid._get_cost(
                    price, self.grid.exchange_ability) * self.sell_coefficient
                #real unbalance that even grid could not meet
                self.excess = unbalance-self.grid.exchange_ability
                excess_penalty = self.excess * self.penalty_coefficient
        else:# unbalance <0, its load shedding model, in this case, deficient penalty is used
            if abs(unbalance) <= self.grid.exchange_ability:
                buy_cost = self.grid._get_cost(price, abs(unbalance))
            else:
                buy_cost = self.grid._get_cost(price, self.grid.exchange_ability)
                self.shedding = abs(unbalance)-self.grid.exchange_ability
                deficient_penalty = self.shedding * self.penalty_coefficient
        battery1_cost = self.battery1._get_cost(self.battery1.energy_change)# we set it as 0 this time
        #battery2_cost = self.battery2._get_cost(self.battery2.energy_change)
        #battery3_cost = self.battery3._get_cost(self.battery3.energy_change)
        dgGAS_cost = self.dgGAS._get_cost(self.dgGAS.current_output)
        dgCOAL_cost = self.dgCOAL._get_cost(self.dgCOAL.current_output)
        dgBIOMASS_cost = self.dgBIOMASS._get_cost(self.dgBIOMASS.current_output)
        dgNUCLEAR_cost = self.dgNUCLEAR._get_cost(self.dgNUCLEAR.current_output)
        dgHYDRO_cost = self.dgHYDRO._get_cost(self.dgHYDRO.current_output)
        dgIMPORTS_cost = self.dgIMPORTS._get_cost(self.dgIMPORTS.current_output)
        dgSTORAGE_cost = self.dgSTORAGE._get_cost(self.dgSTORAGE.current_output)
        dgOTHER_cost = self.dgOTHER._get_cost(self.dgOTHER.current_output)

        reward = -(
            battery1_cost + dgGAS_cost + dgCOAL_cost + dgBIOMASS_cost + dgNUCLEAR_cost + dgHYDRO_cost +\
            dgIMPORTS_cost + dgSTORAGE_cost + dgOTHER_cost +\
            excess_penalty + deficient_penalty - sell_benefit + buy_cost) / 2e3

        self.operation_cost = battery1_cost + dgGAS_cost + dgCOAL_cost + dgBIOMASS_cost +\
        dgNUCLEAR_cost + dgHYDRO_cost + dgIMPORTS_cost + dgSTORAGE_cost + dgOTHER_cost + \
        buy_cost - sell_benefit + (self.shedding + self.excess) * self.penalty_coefficient

        self.unbalance = unbalance
        self.real_unbalance = self.shedding + self.excess
        '''here we also need to store the final step outputs for the final steps
         including, soc, output of units for seeing the final states'''
        final_step_outputs = [self.dgGAS.current_output, self.dgCOAL.current_output,
                              self.dgBIOMASS.current_output, self.dgNUCLEAR.current_output,
                              self.dgHYDRO.current_output, self.dgIMPORTS.current_output,
                              self.dgSTORAGE.current_output, self.dgOTHER.current_output,
                              self.battery1.current_capacity]
        self.current_time += 1
        finish = (self.current_time==self.episode_length)
        if finish:
            self.final_step_outputs = final_step_outputs
            self.current_time = 0
            next_obs = self.reset()
        else:
            next_obs = self._build_state()
        return current_obs, next_obs, float(reward), finish

    def render(self, current_obs, next_obs, reward, finish):
        print('day={}, hour={:2d}, state={}, next_state={}, reward={:.4f}, terminal={}\n'.format(
            self.day, self.current_time, current_obs, next_obs, reward, finish))

    def _load_year_data(self):
        '''this private function is used to load the electricity consumption,
        Renewable generation and related prices in a year as one hour resolution,
        with the cooperation of class DataProcesser and then all these data are
        stored in data processor'''
        pv_df = df_All_2022_hourly[['SOLAR']]
        pv_df['Datetime'] = pd.date_range(start='2022-01-01', periods=len(pv_df), freq='H')
        pv_df.set_index('Datetime', inplace=True)
        wind_df = df_All_2022_hourly[['WIND']]
        wind_df['Datetime'] = pd.date_range(start='2022-01-01', periods=len(wind_df), freq='H')
        wind_df.set_index('Datetime', inplace=True)
        price_df = df_All_2022_hourly[['systemSellPrice']]
        price_df['Datetime'] = pd.date_range(start='2022-01-01', periods=len(price_df), freq='H')
        price_df.set_index('Datetime', inplace=True)
        electricity_df = df_All_2022_hourly[['ND']]
        pv_data = pv_df['SOLAR'].to_numpy(dtype=float)
        wind_data = wind_df['WIND'].to_numpy(dtype=float)
        price = price_df['systemSellPrice'].to_numpy(dtype=float)
        electricity = electricity_df['ND'].to_numpy(dtype=float)
        # netload=electricity-pv_data
        for element in pv_data:
            self.data_manager.add_pv_element(element)
        for element in wind_data:
            self.data_manager.add_wind_element(element)
        for element in price:
            self.data_manager.add_price_element(element)
        for element in electricity:
            self.data_manager.add_electricity_element(element)
        return pv_data, wind_data, price, electricity

"""**Create the Replay Buffer Module**"""

class ReplayBuffer:
    def __init__(self, max_len, state_dim, action_dim, gpu_id=0):
        self.now_len = 0
        self.next_idx = 0
        self.if_full = False
        self.max_len = max_len
        self.data_type = torch.float32
        self.action_dim = action_dim
        self.device = torch.device(f"cuda:{gpu_id}" if (
            torch.cuda.is_available() and (gpu_id >= 0)) else "cpu")
        other_dim = 1 + 1 + self.action_dim
        self.buf_other = torch.empty(size=(max_len, other_dim),
                                     dtype=self.data_type, device=self.device)
        if isinstance(state_dim, int):  # state is pixel
            self.buf_state = torch.empty((max_len, state_dim),
                                         dtype=torch.float32, device=self.device)
        elif isinstance(state_dim, tuple):
            self.buf_state = torch.empty((max_len, *state_dim), dtype=torch.uint8,
                                         device=self.device)
        else:
            raise ValueError('state_dim')
    def extend_buffer(self, state, other):  # CPU array to CPU array
        size = len(other)
        next_idx = self.next_idx + size
        if next_idx > self.max_len:
            self.buf_state[self.next_idx:self.max_len] = state[:self.max_len - self.next_idx]
            self.buf_other[self.next_idx:self.max_len] = other[:self.max_len - self.next_idx]
            self.if_full = True
            next_idx = next_idx - self.max_len
            self.buf_state[0:next_idx] = state[-next_idx:]
            self.buf_other[0:next_idx] = other[-next_idx:]
        else:
            self.buf_state[self.next_idx:next_idx] = state
            self.buf_other[self.next_idx:next_idx] = other
        self.next_idx = next_idx
    def sample_batch(self, batch_size) -> tuple:
        indices = rd.randint(self.now_len - 1, size=batch_size)
        r_m_a = self.buf_other[indices]
        return (r_m_a[:, 0:1],
                r_m_a[:, 1:2],
                r_m_a[:, 2:],
                self.buf_state[indices],
                self.buf_state[indices + 1])
    def update_now_len(self):
        self.now_len = self.max_len if self.if_full else self.next_idx

"""**Define the Arguments for the environment, training, evaluation, etc**"""

class Arguments:
    def __init__(self, agent=None, env_RL=None):
        self.agent = agent  # Deep Reinforcement Learning algorithm
        self.env_RL = env_RL  # the environment for training
        self.cwd = None  # current work directory. None means set automatically
        self.if_remove = False  # remove the cwd folder? (True, False, None:ask me)
        self.visible_gpu = '0'  # for example: os.environ['CUDA_VISIBLE_DEVICES'] = '0, 2,'
        self.worker_num = 2  # rollout workers number pre GPU (adjust it to get high GPU usage)
        self.num_threads = 8  # cpu_num for evaluate model, torch.set_num_threads(self.num_threads)
        self._if_per_or_gae = False
        '''Arguments for training'''
        self.num_episode = 200 #3000 #2000
        self.gamma = 0.995  # discount factor of future rewards
        self.learning_rate = 2 ** -14 #~= 6e-5
        self.soft_update_tau = 2 ** -8 #~= 5e-3
        self.net_dim = 256  # the network width 256
        self.batch_size = 4096  # num of transitions sampled from replay buffer.
        self.repeat_times = 2 ** 3  # repeatedly update network to keep critic's loss small
        self.target_step = 4096 #1000 # collect target_step experiences , then update network, 1024
        self.max_memo = 5000000  # capacity of replay buffer
        ## arguments for controlling exploration
        self.explorate_decay = 0.99
        self.explorate_min = 0.3
        '''Arguments for evaluate'''
        #self.random_seed_list = [1234, 2234, 3234, 4234, 5234]
        self.random_seed_list = [2234]
        self.run_name = 'MIP_DQN_experiments'
        '''Arguments for save and Plot'''
        self.train = True
        self.update_training_data = True
        self.save_network = True
        self.test_network=True
        self.save_test_data=True
        self.compare_with_pyomo=True
        self.plot_on=True
    def init_before_training(self, if_main):
        if self.cwd is None:
            agent_name = self.agent.__class__.__name__
            self.cwd = f'./{agent_name}/{self.run_name}'
        if if_main:
            import shutil  # remove history according to bool(if_remove)
            if self.if_remove is None:
                self.if_remove = bool(input(f"| PRESS 'y' to REMOVE: {self.cwd}? ") == 'y')
            elif self.if_remove:
                shutil.rmtree(self.cwd, ignore_errors=True)
                print(f"| Remove cwd: {self.cwd}")
            os.makedirs(self.cwd, exist_ok=True)
        np.random.seed(self.random_seed)
        torch.manual_seed(self.random_seed)
        torch.set_num_threads(self.num_threads)
        torch.set_default_dtype(torch.float32)
        os.environ['CUDA_VISIBLE_DEVICES'] = str(self.visible_gpu)# control how many GPU is used
def test_one_episode(env_RL, act, device):
    '''to get evaluate information, here record the unbalance of after taking
    action'''
    record_state = []
    record_action = []
    record_reward = []
    record_output = []
    record_cost = []
    record_unbalance = []
    record_system_info = []# [time price, netload,action,real action, output*12,soc,unbalance(exchange+penalty)]
    record_init_info = []#should include month,day,time,intial soc,initial
    env_RL.TRAIN = False
    state = env_RL.reset()
    record_init_info.append([env_RL.month, env_RL.day, env_RL.current_time, env_RL.battery1.current_capacity])
    print(f'current testing month is {env_RL.month}, day is {env_RL.day}, initial_soc is {env_RL.battery1.current_capacity}' )
    for i in range(24):
        s_tensor = torch.as_tensor((state,), device=device)
        a_tensor = act(s_tensor)
        action = a_tensor.detach().cpu().numpy()[0]  # not need detach(), because with torch.no_grad() outside
        real_action = action
        state, next_state, reward, done = env_RL.step(action)
        record_system_info.append([state[0], env_RL.price, env_RL.netload, action, real_action,
                                   env_RL.battery1.SOC(), env_RL.battery1.energy_change,
                                   #env.battery2.SOC(), env.battery2.energy_change,
                                   #env.battery3.SOC(), env.battery3.energy_change,
                                   next_state[4], next_state[5], next_state[6],
                                   next_state[7], next_state[8], next_state[9],
                                   next_state[10], next_state[11],
                                   env_RL.unbalance, env_RL.operation_cost])
        record_state.append(state)
        record_action.append(real_action)
        record_reward.append(reward)
        record_output.append(env_RL.current_output)
        record_unbalance.append(env_RL.unbalance)
        state = next_state
    record_system_info[-1][7:15] = [env_RL.final_step_outputs[0], env_RL.final_step_outputs[1],
                                  env_RL.final_step_outputs[2], env_RL.final_step_outputs[3],
                                     env_RL.final_step_outputs[4], env_RL.final_step_outputs[5],
                                     env_RL.final_step_outputs[6], env_RL.final_step_outputs[7]]
    ## add information of last step soc
    record_system_info[-1][5] = env_RL.final_step_outputs[8]
    record = {'init_info':record_init_info, 'information':record_system_info,
              'state':record_state, 'action':record_action, 'reward':record_reward,
              'cost':record_cost, 'unbalance':record_unbalance,
              'record_output':record_output}
    return record

"""**Build the Actor and Critic Modules for DQN**"""

class Actor(nn.Module):
    def __init__(self, mid_dim, state_dim, action_dim):
        super().__init__()
        self.net = nn.Sequential(nn.Linear(state_dim, mid_dim), nn.ReLU(),
                               nn.Linear(mid_dim, mid_dim), nn.ReLU(),
                               nn.Linear(mid_dim, mid_dim), nn.ReLU(),
                               nn.Linear(mid_dim, action_dim))
    def forward(self, state):
        return self.net(state).tanh()# make the data from -1 to 1
    def get_action(self, state, action_std):#
        action = self.net(state).tanh()
        noise = (torch.randn_like(action)*action_std).clamp(-0.5, 0.5)#
        return (action + noise).clamp(-1.0, 1.0)
class CriticQ(nn.Module):
    def __init__(self, mid_dim, state_dim, action_dim):
        super().__init__()
        self.net_head = nn.Sequential(nn.Linear(state_dim + action_dim, mid_dim), nn.ReLU(),
                                    nn.Linear(mid_dim, mid_dim), nn.ReLU())
        self.net_q1 = nn.Sequential(nn.Linear(mid_dim, mid_dim), nn.ReLU(),
                                  nn.Linear(mid_dim, 1))# we get q1 value
        self.net_q2 = nn.Sequential(nn.Linear(mid_dim, mid_dim), nn.ReLU(),
                                  nn.Linear(mid_dim, 1))# we get q2 value
    def forward(self, value):
        mid = self.net_head(value)
        return self.net_q1(mid)
    def get_q1_q2(self, value):
        mid = self.net_head(value)
        return self.net_q1(mid), self.net_q2(mid)

"""**Build the Agent's Base to Select Actions and Explore the Environment for DQN**"""

class AgentBase:
    def __init__(self):
        self.state = None
        self.device = None
        self.action_dim = None
        self.if_off_policy = None
        self.explore_noise = None
        self.trajectory_list = None
        self.explore_rate = 1.0
        self.criterion = torch.nn.SmoothL1Loss()
    def init(self, net_dim, state_dim, action_dim, learning_rate = 1e-4,
             _if_per_or_gae = False, gpu_id = 0):
        self.device = torch.device(
            f"cuda:{gpu_id}" if (torch.cuda.is_available() and (gpu_id >= 0)) else "cpu")
        self.action_dim = action_dim
        self.cri = self.ClassCri(net_dim, state_dim, action_dim).to(self.device)
        self.act = self.ClassAct(net_dim, state_dim, action_dim).to(
            self.device) if self.ClassAct else self.cri
        self.cri_target = deepcopy(self.cri) if self.if_use_cri_target else self.cri
        self.act_target = deepcopy(self.act) if self.if_use_act_target else self.act
        self.cri_optim = torch.optim.Adam(self.cri.parameters(), learning_rate)
        self.act_optim = torch.optim.Adam(self.act.parameters(),
                                          learning_rate) if self.ClassAct else self.cri
        del self.ClassCri, self.ClassAct
    def select_action(self, state) -> np.ndarray:
        states = torch.as_tensor((state,), dtype=torch.float32, device=self.device)
        action = self.act(states)[0]
        if rd.rand()<self.explore_rate:
            action = (action + torch.randn_like(action) * self.explore_noise).clamp(-1, 1)
        return action.detach().cpu().numpy()
    def explore_env(self, env_RL, target_step):
        trajectory = list()
        state = self.state
        for _ in range(target_step):
            action = self.select_action(state)
            state, next_state, reward, done, = env_RL.step(action)
            trajectory.append((state, (reward, done, *action)))
            state = env_RL.reset() if done else next_state
        self.state = state
        return trajectory
    @staticmethod
    def optim_update(optimizer, objective):
        optimizer.zero_grad()
        objective.backward()
        optimizer.step()
    @staticmethod
    def soft_update(target_net, current_net, tau):
        for tar, cur in zip(target_net.parameters(), current_net.parameters()):
            tar.data.copy_(cur.data * tau + tar.data * (1.0 - tau))
    def save_or_load_agent(self, cwd, if_save):
        def load_torch_file(model_or_optim, _path):
            state_dict = torch.load(_path, map_location=lambda storage, loc: storage)
            model_or_optim.load_state_dict(state_dict)
        name_obj_list = [('actor', self.act), ('act_target', self.act_target),
         ('act_optim', self.act_optim), ('critic', self.cri),
          ('cri_target', self.cri_target), ('cri_optim', self.cri_optim), ]
        name_obj_list = [(name, obj) for name, obj in name_obj_list if obj is not None]
        if if_save:
            for name, obj in name_obj_list:
                save_path = f"{cwd}/{name}.pth"
                torch.save(obj.state_dict(), save_path)
        else:
            for name, obj in name_obj_list:
                save_path = f"{cwd}/{name}.pth"
                load_torch_file(obj, save_path) if os.path.isfile(save_path) else None
    def _update_exploration_rate(self, explorate_decay, explore_rate_min):
        self.explore_rate = max(self.explore_rate * explorate_decay, explore_rate_min)
        '''this function is used to update the explorate probability when select action'''

"""**Build the Agent's MIP-DQN Module**"""

class AgentMIPDQN(AgentBase):
    def __init__(self):
        super().__init__()
        self.explore_noise = 0.5  # standard deviation of exploration noise
        self.policy_noise = 0.2  # standard deviation of policy noise
        self.update_freq = 2  # delay update frequency
        self.if_use_cri_target = self.if_use_act_target = True
        self.ClassCri = CriticQ
        self.ClassAct = Actor
    def update_net(self, buffer, batch_size, repeat_times, soft_update_tau) -> tuple:
        buffer.update_now_len()
        obj_critic = obj_actor = None
        for update_c in range(int(buffer.now_len / batch_size * repeat_times)):# we update too much time?
            obj_critic, state = self.get_obj_critic(buffer, batch_size)
            self.optim_update(self.cri_optim, obj_critic)
            action_pg = self.act(state)  # policy gradient
            obj_actor = -self.cri_target(torch.cat(
                (state, action_pg), dim=-1)).mean()  # use cri_target instead of cri for stable training
            self.optim_update(self.act_optim, obj_actor)
            if update_c % self.update_freq == 0:  # delay update
                self.soft_update(self.cri_target, self.cri, soft_update_tau)
                self.soft_update(self.act_target, self.act, soft_update_tau)
        return obj_critic.item() / 2, obj_actor.item()
    def get_obj_critic(self, buffer, batch_size):
        with torch.no_grad():
            reward, mask, action, state, next_s = buffer.sample_batch(batch_size)
            next_a = self.act_target.get_action(next_s, self.policy_noise)  # policy noise,
            next_q = torch.min(*self.cri_target.get_q1_q2(
                torch.cat((next_s, next_a), dim=-1)))  # twin critics
            q_label = reward + mask * next_q
        q1, q2 = self.cri.get_q1_q2(torch.cat((state, action), dim=-1))
        obj_critic = self.criterion(q1, q_label) + self.criterion(q2, q_label)  # twin critics
        return obj_critic, state
def update_buffer(_trajectory):
    ten_state = torch.as_tensor([item[0] for item in _trajectory],
                                dtype=torch.float32)
    ary_other = torch.as_tensor([item[1] for item in _trajectory])
    ary_other[:, 0] = ary_other[:, 0]   # ten_reward
    ary_other[:, 1] = (1.0 - ary_other[:, 1]) * gamma  # ten_mask = (1.0 - ary_done) * gamma
    buffer.extend_buffer(ten_state, ary_other)
    _steps = ten_state.shape[0]
    _r_exp = ary_other[:, 0].mean()  # other = (reward, mask, action)
    return _steps, _r_exp
def get_episode_return(env_RL, act, device):
    '''get information of one episode during the training'''
    episode_return = 0.0  # sum of rewards in an episode
    episode_unbalance = 0.0
    episode_operation_cost = 0.0
    state = env_RL.reset()
    for i in range(24):
        s_tensor = torch.as_tensor((state,), device=device)
        a_tensor = act(s_tensor)
        action = a_tensor.detach().cpu().numpy()[0]  # not need detach(), because with torch.no_grad() outside
        state, next_state, reward, done,= env_RL.step(action)
        state = next_state
        episode_return += reward
        episode_unbalance += env_RL.real_unbalance
        episode_operation_cost += env_RL.operation_cost
        if done:
            break
    return episode_return, episode_unbalance, episode_operation_cost

"""**Build the Actor MIP Module to get the Best Action and Q Function**"""

class Actor_MIP:
    '''this actor is used to get the best action and Q function, the only input
    should be batch tensor state, action, and network, while the output should be
    batch tensor max_action, batch tensor max_Q'''
    def __init__(self, scaled_parameters, batch_size, net, state_dim, action_dim,
                 env_RL, constrain_on = False):
        self.batch_size = batch_size
        self.net = net
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.env_RL = env_RL
        self.constrain_on = constrain_on
        self.scaled_parameters = scaled_parameters
    def get_input_bounds(self, input_batch_state):
        batch_size = self.batch_size
        batch_input_bounds = []
        lbs_states = input_batch_state.detach().numpy()
        ubs_states = lbs_states
        for i in range(batch_size):
            input_bounds = {}
            for j in range(self.action_dim + self.state_dim):
                if j < self.state_dim:
                    input_bounds[j] = (float(lbs_states[i][j]), float(ubs_states[i][j]))
                else:
                    input_bounds[j] = (float(-1), float(1))
            batch_input_bounds.append(input_bounds)
        return batch_input_bounds
    def predict_best_action(self, state):
        state = state.detach().cpu().numpy()
        v1 = torch.zeros((1, self.state_dim + self.action_dim), dtype=torch.float32)
        '''this function is used to get the best action based on current net'''
        # Move the model to either the available CUDA device or the CPU
        model = self.net.to('cuda' if torch.cuda.is_available() else 'cpu')
        #model = self.net.to('cpu')
        input_bounds = {}
        lb_state = state
        ub_state = state
        for i in range(self.action_dim + self.state_dim):
            if i < self.state_dim:
                input_bounds[i] = (float(lb_state[0][i]), float(ub_state[0][i]))
            else:
                input_bounds[i] = (float(-1), float(1))
        with tempfile.NamedTemporaryFile(suffix='.onnx', delete=False) as f:
            # export neural network to ONNX
            torch.onnx.export(
                model,
                v1,
                f,
                input_names = ['state_action'],
                output_names = ['Q_value'],
                dynamic_axes = {
                    'state_action': {0: 'batch_size'},
                    'Q_value': {0: 'batch_size'}
                }
            )
            # write ONNX model and its bounds using OMLT
        write_onnx_model_with_bounds(f.name, None, input_bounds)
        # load the network definition from the ONNX model
        network_definition = load_onnx_neural_network_with_bounds(f.name)
        # global optimality
        formulation = ReluBigMFormulation(network_definition)
        m = pyo.ConcreteModel()
        m.nn = OmltBlock()
        m.nn.build_formulation(formulation)
        '''# we are now building the surrogate model between action and state'''
        # constrain for battery，
        if self.constrain_on:
            m.power_balance_con1 = pyo.Constraint(expr=(
                    (-m.nn.inputs[7] * self.scaled_parameters[0])+\
                    ((m.nn.inputs[8] * self.scaled_parameters[1])+m.nn.inputs[4]*self.scaled_parameters[5]) +\
                    ((m.nn.inputs[9] * self.scaled_parameters[2])+m.nn.inputs[5]*self.scaled_parameters[6]) +\
                    ((m.nn.inputs[10] * self.scaled_parameters[3])+m.nn.inputs[6]*self.scaled_parameters[7])>=\
                    m.nn.inputs[3] *self.scaled_parameters[4]-self.env.grid.exchange_ability))
            m.power_balance_con2 = pyo.Constraint(expr=(
                    (-m.nn.inputs[7] * self.scaled_parameters[0])+\
                    (m.nn.inputs[8] * self.scaled_parameters[1]+m.nn.inputs[4]*self.scaled_parameters[5]) +\
                    (m.nn.inputs[9] * self.scaled_parameters[2]+m.nn.inputs[5]*self.scaled_parameters[6]) +\
                    (m.nn.inputs[10] * self.scaled_parameters[3]+m.nn.inputs[6]*self.scaled_parameters[7])<=\
                    m.nn.inputs[3] *self.scaled_parameters[4]+self.env.grid.exchange_ability))
        m.obj = pyo.Objective(expr=(m.nn.outputs[0]), sense=pyo.maximize)
        pyo.SolverFactory('gurobi').solve(m, tee=False)
        best_input = pyo.value(m.nn.inputs[:])
        best_action = (best_input[self.state_dim::])
        return best_action

"""**Test the MIP-DQN Function**"""

if __name__ == '__main__':
    args = Arguments()
    '''here record real unbalance'''
    reward_record = {'episode': [], 'steps': [], 'mean_episode_reward': [], 'unbalance': [],
                     'episode_operation_cost': []}
    loss_record = {'episode': [], 'steps': [], 'critic_loss': [], 'actor_loss': [], 'entropy_loss': []}
    args.visible_gpu = '0'
    args.save_network=True
    args.test_network= True
    args.save_test_data=True
    args.compare_with_pyomo=True
    for seed in args.random_seed_list:
        args.random_seed = seed
        # set different seed
        args.agent = AgentMIPDQN()
        agent_name = f'{args.agent.__class__.__name__}'
        args.agent.cri_target = True
        args.env_RL = ESSEnv()
        args.init_before_training(if_main=True)
        '''init agent and environment'''
        agent = args.agent
        env_RL = args.env_RL
        agent.init(args.net_dim, env_RL.state_space.shape[0], env_RL.action_space.shape[0],
                   args.learning_rate, args._if_per_or_gae)
        '''init replay buffer'''
        buffer = ReplayBuffer(max_len=args.max_memo, state_dim=env_RL.state_space.shape[0],
                              action_dim=env_RL.action_space.shape[0])
        '''start training'''
        cwd = args.cwd
        gamma = args.gamma
        batch_size = args.batch_size  # how much data should be used to update net
        target_step = args.target_step  # how manysteps of one episode should stop
        repeat_times = args.repeat_times  # how many times should update for one batch size data
        soft_update_tau = args.soft_update_tau
        agent.state = env_RL.reset()
        '''collect data and train and update network'''
        num_episode = args.num_episode
        args.train=True
        args.save_network=True
        args.test_network= True
        args.save_test_data=True
        args.compare_with_pyomo=True
        if args.train:
            collect_data = True
            while collect_data:
                print(f'buffer:{buffer.now_len}')
                with torch.no_grad():
                    trajectory = agent.explore_env(env_RL, target_step)
                    steps, r_exp = update_buffer(trajectory)
                    buffer.update_now_len()
                if buffer.now_len >= 10000:
                    collect_data = False
            for i_episode in range(num_episode):
                critic_loss, actor_loss = agent.update_net(
                    buffer, batch_size, repeat_times, soft_update_tau)
                loss_record['critic_loss'].append(critic_loss)
                loss_record['actor_loss'].append(actor_loss)
                with torch.no_grad():
                    episode_reward, episode_unbalance, episode_operation_cost = get_episode_return(
                        env_RL, agent.act, agent.device)
                    reward_record['mean_episode_reward'].append(episode_reward)
                    reward_record['unbalance'].append(episode_unbalance)
                    reward_record['episode_operation_cost'].append(episode_operation_cost)
                print(
                    f'current episode is {i_episode}, reward:{episode_reward}, unbalance:{episode_unbalance}, buffer_length: {buffer.now_len}')
                if i_episode % 10 == 0:
                    # target_step
                    with torch.no_grad():
                        agent._update_exploration_rate(args.explorate_decay, args.explorate_min)
                        trajectory = agent.explore_env(env_RL, target_step)
                        steps, r_exp = update_buffer(trajectory)
    if args.update_training_data:
        loss_record_path = f'{args.cwd}/loss_data.pkl'
        reward_record_path = f'{args.cwd}/reward_data.pkl'
        with open(loss_record_path, 'wb') as tf:
            pickle.dump(loss_record, tf)
        with open(reward_record_path, 'wb') as tf:
            pickle.dump(reward_record, tf)
    act_save_path = f'{args.cwd}/actor.pth'
    cri_save_path = f'{args.cwd}/critic.pth'
    print('training data have been saved')
    if args.save_network:
        torch.save(agent.act.state_dict(), act_save_path)
        torch.save(agent.cri.state_dict(), cri_save_path)
        print('training finished and actor and critic parameters have been saved')
    if args.test_network:
        args.cwd = agent_name
        agent.act.load_state_dict(torch.load(act_save_path))
        print('parameters have been reloaded and testing')
        record = test_one_episode(env_RL, agent.act, agent.device)
        eval_data = pd.DataFrame(record['information'])
        eval_data.columns = ['time_step', 'price', 'netload', 'action', 'real_action',
                             'soc', 'battery1', 'GAS', 'COAL', 'BIOMASS', 'NUCLEAR', 'HYDRO',
                             'IMPORTS', 'STORAGE', 'OTHER', 'unbalance', 'operation_cost']
    if args.save_test_data:
        test_data_save_path = f'{args.cwd}/test_data.pkl'
        with open(test_data_save_path, 'wb') as tf:
            pickle.dump(record, tf)
    '''compare with pyomo data and results'''
    if args.compare_with_pyomo:
        month = record['init_info'][0][0]
        day = record['init_info'][0][1]
        initial_soc = record['init_info'][0][3]
        print(initial_soc)

"""**Print Month, Day, and Initial SOC**"""

print(f'Month is: {month}, Day is:{day}, Initial SOC is:{initial_soc}')

"""**Visualize Historical Wind, Solar, and Price Data for Modelling Using MINLP**"""

month = month
day = day
data_manager = DataManager()
def _load_year_data():
    '''this private function is used to load the electricity consumption,
    Renewable generation and related prices in a year as one hour resolution,
    with the cooperation of class DataProcesser and then all these data are
    stored in data processor'''
    pv_df = df_All_2022_hourly[['SOLAR']]
    pv_df['Datetime'] = pd.date_range(start='2022-01-01', periods=len(pv_df), freq='H')
    pv_df.set_index('Datetime', inplace=True)
    wind_df = df_All_2022_hourly[['WIND']]
    wind_df['Datetime'] = pd.date_range(start='2022-01-01', periods=len(wind_df), freq='H')
    wind_df.set_index('Datetime', inplace=True)
    price_df = df_All_2022_hourly[['systemSellPrice']]
    price_df['Datetime'] = pd.date_range(start='2022-01-01', periods=len(price_df), freq='H')
    price_df.set_index('Datetime', inplace=True)
    electricity_df = df_All_2022_hourly[['ND']]
    pv_data = pv_df['SOLAR'].to_numpy(dtype=float)
    wind_data = wind_df['WIND'].to_numpy(dtype=float)
    price = price_df['systemSellPrice'].to_numpy(dtype=float)
    electricity = electricity_df['ND'].to_numpy(dtype=float)
    # netload=electricity-pv_data
    for element in pv_data:
        data_manager.add_pv_element(element)
    for element in wind_data:
        data_manager.add_wind_element(element)
    for element in price:
        data_manager.add_price_element(element)
    for element in electricity:
        data_manager.add_electricity_element(element)
    return pv_data, wind_data, price, electricity
_load_year_data()
pv = data_manager.get_series_pv_data(month, day)
pv = {i: val for i, val in enumerate(pv)}
wind = data_manager.get_series_wind_data(month, day)
wind = {i: val for i, val in enumerate(wind)}
price = data_manager.get_series_price_data(month, day)
price = {i: val for i, val in enumerate(price)}
load = data_manager.get_series_electricity_cons_data(month, day)
load = {i: val for i, val in enumerate(load)}

"""**Visualize Historical Wind, Solar, and Price Data**"""

fig, ax = plt.subplots(figsize=(15, 6))
plt.gca().set_facecolor("white")
plt.gca().spines[['top', 'right']].set_visible(False)
Price_plot = sns.barplot(x=list(range(1, 24)), y=[price[h] for h in range(1, 24)])
Price_plot.set_xticklabels(Price_plot.get_xticklabels());
plt.title(f'Historical Price Data for {day}/{month}/2022',
          size=22, fontweight='bold')
plt.xlabel('Every Hour (Year 2022)', size=25, fontweight='bold')
plt.ylabel('Price (£/MWh)', size=25, fontweight='bold')
plt.xticks(rotation=30, size=15, fontweight='bold')
plt.yticks(rotation=0, ha='right', size=15, fontweight='bold')
ax = plt.gca()
# Set linewidth and edge color for each spine
for spine in ax.spines.values():
    spine.set_linewidth(2)
    spine.set_edgecolor('black')
ax.xaxis.grid(False)  # Remove x-axis grid lines
ax.yaxis.grid(True)
# Save the plot to a directory in Google Drive
output_dir = '/content/gdrive/MyDrive/Energy_System_RL_Modelling_Plots'
plt.savefig(f'{output_dir}/Historical_Price_data_scenario.png')
plt.tight_layout()
plt.show()

fig, ax = plt.subplots(figsize=(15, 6))
plt.gca().set_facecolor("white")
plt.gca().spines[['top', 'right']].set_visible(False)
PV_plot = sns.barplot(x=list(range(1, 24)), y=[pv[h] for h in range(1, 24)],
                        label='PV', color='maroon')
PV_plot.set_xticklabels(PV_plot.get_xticklabels())
#plt.title('Historical Solar PV data - December 1st, 2022', size=22, fontweight='bold')
plt.title(f'Historical Solar PV for {day}/{month}/2022',
          size=22, fontweight='bold')
plt.xlabel('Every Hour (Year 2022)', size=25, fontweight='bold')
plt.ylabel('MWh', size=25, fontweight='bold')
plt.xticks(rotation=30, size=15, fontweight='bold')
plt.yticks(rotation=0, ha='right', size=15, fontweight='bold')
plt.legend(loc='best', ncol=1, frameon=True, fontsize='15',
           fancybox=True, framealpha=1, shadow=True, borderpad=2)
ax = plt.gca()
for spine in ax.spines.values():
    spine.set_linewidth(2)
    spine.set_edgecolor('black')
ax.xaxis.grid(False)
ax.yaxis.grid(True)
output_dir = '/content/gdrive/MyDrive/Energy_System_RL_Modelling_Plots'
plt.savefig(f'{output_dir}/Historical_PV_data_scenario.png')
plt.tight_layout()
#plt.legend()
plt.show()

fig, ax = plt.subplots(figsize=(15, 6))
plt.gca().set_facecolor("white")
plt.gca().spines[['top', 'right']].set_visible(False)
wind_plot = sns.barplot(x=list(range(1, 24)), y=[wind[h] for h in range(1, 24)],
                        label='Wind', color='navy')
wind_plot.set_xticklabels(wind_plot.get_xticklabels())
#plt.title('Historical Solar PV data - December 1st, 2022', size=22, fontweight='bold')
plt.title(f'Historical Wind data for {day}/{month}/2022',
          size=22, fontweight='bold')
plt.xlabel('Every Hour (Year 2022)', size=25, fontweight='bold')
plt.ylabel('MWh', size=25, fontweight='bold')
plt.xticks(rotation=30, size=15, fontweight='bold')
plt.yticks(rotation=0, ha='right', size=15, fontweight='bold')
plt.legend(loc='best', ncol=1, frameon=True, fontsize='15',
           fancybox=True, framealpha=1, shadow=True, borderpad=2)
ax = plt.gca()
for spine in ax.spines.values():
    spine.set_linewidth(2)
    spine.set_edgecolor('black')
ax.xaxis.grid(False)
ax.yaxis.grid(True)
output_dir = '/content/gdrive/MyDrive/Energy_System_RL_Modelling_Plots'
plt.savefig(f'{output_dir}/Historical_Wind_data_scenario.png')
plt.tight_layout()
#plt.legend()
plt.show()

"""**Historical Power Demand Data**

We store the demand as a dictionary indexed by each hour (between 1 and 24) and valued by the amount of power needed.
"""

load_df_ = df_All_2022_hourly[['ND']]
load_df_['snapshots'] = pd.date_range(
    start='2022-01-01', periods=len(load_df_), freq='H')
load_df_.set_index('snapshots', inplace=True)
# Extracting year, month, day, and hour from the index
load_df_['YEAR'] = load_df_.index.year
load_df_['MONTH'] = load_df_.index.month
load_df_['DAY'] = load_df_.index.day
load_df_['HOUR'] = load_df_.index.hour
# Creating the DataFrame with separate columns for each component
load_df = pd.DataFrame({
    'YEAR': load_df_['YEAR'],
    'MONTH': load_df_['MONTH'],
    'DAY': load_df_['DAY'],
    'HOUR': load_df_['HOUR'],
    'LOAD': load_df_['ND']  # Use 'ND' instead of 'values'
})
# Filter the DataFrame for the specific date
df_subset = load_df[(load_df['YEAR'] == 2022) & (load_df['MONTH'] == month) & (load_df['DAY'] == day)]
# Store the demand to a dictionary
d = df_subset.set_index(['HOUR']).LOAD.to_dict()
H = set(d.keys()) # set of hours in a day (1 through 24)
# We can visualize the power usage.
fig, ax = plt.subplots(figsize=(15, 6))
plt.gca().set_facecolor("white")
plt.gca().spines[['top', 'right']].set_visible(False)
# Use seaborn for barplot
demand_plot = sns.barplot(x=list(range(1, 24)), y=[d[h] for h in range(1, 24)])
demand_plot.set_xticklabels(demand_plot.get_xticklabels())
plt.title(f'Historical power demand data for {day}/{month}/2022',
          size=22, fontweight='bold')
plt.xlabel('Every Hour (Year 2022)', size=25, fontweight='bold')
plt.ylabel('Load Demand (MWh)', size=25, fontweight='bold')
plt.xticks(rotation=30, size=15, fontweight='bold')
plt.yticks(rotation=0, ha='right', size=15, fontweight='bold')
ax = plt.gca()
# Set linewidth and edge color for each spine
for spine in ax.spines.values():
    spine.set_linewidth(2)
    spine.set_edgecolor('black')
ax.xaxis.grid(False)  # Remove x-axis grid lines
ax.yaxis.grid(True)
# Save the plot to a directory
output_dir = '/content/gdrive/MyDrive/Energy_System_RL_Modelling_Plots'
plt.savefig(f'{output_dir}/Historical_power_demand_data_scenario.png')
plt.tight_layout()
plt.show()

"""###**The Energy Optimization Model**

The **Energy Systems Optimal Scheduling problem** can be modeled by Model-Based approaches such that they consider the uncertainties introduced by variable renewable energy sources using a probability distribution function or by leveraging a set of representative scenarios, constructing a complex mathematical formulation considering the system’s operational constraints.

This formulation leads to a stochastic multi-stage mathematical programming model that can be solved using
commercial **mixed-integer nonlinear programming (MINLP) Model**. MINLP is capable of providing good quality solutions, existing model-based
approaches are not adequate for real-time operation due to the large computational time required. Coupled with a large number of distributed generation (DG) units and the increasing level of uncertainty, innovative approaches are urgently needed, capable of providing in realtime good-quality solutions while still enforcing operational and technical constraints.

To overcome this, model-free approaches have been introduced as an alternative solution. The most promising model-free approach is based on the use of reinforcement learning
(RL)

**Solving the mixed-integer nonlinear programming (MINLP) problem With Gurobi Solver**

We model this decision-making problem using mathematical optimization. Three components of a mathematical optimization model are used: the objective function to be optimized, the decision variables, and the constraints. In our power generation problem, the objective is to **minimize the overall costs.** The decision variables model the power generation schedule. The constraints capture basic requirements such as ensuring that the power supply meets the demand, as well as practical limitations such as the minimum and maximum production levels for each power plant. By finding the optimally cost-efficient schedule, this model helps power plant operators get the best output from their facilities while minimizing the overall costs.
"""

def optimization_base_result(env_RL, month, day, initial_soc):
    pv = env_RL.data_manager.get_series_pv_data(month, day)
    wind = env_RL.data_manager.get_series_wind_data(month, day)
    price = env_RL.data_manager.get_series_price_data(month, day)
    load = env_RL.data_manager.get_series_electricity_cons_data(month, day)
    period = env_RL.episode_length
    # parameters
    DG_parameters = env_RL.dg_parameters
    def get_dg_info(parameters):
        p_max = []
        p_min = []
        ramping_up = []
        ramping_down = []
        dg_cost = []
        for name, gen_info in parameters.items():
            p_max.append(gen_info['power_output_max'])
            p_min.append(gen_info['power_output_min'])
            ramping_up.append(gen_info['ramping_up'])
            ramping_down.append(gen_info['ramping_down'])
            dg_cost.append(gen_info['dg_cost'])
        return p_max, p_min, ramping_up, ramping_down, dg_cost
    p_max, p_min, ramping_up, ramping_down, dg_cost = get_dg_info(
        parameters = DG_parameters)
    battery_parameters = env_RL.battery_parameters
    NUM_GEN = len(DG_parameters.keys())
    battery_capacity = env_RL.battery1.capacity
    battery_efficiency = env_RL.battery1.efficiency
    m = gp.Model("PowerGeneration")
    # add the be the amount of power to be generated from power plant i during
    # hour h; this has to be at least 0, variable to the model.
    z = m.addVars(NUM_GEN, period, name = "z", lb = 0) # power generated in each plant for each hour
    # add the Binary variables to the model : Let u be an indicator of whether
    # plant i is on during hour h. It act as on/off switches - 1 if true, or 0 otherwise.
    u = m.addVars(NUM_GEN, period, name = "u", vtype = GRB.BINARY) # is the plant on? for each plant and hour
    # let v and w and be indicators of whether plant i is started up or shut down,
    # respectively, for hour h.
    v = m.addVars(NUM_GEN, period, name = "v", vtype = GRB.BINARY) # start up the plant? for each plant and hour
    w = m.addVars(NUM_GEN, period, name = "w", vtype = GRB.BINARY) # shut down the plant? for each plant and hour
    bec = m.addVars(period, vtype = GRB.CONTINUOUS, lb = -env_RL.battery1.max_charge,
                    ub = env_RL.battery1.max_charge, name = 'bec')
    gei = m.addVars(period, vtype = GRB.CONTINUOUS, lb = 0, ub = env_RL.grid.exchange_ability,
                    name = 'gei')# set constrains for exchange between external grid and distributed energy system
    gee = m.addVars(period, vtype = GRB.CONTINUOUS, lb = 0, ub = env_RL.grid.exchange_ability,
                    name='gee')
    soc = m.addVars(period, vtype = GRB.CONTINUOUS, lb = 0.2, ub = 0.8, name='soc')
    print("This model has", len(z) + len(u) + len(v) + len(w) +\
          len(bec) + len(gei) + len(gee) + len(soc), "decision variables.")
    # Next, we tell the model what range of values the decision variables can take.
    # This is done through constraints.
    # Constraint: Meet demand:
    # First, we want to make sure that the total power generated by all the power
    # plants in each hour is > OR equal to the power demand in that hour.
    m.addConstrs(((
        sum(z[i, h] for i in range(NUM_GEN)) + pv[h] + wind[h] + gei[h] >= load[h] + bec[h] + gee[h]
        ) for h in range(period)), name = 'powerbalance')
    #Constraint: State of Charge (SOC)
    m.addConstr(battery_capacity * soc[0] == battery_capacity * initial_soc + (
        bec[0] * battery_efficiency), name = 'soc0')
    m.addConstrs((
        battery_capacity * soc[h] == battery_capacity * soc[h-1] + (
            bec[h] * battery_efficiency) for h in range(1, period)), name = 'soc update')
    # Constraint: Maximum and minimum generation levels:
    # We make sure that the power generated from each plant does not exceed the plant's
    # maximum capacity (denoted by). We also want to make sure that when the plant
    # is "off", when, we do not generate any power.
    # Furthermore, recall that each plant needs to generate a certain minimum %
    # amount of power, i.e: plants must produce at least 1% of their capacity.
    m.addConstrs((z[i, h] <= u[i, h] * p_max[i] for i in range(
        NUM_GEN) for h in range(period)), 'output_max')
    m.addConstrs((z[i, h] >= u[i, h] * p_min[i] for i in range(
        NUM_GEN) for h in range(period)), 'output_min')
    # Constraint: Max rampdown, rampup:
    # While operating the power plants, it is preferable not to cause drastic changes
    # in power generation. We can enforce a limit on the speed at which power generation
    # is ramped up or ramped down. We can define constraints that ensure that the
    # magnitude of quantity is no more than a certain percentage of the maximum
    # capacity. This percentage is given by the ramp up/ramp down speed.
    m.addConstrs((
        z[i, h + 1] - z[i, h] <= ramping_up[i] for i in range(
            NUM_GEN) for h in range(period - 1)), 'ramping_up')
    m.addConstrs((
        z[i, h] - z[i, h + 1] <= ramping_down[i] for i in range(
            NUM_GEN) for h in range(period - 1)), 'ramping_down')
    # Constraint: If switched on, must be on:
    # Next, we ensure that when a power plant is switched "on," the plant is "on"
    # (and in effect starts generating power). Mathematically, when v = 1,
    # u is set to 1. Similarly, when a plant is "switched off," the plant is "off";
    # when w is 1, u is set to 0.
    m.addConstrs((v[i, h] <= u[i, h]) for i in range(NUM_GEN) for h in range(
        period))
    m.addConstrs((w[i, h] <= 1 - u[i, h]) for i in range(NUM_GEN) for h in range(
        period))
    # Constraint: Link startup/shutdown variables to "on"/"off" variables:
    # Finally, we link the startup/shutdown variables (v and w) with the "on"/"off"
    # variables (u).
    # Three possible scenario values are: -1, 0, 1.
    #If it is -1, it means that the plant is "switched off" for hour h, forcing
    # the variable w to be 1.
    # If it is 1, it means that the plant is "switched on" for hour h, forcing the
    # variable v to be 1.
    # If it is 0, it means that the plant is neither "switched on" nor "switched off"
    # for hour h; forcing the variables w and v to be 0.
    m.addConstrs((
        v[i, h] - w[i, h] == u[i, h] - u[i, h-1]) for i in range(
            NUM_GEN) for h in range(period) if h > 1)
    # Objective: Minimize the total costs
    objective = gp.quicksum((dg_cost[i] * u[i, h])for h in range(
        period) for i in range(NUM_GEN)) # operating cost
    cost_grid_import = gp.quicksum(gei[h] * price[h] for h in range(period))
    cost_grid_export = gp.quicksum(
        gee[h] * price[h] * env_RL.sell_coefficient for h in range(period))
    m.setObjective((objective+cost_grid_import-cost_grid_export), sense = GRB.MINIMIZE)
    m.update()
    print(m.getAttr("VarName", m.getVars()))
    print('==========================================================================')
    print(m.getAttr("ConstrName", m.getConstrs()))
    print('==========================================================================')
    obj = m.getObjective()
    print(f"\nobj: {obj}")
    print('==========================================================================')
    m.optimize()

    output_record={'pv':[], 'wind':[], 'price':[],'load':[],'netload':[],'soc':[],'battery_energy_change':[],'grid_import':[],'grid_export':[],'GAS':[],'COAL':[],'BIOMASS':[],'NUCLEAR':[],'HYDRO':[],'IMPORTS':[],'STORAGE':[],'OTHER':[],'step_cost':[]}
    for h in range(period):
        gen_cost = sum((u[i, h].x * (dg_cost[i])) for i in range(NUM_GEN))
        grid_import_cost = gei[h].x * price[h]
        grid_export_cost = gee[h].x * price[h] *env_RL.sell_coefficient
        output_record['pv'].append(pv[h])
        output_record['wind'].append(wind[h])
        output_record['price'].append(price[h])
        output_record['load'].append(load[h])
        output_record['netload'].append(load[h]-pv[h]-wind[h])
        output_record['soc'].append(soc[h].x)
        output_record['battery_energy_change'].append(bec[h].x)
        output_record['grid_import'].append(gei[h].x)
        output_record['grid_export'].append(gee[h].x)
        output_record['GAS'].append(z[0, h].x)
        output_record['COAL'].append(z[1, h].x)
        output_record['BIOMASS'].append(z[2, h].x)
        output_record['NUCLEAR'].append(z[3, h].x)
        output_record['HYDRO'].append(z[4, h].x)
        output_record['IMPORTS'].append(z[5, h].x)
        output_record['STORAGE'].append(z[6, h].x)
        output_record['OTHER'].append(z[7, h].x)
        output_record['step_cost'].append(gen_cost+grid_import_cost-grid_export_cost)
    output_record_df = pd.DataFrame.from_dict(output_record)
    return output_record_df

month = record['init_info'][0][0]
day = record['init_info'][0][1]
initial_soc = record['init_info'][0][3]
print(f'Month is: {month}, Day is:{day}, Initial SOC is:{initial_soc}')
base_result = optimization_base_result(env_RL, month, day, initial_soc)

"""##**Optimal Solution Results Analysis**"""

def plot_optimization_result(datasource, directory):  # data source is dataframe
    sns.set_theme(style='whitegrid')
    plt.rcParams["figure.figsize"] = (22, 20)
    fig, axs = plt.subplots(3, 1)
    # fig.tight_layout()
    plt.subplots_adjust(wspace=0.6, hspace=0.4)
    plt.autoscale(tight=True)
    # plt.subplots_adjust(wspace=0.7, hspace=0.3)
    T = np.array([i for i in range(24)])
    # plot step cost
    axs[0].cla()
    axs[0].set_ylabel('Costs', size=35, fontweight='bold')
    axs[0].set_xlabel('Time(h)', size=35, fontweight='bold')
    axs[0].bar(T, datasource['step_cost'], color = 'firebrick')
    axs[0].set_title('Step Cost', size=40, fontweight='bold')
    axs[0].tick_params(axis='x', rotation=30, labelsize=25)
    axs[0].tick_params(axis='y', rotation=0, labelsize=30)
    #axs[0] = plt.gca()
    for spine in axs[0].spines.values():
        spine.set_linewidth(2)
        spine.set_edgecolor('black')
    axs[0].xaxis.grid(False)
    axs[0].yaxis.grid(True)
    # plot soc and price at first
    axs[1].cla()
    axs[1].set_ylabel('Price', size=35, fontweight='bold')
    axs[1].set_xlabel('Time(h)', size=35, fontweight='bold')
    axs[1].plot(T, datasource['price'], drawstyle='steps-mid', label='Price',
                color='pink', linewidth = 3.0)
    axs_soc = axs[1].twinx()
    axs_soc.set_ylabel('SOC')
    axs_soc.plot(T, datasource['soc'], drawstyle='steps-mid',  label='SOC',
                 color = 'limegreen', linewidth = 3.0)
    # Combine legends from both Axes
    lines, labels = axs[1].get_legend_handles_labels()
    lines_soc, labels_soc = axs_soc.get_legend_handles_labels()
    axs[1].legend(lines + lines_soc, labels + labels_soc,
                  loc='best', ncol=1, frameon=True, fontsize='20',
           fancybox=True, framealpha=1, shadow=True, borderpad=2, labelspacing=0.3)
    axs[1].set_title('SOC and Price', size=35, fontweight='bold')
    axs[1].tick_params(axis='x', rotation=30, labelsize=25)
    axs[1].tick_params(axis='y', rotation=0, labelsize=25)
    axs[1] = plt.gca()
    for spine in axs[1].spines.values():
        spine.set_linewidth(2)
        spine.set_edgecolor('black')
    axs[1].xaxis.grid(False)
    axs[1].yaxis.grid(True)
    # plot accumulated generation and consumption here
    axs[2].cla()
    axs[2].set_ylabel('Outputs of DGs and Battery', size=25, fontweight='bold')
    axs[2].set_xlabel('Time(h)', size=35, fontweight='bold')
    battery_positive = np.array(datasource['battery_energy_change'])
    battery_negative = np.array(datasource['battery_energy_change'])
    battery_negative = np.minimum(battery_negative, 0)  # discharge
    battery_positive = np.maximum(battery_positive, 0)  # charge
    imported_from_grid = np.array(datasource['grid_import'])
    exported_2_grid = np.array(datasource['grid_export'])
    axs[2].bar(T, datasource['GAS'], label='GAS', color = 'purple')
    axs[2].bar(T, datasource['COAL'], label='COAL', bottom=datasource['GAS'],
               color = 'fuchsia')
    axs[2].bar(T, datasource['BIOMASS'], label='BIOMASS',
                  bottom=datasource['COAL'] + datasource['GAS'], color = 'rosybrown')
    axs[2].bar(T, datasource['NUCLEAR'], label='NUCLEAR',
                  bottom=datasource['BIOMASS'] + datasource['COAL'] +\
               datasource['GAS'], color = 'darkorange')
    axs[2].bar(T, datasource['HYDRO'], label='HYDRO',
                  bottom=datasource['NUCLEAR'] + datasource['BIOMASS'] +\
               datasource['COAL'] + datasource['GAS'], color = 'dodgerblue')
    axs[2].bar(T, datasource['IMPORTS'], label='IMPORTS', bottom=datasource['HYDRO'] + datasource['NUCLEAR'] + datasource['BIOMASS'] + datasource['COAL'] + datasource['GAS'],
               color = 'darkred')
    axs[2].bar(T, datasource['STORAGE'], label='STORAGE', bottom=datasource['IMPORTS'] + datasource['HYDRO'] + datasource['NUCLEAR'] + datasource['BIOMASS'] + datasource['COAL'] + datasource['GAS'],
               color = 'steelblue')
    axs[2].bar(T, datasource['OTHER'], label='OTHER', bottom=datasource['STORAGE'] + datasource['IMPORTS'] + datasource['HYDRO'] + datasource['NUCLEAR'] + datasource['BIOMASS'] + datasource['COAL'] + datasource['GAS'],
               color = 'palegreen')
    axs[2].bar(T, -battery_positive, color='blue', hatch='/', label='battery charge')
    axs[2].bar(T, -battery_negative, hatch='/', label='battery discharge', bottom=datasource['OTHER'] + datasource['STORAGE'] + datasource['IMPORTS'] + datasource['HYDRO'] + datasource['NUCLEAR'] + datasource['BIOMASS'] + datasource['COAL'] + datasource['GAS'],
               color = 'peachpuff')
    # import as generate
    axs[2].bar(T, imported_from_grid, label='import from grid', bottom=-battery_negative + datasource['OTHER'] + datasource['STORAGE'] + datasource['IMPORTS'] + datasource['HYDRO'] + datasource['NUCLEAR'] + datasource['BIOMASS'] + datasource['COAL'] + datasource['GAS'],
               color = 'peru')
    # export as load
    axs[2].bar(T, -exported_2_grid, label='export to grid', bottom=-battery_positive,
               color = 'darkseagreen')
    axs[2].plot(T, datasource['netload'], label='netload',  drawstyle='steps-mid',
                alpha=0.7, linewidth = 5.0, color = 'gold')
    axs[2].set_title('Accumulated Generation and Consumption',
                     size=35, fontweight='bold')
    axs[2].tick_params(axis='x', rotation=30, labelsize=25)
    axs[2].tick_params(axis='y', rotation=0, labelsize=25)
    axs[2].legend(loc='best', ncol=4, frameon=True, fontsize='15',
           fancybox=True, framealpha=1, shadow=True, borderpad=2, labelspacing=0.3)
    #fig.savefig(f"{directory}/optimization_information.svg", format='svg', dpi=600, bbox_inches='tight')
    fig.suptitle(f'MINLP Optimization Results for {day}/{month}/2022',
              size=45, fontweight='bold', y = 1.0)
    #axs[2] = plt.gca()
    for spine in axs[2].spines.values():
        spine.set_linewidth(2)
        spine.set_edgecolor('black')
    axs[2].xaxis.grid(False)
    axs[2].yaxis.grid(True)
    output_dir = '/content/gdrive/MyDrive/Energy_System_RL_Modelling_Plots'
    plt.savefig(f'{output_dir}/MINLP_Optimization_results.png',
                format='svg', dpi=600, bbox_inches='tight')
    plt.tight_layout()
    #plt.legend()
    plt.show()
    print('optimization result has been ploted')
plot_args = PlotArgs()
plot_args.feature_change = ''
args.cwd = agent_name  # change
plot_dir = make_dir(args.cwd, plot_args.feature_change)
plot_optimization_result(base_result, plot_dir)
#plot_evaluation_information(args.cwd + '/' + 'test_data.pkl', plot_dir)
'''compare the different cost get from Gurobi and DQN'''
ratio = sum(eval_data['operation_cost']) / sum(base_result['step_cost'])
MIP_DQN_Model_Op_Cost = sum(eval_data['operation_cost'])
MINLP_Model_Op_Cost = sum(base_result['step_cost'])
print(f'MIP-DQN Model Operation Cost is: {MIP_DQN_Model_Op_Cost:.3f}')
print(f'MINLP Model Operation Cost is: {MINLP_Model_Op_Cost:.3f}')
print(f'Ratio of MIP_DQN and MINLP Model Operation Cost is: {ratio:.3f}')

'''compare the different cost get from PPO and MIP-DQN'''
#ratio = sum(eval_data['operation_cost']) / sum(eval_data_PPO['operation_cost'])
MIP_DQN_Model_Op_Cost = sum(eval_data['operation_cost'])
MINLP_Model_Op_Cost = sum(base_result['step_cost'])
cheapest_model_cost, cheapest_model_name = min(
    (MIP_DQN_Model_Op_Cost, 'MIP-DQN'),
    (MINLP_Model_Op_Cost, 'MINLP'),
    key=lambda x: x[0]
)
print(f'Month is: {month}, Day is:{day}, Initial SOC is:{initial_soc:.3f}')
print(f'MIP-DQN Model Operation Cost is: {MIP_DQN_Model_Op_Cost:.3f}')
print(f'MINLP Model Operation Cost is: {MINLP_Model_Op_Cost:.3f}')
print(f'The Cheapest Model Operation Cost is: {cheapest_model_cost:.3f} (Model: {cheapest_model_name})')
#print(f'Ratio of MIP_DQN and PPO Model Operation Cost is: {ratio}')

"""##**Results Analysis and Comparison For MINLP and MIP-DQN**

**Obtain the Training Loss Data**
"""

datasource = '/content/AgentMIPDQN/MIP_DQN_experiments/loss_data.pkl'
with open(datasource, 'rb') as tf:
    loss_data = pickle.load(tf)
print(loss_data.keys())
# Accessing data using keys
episode_value = loss_data['episode']
steps_value = loss_data['steps']
critic_loss_value = loss_data['critic_loss']
actor_loss_value = loss_data['actor_loss']
entropy_loss_value = loss_data['entropy_loss']
# Printing the values
print(f"Episode: {episode_value}")
print(f"Steps: {steps_value}")
print(f"Critic Loss: {critic_loss_value}")
print(f"Actor Loss: {actor_loss_value}")
print(f"Entropy Loss: {entropy_loss_value}")

"""**Plot the Critic and Actor Networks Loss Data**"""

plt.figure(figsize=(15, 9))
plt.gca().set_facecolor("white")
plt.gca().spines[['top', 'right']].set_visible(False)
episode_values = list(range(len(critic_loss_value)))
pd.Series(critic_loss_value, index=episode_values).plot(
    kind='line', color='sienna', label='Critic Network Loss', linewidth=3.0)
pd.Series(actor_loss_value, index=episode_values).plot(
    kind='line', color='violet', label='Actor Network Loss', linewidth=3.0)
plt.title('Critic and Actor Networks Training Loss Data', size=25, fontweight='bold')
plt.xlabel('Episode', size=25, fontweight='bold')
plt.ylabel('Loss', size=25, fontweight='bold')
plt.xticks(rotation=30, size=15, fontweight='bold')
plt.yticks(rotation=0, ha='right', size=15, fontweight='bold')
ax = plt.gca()
# Set linewidth and edge color for each spine
for spine in ax.spines.values():
    spine.set_linewidth(2)
    spine.set_edgecolor('black')
ax.xaxis.grid(False)  # Remove x-axis grid lines
ax.yaxis.grid(True)
plt.legend(loc='best', ncol=1, frameon=True, fontsize='15',
           fancybox=True, framealpha=1, shadow=True, borderpad=2)
# Save the plot to a directory in Google Drive
output_dir = '/content/gdrive/MyDrive/Energy_System_RL_Modelling_Plots'
plt.savefig(f'{output_dir}/Critic_and_Actor_Networks_Training_Loss_Data.png')
plt.tight_layout()
plt.show()

"""**Obtain the Training Reward Data**"""

datasource = '/content/AgentMIPDQN/MIP_DQN_experiments/reward_data.pkl'
with open(datasource, 'rb') as tf:
    reward_data = pickle.load(tf)
print(reward_data.keys())
# Accessing data using keys
episode_value = reward_data['episode']
steps_value = reward_data['steps']
mean_episode_reward_value = reward_data['mean_episode_reward']
unbalance_value = reward_data['unbalance']
episode_operation_cost_value = reward_data['episode_operation_cost']
# Printing the values
print(f"Episode: {episode_value}")
print(f"Steps: {steps_value}")
print(f"Mean Episode Reward: {mean_episode_reward_value}")
print(f"unbalance: {unbalance_value}")
print(f"Episode Operation Cost: {episode_operation_cost_value}")

"""**Plot the Training Mean Episode, Unbalance, and Operation Cost Loss Data**"""

plt.figure(figsize=(15, 9))
plt.gca().set_facecolor("white")
plt.gca().spines[['top', 'right']].set_visible(False)
episode_values = list(range(len(mean_episode_reward_value)))
pd.Series(mean_episode_reward_value, index=episode_values).plot(
    kind='line', color='maroon', label='Mean Episode Reward Loss', linewidth=3.0)
pd.Series(unbalance_value, index=episode_values).plot(
    kind='line', color='navy', label='Unbalance Loss', linewidth=3.0)
pd.Series(episode_operation_cost_value, index=episode_values).plot(
    kind='line', color='green', label='Episode Operation Cost Loss', linewidth=3.0)
plt.title('Training Mean Episode, Unbalance, and Operation Cost Loss Data',
          size=25, fontweight='bold')
plt.xlabel('Episode', size=25, fontweight='bold')
plt.ylabel('Loss', size=25, fontweight='bold')
plt.xticks(rotation=30, size=15, fontweight='bold')
plt.yticks(rotation=0, ha='right', size=15, fontweight='bold')
plt.yscale('symlog')
ax = plt.gca()
# Set linewidth and edge color for each spine
for spine in ax.spines.values():
    spine.set_linewidth(2)
    spine.set_edgecolor('black')
ax.xaxis.grid(False)  # Remove x-axis grid lines
ax.yaxis.grid(True)
plt.legend(loc='best', ncol=1, frameon=True, fontsize='15',
           fancybox=True, framealpha=1, shadow=True, borderpad=2)
# Save the plot to a directory in Google Drive
output_dir = '/content/gdrive/MyDrive/Energy_System_RL_Modelling_Plots'
plt.savefig(f'{output_dir}/Training_Mean_Episode_Unbalance_and_Operation_Cost_Loss_Data.png')
plt.tight_layout()
plt.show()

"""**Obtain the Test Data Results**"""

datasource = '/content/AgentMIPDQN/test_data.pkl'
with open(datasource, 'rb') as tf:
    test_data = pickle.load(tf)
print(test_data.keys())
# Accessing data using keys
init_info_value = test_data['init_info']
information_value = test_data['information']
state_value = test_data['state']
action_value = test_data['action']
reward_value = test_data['reward']
cost_value = test_data['cost']
unbalance_value = test_data['unbalance']
record_output_value = test_data['record_output']
# Printing the values
print(f"Initial Information: {init_info_value}")
print(f"Information: {information_value}")
print(f"State: {state_value}")
print(f"Action: {action_value}")
print(f"Reward: {reward_value}")
print(f"Cost: {cost_value}")
print(f"Unbalance: {unbalance_value}")
print(f"Record Output: {record_output_value}")
eval_data = pd.DataFrame(test_data['information'])
eval_data.columns = ['time_step', 'price', 'netload', 'action', 'real_action',
                     'soc', 'battery1', 'GAS', 'COAL', 'BIOMASS', 'NUCLEAR', 'HYDRO',
                     'IMPORTS', 'STORAGE', 'OTHER', 'unbalance', 'operation_cost']
eval_data

"""**Plot Unbalance of Plant Generation For MIP-DQN**"""

sns.set_theme(style='whitegrid')
plt.rcParams["figure.figsize"] = (16, 9)
fig, axs = plt.subplots(1, 1)
plt.subplots_adjust(wspace=0.7, hspace=0.3)
plt.autoscale(tight=True)
axs.cla()  # Use axs, not axs[0, 0]
axs.set_ylabel('Unbalance of Generation')
fixed_date = datetime(2022, 1, 1)
period = [fixed_date + timedelta(hours=i) for i in range(24)]
axs.bar(period, eval_data['unbalance'],
        label='Exchange with Grid : MIP-DQN', width=0.03, color = 'sienna')
plt.title(f'Unbalance of Generation {day}/{month}/2022', size=25, fontweight='bold')
plt.ylabel('MW', size=25, fontweight='bold')
plt.xlabel('Time (Hour)', size=25, fontweight='bold')
plt.xticks(rotation=30, size=15, fontweight='bold')
plt.yticks(rotation=0, ha='right', size=15, fontweight='bold')
ax = plt.gca()
# Set linewidth and edge color for each spine
for spine in ax.spines.values():
    spine.set_linewidth(2)
    spine.set_edgecolor('black')
ax.xaxis.grid(False)  # Remove x-axis grid lines
ax.yaxis.grid(True)
plt.legend(loc='best', ncol=1, frameon=True, fontsize='15',
           fancybox=True, framealpha=1, shadow=True, borderpad=2)
# Save the plot to a directory in Google Drive
output_dir = '/content/gdrive/MyDrive/Energy_System_RL_Modelling_Plots'
plt.savefig(f'{output_dir}/Unbalance_of_Generation.png')
plt.tight_layout()
plt.show()

"""**Plot Net Load For Both Models**"""

sns.set_theme(style='whitegrid')
plt.rcParams["figure.figsize"] = (16, 9)
fig, axs = plt.subplots(1, 1)
plt.subplots_adjust(wspace=0.7, hspace=0.3)
plt.autoscale(tight=True)
axs.cla()  # Use axs, not axs[0, 0]
fixed_date = datetime(2022, 1, 1)
period = [fixed_date + timedelta(hours=i) for i in range(24)]
axs.bar(period, eval_data['netload'], label = 'Netload - MIP-DQN', width = 0.01,
        color='orange', align='edge')
axs.bar(period, base_result['netload'], label='Netload - MINLP',
         color = 'crimson', width=0.01, align='center')
axs.xaxis_date()
axs.xaxis.set_major_formatter(plt.matplotlib.dates.DateFormatter('%H:%M'))
plt.title(f'Comparison of Net Load for Both Models {day}/{month}/2022',
          size=35, fontweight='bold')
plt.ylabel('Netload (MWh)', size=25, fontweight='bold')
plt.xlabel('Time (Hour)', size=25, fontweight='bold')
plt.xticks(rotation=30, size=15, fontweight='bold')
plt.yticks(rotation=0, ha='right', size=15, fontweight='bold')
ax = plt.gca()
# Set linewidth and edge color for each spine
for spine in ax.spines.values():
    spine.set_linewidth(2)
    spine.set_edgecolor('black')
ax.xaxis.grid(False)  # Remove x-axis grid lines
ax.yaxis.grid(True)
axs.set_yscale("log")
axs.legend(loc='best', ncol=1, frameon=True, fontsize='15', labelspacing = 0.5,
           fancybox=True, framealpha=1, shadow=True, borderpad=2)
# Save the plot to a directory in Google Drive
output_dir = '/content/gdrive/MyDrive/Energy_System_RL_Modelling_Plots'
plt.savefig(f'{output_dir}/Comparison_of_both_models_NetLoad.png')
plt.tight_layout()
plt.show()

"""**Plot Operation Cost for Both Models**"""

sns.set_theme(style='whitegrid')
plt.rcParams["figure.figsize"] = (16, 9)
fig, axs = plt.subplots(1, 1)
plt.subplots_adjust(wspace=0.7, hspace=0.3)
plt.autoscale(tight=True)
fixed_date = datetime(2022, 1, 1)
period = [fixed_date + timedelta(hours=i) for i in range(24)]
axs.cla()
axs.bar(period, eval_data['operation_cost'], label = 'Operation Cost - MIP-DQN',
        width = 0.02, color='lightblue', align='edge')
axs.bar(period, base_result['step_cost'], label = 'Operation Cost - MINLP',
              width = 0.02, color='lightcoral', align='center')
axs.xaxis_date()
axs.xaxis.set_major_formatter(plt.matplotlib.dates.DateFormatter('%H:%M'))
plt.title(f'Operation Cost Comparison Between Models {day}/{month}/2022',
          size=25, fontweight='bold')
plt.ylabel('£/MWh', size=25, fontweight='bold')
plt.xlabel('Time (Hour)', size=25, fontweight='bold')
plt.xticks(rotation=30, size=15, fontweight='bold')
plt.yticks(rotation=0, ha='right', size=15, fontweight='bold')
ax = plt.gca()
# Set linewidth and edge color for each spine
for spine in ax.spines.values():
    spine.set_linewidth(2)
    spine.set_edgecolor('black')
ax.xaxis.grid(False)  # Remove x-axis grid lines
ax.yaxis.grid(True)
axs.legend(loc='best', ncol=1, frameon=True, fontsize='15', labelspacing = 0.5,
           fancybox=True, framealpha=1, shadow=True, borderpad=2)
# Save the plot to a directory in Google Drive
output_dir = '/content/gdrive/MyDrive/Energy_System_RL_Modelling_Plots'
plt.savefig(f'{output_dir}/Cost.png')
plt.tight_layout()
plt.show()

"""**Plot the Energy Price For Both Models**"""

sns.set_theme(style='whitegrid')
plt.rcParams["figure.figsize"] = (16, 9)
fig, axs = plt.subplots(1, 1)
plt.subplots_adjust(wspace=0.7, hspace=0.3)
plt.autoscale(tight=True)
axs.cla()  # Use axs, not axs[0, 0]
fixed_date = datetime(2022, 1, 1)
period = [fixed_date + timedelta(hours=i) for i in range(24)]
axs.plot(period, eval_data['price'], drawstyle='steps-mid', label = 'Price - MIP-DQN',
         color = 'purple', linewidth=7.0, marker='o')
axs.plot(period, base_result['price'], drawstyle='steps-mid',
               label='Price - MINLP',  color = 'navy', linewidth=7.0)
axs.legend(loc='upper left', fontsize = 12, frameon = False, labelspacing = 0.5)
axs.xaxis_date()
axs.xaxis.set_major_formatter(plt.matplotlib.dates.DateFormatter('%H:%M'))
plt.title(f'Energy Price for Both Models {day}/{month}/2022',
          size=25, fontweight='bold')
plt.ylabel('Price (£/MWh)', size=25, fontweight='bold')
plt.xlabel('Time (Hour)', size=25, fontweight='bold')
plt.xticks(rotation=30, size=15, fontweight='bold')
plt.yticks(rotation=0, ha='right', size=15, fontweight='bold')
ax = plt.gca()
# Set linewidth and edge color for each spine
for spine in ax.spines.values():
    spine.set_linewidth(2)
    spine.set_edgecolor('black')
ax.xaxis.grid(False)  # Remove x-axis grid lines
ax.yaxis.grid(True)
axs.legend(loc='best', ncol=1, frameon=True, fontsize='15', labelspacing = 0.5,
           fancybox=True, framealpha=1, shadow=True, borderpad=2)
# Save the plot to a directory in Google Drive
output_dir = '/content/gdrive/MyDrive/Energy_System_RL_Modelling_Plots'
plt.savefig(f'{output_dir}/Energy_Price.png')
plt.tight_layout()
plt.show()

"""**State of Charge (SOC) for Both MIP-DQN and MINLP Models**"""

sns.set_theme(style='whitegrid')
plt.rcParams["figure.figsize"] = (16, 9)
fig, axs = plt.subplots(1, 1)
plt.subplots_adjust(wspace=0.7, hspace=0.3)
plt.autoscale(tight=True)
axs.cla()  # Use axs, not axs[0, 0]
fixed_date = datetime(2022, 1, 1)
period = [fixed_date + timedelta(hours=i) for i in range(24)]
axs.plot(period, eval_data['soc'], drawstyle='steps-mid', label='SOC - MIP-DQN',
         color='green', linewidth=3.0)
axs.plot(period, base_result['soc'], drawstyle='steps-mid', label='SOC - MINLP',
         color='maroon', linewidth=3.0)
axs.legend(loc='upper left', fontsize = 12, frameon = False, labelspacing = 0.5)
axs.xaxis_date()
axs.xaxis.set_major_formatter(plt.matplotlib.dates.DateFormatter('%H:%M'))
plt.title(f'State of Charge (SOC) {day}/{month}/2022',
          size=25, fontweight='bold')
plt.xlabel('SOC', size=25, fontweight='bold')
plt.ylabel('Time (Hour)', size=25, fontweight='bold')
plt.xticks(rotation=30, size=15, fontweight='bold')
plt.yticks(rotation=0, ha='right', size=15, fontweight='bold')
ax = plt.gca()
# Set linewidth and edge color for each spine
for spine in ax.spines.values():
    spine.set_linewidth(2)
    spine.set_edgecolor('black')
ax.xaxis.grid(False)  # Remove x-axis grid lines
ax.yaxis.grid(True)
axs.legend(loc='best', ncol=1, frameon=True, fontsize='15', labelspacing = 0.5,
           fancybox=True, framealpha=1, shadow=True, borderpad=2)
# Save the plot to a directory in Google Drive
output_dir = '/content/gdrive/MyDrive/Energy_System_RL_Modelling_Plots'
plt.savefig(f'{output_dir}/SOC.png')
plt.tight_layout()
plt.show()

"""#**Build the Mixed Integer Programming - Proximal Policy Optimization (PPO)**

**Build The Actor and Critic Modules for PPO Model**
"""

class ActorPPO(nn.Module):
    def __init__(self, mid_dim, state_dim, action_dim,layer_norm=False):
        super().__init__()
        self.net = nn.Sequential(nn.Linear(state_dim, mid_dim), nn.ReLU(),
                                 nn.Linear(mid_dim, mid_dim), nn.ReLU(),
                                 nn.Linear(mid_dim, mid_dim), nn.Hardswish(),
                                 nn.Linear(mid_dim, action_dim),)
        # the logarithm (log) of standard deviation (std) of action, it is a trainable parameter
        self.a_logstd = nn.Parameter(
            torch.zeros((1, action_dim)) - 0.5, requires_grad=True)
        self.sqrt_2pi_log = np.log(np.sqrt(2 * np.pi))
        if layer_norm:
            self.layer_norm(self.net)
    @staticmethod
    def layer_norm(layer, std=1.0, bias_const=0.0):
        for l in layer:
            if hasattr(l, 'weight'):
                torch.nn.init.orthogonal_(l.weight, std)
                torch.nn.init.constant_(l.bias, bias_const)
    def forward(self, state):
        return self.net(state).tanh()  # action.tanh()
    def get_action(self, state):
        a_avg = self.net(state)# too big for the action
        a_std = self.a_logstd.exp()
        noise = torch.randn_like(a_avg)
        action = a_avg + noise * a_std
        return action, noise
    def get_logprob_entropy(self, state, action):
        a_avg = self.net(state)
        a_std = self.a_logstd.exp()
        delta = ((a_avg - action) / a_std).pow(2) * 0.5# delta here is the diverse between the
        logprob = -(self.a_logstd + self.sqrt_2pi_log + delta).sum(1)  # new_logprob
        dist_entropy = (logprob.exp() * logprob).mean()  # policy entropy
        return logprob, dist_entropy
    def get_old_logprob(self, _action, noise):  # noise = action - a_noise
        delta = noise.pow(2) * 0.5
        return -(self.a_logstd + self.sqrt_2pi_log + delta).sum(1)  # old_logprob
class CriticAdv(nn.Module):
    def __init__(self, mid_dim, state_dim, _action_dim, layer_norm=False):
        super().__init__()
        self.net = nn.Sequential(nn.Linear(state_dim, mid_dim), nn.ReLU(),
                                 nn.Linear(mid_dim, mid_dim), nn.ReLU(),
                                 nn.Linear(mid_dim, mid_dim), nn.Hardswish(),
                                 nn.Linear(mid_dim, 1))
        if layer_norm:
            self.layer_norm(self.net, std=1.0)
    @staticmethod
    def layer_norm(layer, std=1.0, bias_const=0.0):
        for l in layer:
            if hasattr(l,'weight'):
                torch.nn.init.orthogonal_(l.weight, std)
                torch.nn.init.constant_(l.bias, bias_const)
    def forward(self, state):
        return self.net(state)  # Advantage value

"""**Build the Agent's Base to Select Actions and Explore the Environment for PPO**"""

class AgentPPO:
    def __init__(self):
        super().__init__()
        self.state = None
        self.device = None
        self.action_dim = None
        self.get_obj_critic = None
        self.criterion = torch.nn.SmoothL1Loss()
        self.cri = self.cri_target = self.if_use_cri_target = self.cri_optim = self.ClassCri = None
        self.act = self.act_target = self.if_use_act_target = self.act_optim = self.ClassAct = None
        '''init modify'''
        self.ClassCri = CriticAdv
        self.ClassAct = ActorPPO
        self.ratio_clip = 0.2  # ratio.clamp(1 - clip, 1 + clip)
        self.lambda_entropy = 0.02  # could be 0.01~0.05
        self.lambda_gae_adv = 0.98  # could be 0.95~0.99, GAE (Generalized Advantage Estimation. ICLR.2016.)
        self.get_reward_sum = None  # self.get_reward_sum_gae if if_use_gae else self.get_reward_sum_raw
        self.trajectory_list = None
    def init(self, net_dim, state_dim, action_dim, learning_rate=1e-4, if_use_gae=False, gpu_id=0):
        self.device = torch.device(f"cuda:{gpu_id}" if (torch.cuda.is_available() and (gpu_id >= 0)) else "cpu")
        self.trajectory_list = list()
        self.get_reward_sum = self.get_reward_sum_gae if if_use_gae else self.get_reward_sum_raw# choose whether to use gae or not
        self.cri = self.ClassCri(net_dim, state_dim, action_dim).to(self.device)
        self.act = self.ClassAct(net_dim, state_dim, action_dim).to(self.device) if self.ClassAct else self.cri
        self.cri_target = deepcopy(self.cri) if self.if_use_cri_target else self.cri
        self.act_target = deepcopy(self.act) if self.if_use_act_target else self.act
        self.cri_optim = torch.optim.Adam(self.cri.parameters(), learning_rate)
        self.act_optim = torch.optim.Adam(self.act.parameters(), learning_rate) if self.ClassAct else self.cri
        del self.ClassCri, self.ClassAct# why del self.ClassCri and self.ClassAct here, to save memory?
    def select_action(self, state):
        states = torch.as_tensor((state,), dtype=torch.float32, device=self.device)
        actions, noises = self.act.get_action(states)
        return actions[0].detach().cpu().numpy(), noises[0].detach().cpu().numpy()
    def explore_env(self, env, target_step):
        trajectory_temp = list()
        state = self.state# sent the state to the agent and then agent sent the state to the method
        last_done = 0
        for i in range(target_step):#
            action, noise = self.select_action(state)
            state,next_state, reward, done,= env.step(np.tanh(action))# here the step of cut action is finally organized into the environment.
            trajectory_temp.append((state, reward, done, action, noise))
            if done:
                state = env.reset()
                last_done = i
            else:
                state = next_state
        self.state = state
        '''splice list'''
        trajectory_list = self.trajectory_list + trajectory_temp[:last_done + 1]# store 0 trajectory information to the list
        self.trajectory_list = trajectory_temp[last_done:]
        return trajectory_list # after this function it return trajectory list
    def update_net(self, buffer, batch_size, repeat_times, soft_update_tau):
        '''put data extract and update network together'''
        with torch.no_grad():
            buf_len = buffer[0].shape[0]
            buf_state, buf_action, buf_noise, buf_reward, buf_mask = [ten.to(self.device) for ten in buffer]# decompose buffer data
            # (ten_state, ten_action, ten_noise, ten_reward, ten_mask) = buffer
            '''get buf_r_sum, buf_logprob'''
            bs = 4096  # set a smaller 'BatchSize' when out of GPU memory.# 1024# could change to 4096
            buf_value = [self.cri_target(buf_state[i:i + bs]) for i in range(0, buf_len, bs)]#
            buf_value = torch.cat(buf_value, dim=0)
            buf_logprob = self.act.get_old_logprob(buf_action, buf_noise)
            buf_r_sum, buf_advantage = self.get_reward_sum(buf_len, buf_reward, buf_mask, buf_value)  # detach()
            # normalize advantage
            buf_advantage = (buf_advantage - buf_advantage.mean()) / (buf_advantage.std() + 1e-5)
            del buf_noise, buffer[:]
        '''PPO: Surrogate objective of Trust Region'''
        obj_critic = obj_actor = None
        for _ in range(int(buf_len / batch_size * repeat_times)):
            indices = torch.randint(buf_len, size=(batch_size,), requires_grad=False, device=self.device)
            state = buf_state[indices]
            action = buf_action[indices]
            r_sum = buf_r_sum[indices]
            logprob = buf_logprob[indices]
            advantage = buf_advantage[indices]
            new_logprob, obj_entropy = self.act.get_logprob_entropy(state, action)  # it is obj_actor
            ratio = (new_logprob - logprob.detach()).exp()
            surrogate1 = advantage * ratio
            surrogate2 = advantage * ratio.clamp(1 - self.ratio_clip, 1 + self.ratio_clip)
            obj_surrogate = -torch.min(surrogate1, surrogate2).mean()
            obj_actor = obj_surrogate + obj_entropy * self.lambda_entropy
            self.optim_update(self.act_optim, obj_actor)# update actor
            value = self.cri(state).squeeze(1)  # critic network predicts the reward_sum (Q value) of state
            # obj_critic = self.criterion(value, r_sum) / (r_sum.std() + 1e-6)#use smoothloss L1 to evaluate the value loss
            obj_critic = self.criterion(value, r_sum)
            self.optim_update(self.cri_optim, obj_critic)#calculate and update the back propogation of value loss
            self.soft_update(self.cri_target, self.cri, soft_update_tau) if self.cri_target is not self.cri else None# choose whether to use soft update
        a_std_log = getattr(self.act, 'a_std_log', torch.zeros(1))
        return obj_critic.item(), obj_actor.item(), a_std_log.mean().item()  # logging_tuple
    def get_reward_sum_raw(self, buf_len, buf_reward, buf_mask, buf_value) -> (torch.Tensor, torch.Tensor):
        buf_r_sum = torch.empty(buf_len, dtype=torch.float32, device=self.device)  # reward sum
        pre_r_sum = 0
        for i in range(buf_len - 1, -1, -1):
            buf_r_sum[i] = buf_reward[i] + buf_mask[i] * pre_r_sum
            pre_r_sum = buf_r_sum[i]
        buf_advantage = buf_r_sum - (buf_mask * buf_value[:, 0])
        return buf_r_sum, buf_advantage
    def get_reward_sum_gae(self, buf_len, ten_reward, ten_mask, ten_value) -> (torch.Tensor, torch.Tensor):
        buf_r_sum = torch.empty(buf_len, dtype=torch.float32, device=self.device)  # old policy value
        buf_advantage = torch.empty(buf_len, dtype=torch.float32, device=self.device)  # advantage value
        pre_r_sum = 0
        pre_advantage = 0  # advantage value of previous step
        for i in range(buf_len - 1, -1, -1):
            buf_r_sum[i] = ten_reward[i] + ten_mask[i] * pre_r_sum
            pre_r_sum = buf_r_sum[i]
            buf_advantage[i] = ten_reward[i] + ten_mask[i] * (pre_advantage - ten_value[i])  # fix a bug here
            pre_advantage = ten_value[i] + buf_advantage[i] * self.lambda_gae_adv
        return buf_r_sum, buf_advantage
    @staticmethod
    def optim_update(optimizer, objective):
        optimizer.zero_grad()
        objective.backward()
        optimizer.step()
    @staticmethod
    def soft_update(target_net, current_net, tau):
        for tar, cur in zip(target_net.parameters(), current_net.parameters()):
            tar.data.copy_(cur.data.__mul__(tau) + tar.data.__mul__(1.0 - tau))

"""**Build the Agent's PPO Update Buffer**"""

def update_buffer(_trajectory):
    _trajectory = list(map(list, zip(*_trajectory)))  # 2D-list transpose, here cut the trajectory into 5 parts
    ten_state = torch.as_tensor(_trajectory[0])#tensor state here
    # ten_reward = torch.as_tensor(_trajectory[1], dtype=torch.float32) * reward_scale# tensor reward here
    ten_reward = torch.as_tensor(_trajectory[1], dtype=torch.float32)
    ten_mask = (1.0 - torch.as_tensor(_trajectory[2], dtype=torch.float32)) * gamma  # _trajectory[2] = done, replace done by mask, save memory
    ten_action = torch.as_tensor(_trajectory[3])
    ten_noise = torch.as_tensor(_trajectory[4], dtype=torch.float32)
    buffer[:] = (ten_state, ten_action, ten_noise, ten_reward, ten_mask)#list store tensors
    _steps = ten_reward.shape[0]# how many steps are collected in all trajectories
    _r_exp = ten_reward.mean()# the mean reward
    return _steps, _r_exp

"""**Test the PPO Function**"""

if __name__ == '__main__':
    args = Arguments()
    '''here record real unbalance'''
    reward_record = {'episode': [], 'steps': [], 'mean_episode_reward': [], 'unbalance': [],
                     'episode_operation_cost': []}
    loss_record = {'episode': [], 'steps': [], 'critic_loss': [], 'actor_loss': [], 'entropy_loss': []}
    args.visible_gpu = '0'
    args.save_network=True
    args.test_network= True
    args.save_test_data=True
    args.compare_with_pyomo=True
    for seed in args.random_seed_list:
        args.random_seed = seed
        # set different seed
        args.agent = AgentPPO()
        agent_name = f'{args.agent.__class__.__name__}'
        args.agent.cri_target = True
        args.env_RL = ESSEnv()
        args.init_before_training(if_main=True)
        '''init agent and environment'''
        agent = args.agent
        env_RL = args.env_RL
        agent.init(args.net_dim, env_RL.state_space.shape[0], env_RL.action_space.shape[0],
                   args.learning_rate, args._if_per_or_gae)
        '''init buffer'''
        buffer = list()
        '''start training'''
        cwd = args.cwd
        gamma = args.gamma
        batch_size = args.batch_size  # how much data should be used to update net
        target_step = args.target_step  # how manysteps of one episode should stop
        repeat_times = args.repeat_times  # how many times should update for one batch size data
        soft_update_tau = args.soft_update_tau
        agent.state = env_RL.reset()
        '''collect data and train and update network'''
        num_episode = args.num_episode
        args.train=True
        args.save_network=True
        args.test_network= True
        args.save_test_data=True
        args.compare_with_pyomo=True
        if args.train:
            for i_episode in range(num_episode):
                with torch.no_grad():
                    trajectory_list = agent.explore_env(env_RL, target_step)
                    steps, r_exp = update_buffer(trajectory_list)
                critic_loss, actor_loss, entropy_loss = agent.update_net(
                    buffer, batch_size, repeat_times, soft_update_tau)
                loss_record['critic_loss'].append(critic_loss)
                loss_record['actor_loss'].append(actor_loss)
                loss_record['entropy_loss'].append(entropy_loss)
                with torch.no_grad():
                    episode_reward, episode_unbalance, episode_operation_cost = get_episode_return(
                        env_RL, agent.act, agent.device)
                    reward_record['mean_episode_reward'].append(episode_reward)
                    reward_record['unbalance'].append(episode_unbalance)
                    reward_record['episode_operation_cost'].append(episode_operation_cost)
                print(f'current epsiode is {i_episode}, reward:{episode_reward}, unbalance:{episode_unbalance}')
    if args.update_training_data:
        loss_record_path = f'{args.cwd}/loss_data_PPO.pkl'
        reward_record_path = f'{args.cwd}/reward_data_PPO.pkl'
        with open(loss_record_path, 'wb') as tf:
            pickle.dump(loss_record, tf)
        with open(reward_record_path, 'wb') as tf:
            pickle.dump(reward_record, tf)
    act_save_path = f'{args.cwd}/actor_PPO.pth'
    cri_save_path = f'{args.cwd}/critic_PPO.pth'
    print('training data have been saved')
    if args.save_network:
        torch.save(agent.act.state_dict(), act_save_path)
        torch.save(agent.cri.state_dict(), cri_save_path)
        print('training finished and actor and critic parameters have been saved')
    if args.test_network:
        args.cwd = agent_name
        agent.act.load_state_dict(torch.load(act_save_path))
        print('parameters have been reloaded and testing')
        record = test_one_episode(env_RL, agent.act, agent.device)
        eval_data_PPO = pd.DataFrame(record['information'])
        eval_data_PPO.columns = ['time_step', 'price', 'netload', 'action', 'real_action',
                             'soc', 'battery1', 'GAS', 'COAL', 'BIOMASS', 'NUCLEAR', 'HYDRO',
                             'IMPORTS', 'STORAGE', 'OTHER', 'unbalance', 'operation_cost']
    if args.save_test_data:
        test_data_save_path = f'{args.cwd}/test_data_PPO.pkl'
        with open(test_data_save_path, 'wb') as tf:
            pickle.dump(record, tf)
    '''compare with pyomo data and results'''
    if args.compare_with_pyomo:
        month = record['init_info'][0][0]
        day = record['init_info'][0][1]
        initial_soc = record['init_info'][0][3]
        print(initial_soc)

"""##**Results Analysis and Comparison For MINLP, MIP-DQN, and PPO**

**Obtain the Training Loss Data**
"""

datasource = '/content/AgentPPO/MIP_DQN_experiments/loss_data_PPO.pkl'
with open(datasource, 'rb') as tf:
    loss_data = pickle.load(tf)
print(loss_data.keys())
# Accessing data using keys
episode_value = loss_data['episode']
steps_value = loss_data['steps']
critic_loss_value = loss_data['critic_loss']
actor_loss_value = loss_data['actor_loss']
entropy_loss_value = loss_data['entropy_loss']
# Printing the values
print(f"Episode: {episode_value}")
print(f"Steps: {steps_value}")
print(f"Critic Loss: {critic_loss_value}")
print(f"Actor Loss: {actor_loss_value}")
print(f"Entropy Loss: {entropy_loss_value}")

"""**Plot the Critic and Actor Networks Loss Data**"""

plt.figure(figsize=(15, 9))
plt.gca().set_facecolor("white")
plt.gca().spines[['top', 'right']].set_visible(False)
episode_values = list(range(len(critic_loss_value)))
pd.Series(critic_loss_value, index=episode_values).plot(
    kind='line', color='sienna', label='Critic Network Loss', linewidth=3.0)
pd.Series(actor_loss_value, index=episode_values).plot(
    kind='line', color='violet', label='Actor Network Loss', linewidth=3.0)
plt.title('PPO Critic and Actor Networks Training Loss Data', size=25, fontweight='bold')
plt.xlabel('Episode', size=25, fontweight='bold')
plt.ylabel('Loss', size=25, fontweight='bold')
plt.xticks(rotation=30, size=15, fontweight='bold')
plt.yticks(rotation=0, ha='right', size=15, fontweight='bold')
ax = plt.gca()
# Set linewidth and edge color for each spine
for spine in ax.spines.values():
    spine.set_linewidth(2)
    spine.set_edgecolor('black')
ax.xaxis.grid(False)  # Remove x-axis grid lines
ax.yaxis.grid(True)
plt.legend(loc='best', ncol=1, frameon=True, fontsize='15',
           fancybox=True, framealpha=1, shadow=True, borderpad=2)
# Save the plot to a directory in Google Drive
output_dir = '/content/gdrive/MyDrive/Energy_System_RL_Modelling_Plots'
plt.savefig(f'{output_dir}/Critic_and_Actor_Networks_Training_Loss_Data_PPO.png')
plt.tight_layout()
plt.show()

"""**Obtain the Training Reward Data**"""

datasource = '/content/AgentPPO/MIP_DQN_experiments/reward_data_PPO.pkl'
with open(datasource, 'rb') as tf:
    reward_data = pickle.load(tf)
print(reward_data.keys())
# Accessing data using keys
episode_value = reward_data['episode']
steps_value = reward_data['steps']
mean_episode_reward_value = reward_data['mean_episode_reward']
unbalance_value = reward_data['unbalance']
episode_operation_cost_value = reward_data['episode_operation_cost']
# Printing the values
print(f"Episode: {episode_value}")
print(f"Steps: {steps_value}")
print(f"Mean Episode Reward: {mean_episode_reward_value}")
print(f"unbalance: {unbalance_value}")
print(f"Episode Operation Cost: {episode_operation_cost_value}")

"""**Plot the Training Mean Episode, Unbalance, and Operation Cost Loss Data**"""

plt.figure(figsize=(15, 9))
plt.gca().set_facecolor("white")
plt.gca().spines[['top', 'right']].set_visible(False)
episode_values = list(range(len(mean_episode_reward_value)))
pd.Series(mean_episode_reward_value, index=episode_values).plot(
    kind='line', color='maroon', label='Mean Episode Reward Loss', linewidth=3.0)
pd.Series(unbalance_value, index=episode_values).plot(
    kind='line', color='navy', label='Unbalance Loss', linewidth=3.0)
pd.Series(episode_operation_cost_value, index=episode_values).plot(
    kind='line', color='green', label='Episode Operation Cost Loss', linewidth=3.0)
plt.title('PPO Training Mean Episode, Unbalance, and Operation Cost Loss Data',
          size=25, fontweight='bold')
plt.xlabel('Episode', size=25, fontweight='bold')
plt.ylabel('Loss', size=25, fontweight='bold')
plt.xticks(rotation=30, size=15, fontweight='bold')
plt.yticks(rotation=0, ha='right', size=15, fontweight='bold')
plt.yscale('symlog')
ax = plt.gca()
# Set linewidth and edge color for each spine
for spine in ax.spines.values():
    spine.set_linewidth(2)
    spine.set_edgecolor('black')
ax.xaxis.grid(False)  # Remove x-axis grid lines
ax.yaxis.grid(True)
plt.legend(loc='best', ncol=1, frameon=True, fontsize='15',
           fancybox=True, framealpha=1, shadow=True, borderpad=2)
# Save the plot to a directory in Google Drive
output_dir = '/content/gdrive/MyDrive/Energy_System_RL_Modelling_Plots'
plt.savefig(f'{output_dir}/Training_Mean_Episode_Unbalance_and_Operation_Cost_Loss_Data_PPO.png')
plt.tight_layout()
plt.show()

"""**Obtain the Test Data Results**"""

datasource = '/content/AgentPPO/test_data_PPO.pkl'
with open(datasource, 'rb') as tf:
    test_data = pickle.load(tf)
print(test_data.keys())
# Accessing data using keys
init_info_value = test_data['init_info']
information_value = test_data['information']
state_value = test_data['state']
action_value = test_data['action']
reward_value = test_data['reward']
cost_value = test_data['cost']
unbalance_value = test_data['unbalance']
record_output_value = test_data['record_output']
# Printing the values
print(f"Initial Information: {init_info_value}")
print(f"Information: {information_value}")
print(f"State: {state_value}")
print(f"Action: {action_value}")
print(f"Reward: {reward_value}")
print(f"Cost: {cost_value}")
print(f"Unbalance: {unbalance_value}")
print(f"Record Output: {record_output_value}")
eval_data_PPO = pd.DataFrame(test_data['information'])
eval_data_PPO.columns = ['time_step', 'price', 'netload', 'action', 'real_action',
                     'soc', 'battery1', 'GAS', 'COAL', 'BIOMASS', 'NUCLEAR', 'HYDRO',
                     'IMPORTS', 'STORAGE', 'OTHER', 'unbalance', 'operation_cost']
eval_data_PPO

"""**Plot Unbalance of Plant Generation For PPO and MIP-DQN**"""

sns.set_theme(style='whitegrid')
plt.rcParams["figure.figsize"] = (16, 9)
fig, axs = plt.subplots(1, 1)
plt.subplots_adjust(wspace=0.7, hspace=0.3)
plt.autoscale(tight=True)
axs.cla()  # Use axs, not axs[0, 0]
axs.set_ylabel('Unbalance of Generation')
fixed_date = datetime(2022, 1, 1)
period = [fixed_date + timedelta(hours=i) for i in range(24)]
axs.bar(period, eval_data_PPO['unbalance'],
        label='Exchange with Grid-PPO', width=0.02, color = 'sienna', align = 'center')
axs.bar(period, eval_data['unbalance'],
        label='Exchange with Grid-MIP-DQN', width=0.02, color = 'gold', align = 'edge')
plt.title(f'Unbalance of Generation - PPO Vs MIP-DQN {day}/{month}/2022',
          size=25, fontweight='bold')
plt.ylabel('MW', size=25, fontweight='bold')
plt.xlabel('Time (Hour)', size=25, fontweight='bold')
plt.xticks(rotation=30, size=15, fontweight='bold')
plt.yticks(rotation=0, ha='right', size=15, fontweight='bold')
ax = plt.gca()
# Set linewidth and edge color for each spine
for spine in ax.spines.values():
    spine.set_linewidth(2)
    spine.set_edgecolor('black')
ax.xaxis.grid(False)  # Remove x-axis grid lines
ax.yaxis.grid(True)
plt.legend(loc='best', ncol=1, frameon=True, fontsize='15',
           fancybox=True, framealpha=1, shadow=True, borderpad=2)
# Save the plot to a directory in Google Drive
output_dir = '/content/gdrive/MyDrive/Energy_System_RL_Modelling_Plots'
plt.savefig(f'{output_dir}/Unbalance_of_Generation_PPO.png')
plt.tight_layout()
plt.show()

"""**Plot Net Load For All Models (MIP-DQN, PPO & MINLP)**"""

sns.set_theme(style='whitegrid')
plt.rcParams["figure.figsize"] = (16, 9)
fig, axs = plt.subplots(1, 1)
plt.subplots_adjust(wspace=0.7, hspace=0.3)
plt.autoscale(tight=True)
axs.cla()  # Use axs, not axs[0, 0]
fixed_date = datetime(2022, 1, 1)
period = [fixed_date + timedelta(hours=i) for i in range(24)]
axs.bar(period, eval_data_PPO['netload'], label = 'Netload - PPO', width = 0.01,
        color='orange', align='edge')
axs.bar(period, eval_data['netload'], label='Netload - MIP-DQN',
         color = 'crimson', width=0.01, align='center')
axs.bar(period, base_result['netload'], label='Netload - MINLP',
         color = 'navy', width=0.01, align='edge')
axs.xaxis_date()
axs.xaxis.set_major_formatter(plt.matplotlib.dates.DateFormatter('%H:%M'))
plt.title(f'Comparison of Net Load for All Models {day}/{month}/2022',
          size=35, fontweight='bold')
plt.ylabel('Netload (MWh)', size=25, fontweight='bold')
plt.xlabel('Time (Hour)', size=25, fontweight='bold')
plt.xticks(rotation=30, size=15, fontweight='bold')
plt.yticks(rotation=0, ha='right', size=15, fontweight='bold')
ax = plt.gca()
# Set linewidth and edge color for each spine
for spine in ax.spines.values():
    spine.set_linewidth(2)
    spine.set_edgecolor('black')
ax.xaxis.grid(False)  # Remove x-axis grid lines
ax.yaxis.grid(True)
axs.set_yscale("log")
axs.legend(loc='best', ncol=1, frameon=True, fontsize='15', labelspacing = 0.5,
           fancybox=True, framealpha=1, shadow=True, borderpad=2)
# Save the plot to a directory in Google Drive
output_dir = '/content/gdrive/MyDrive/Energy_System_RL_Modelling_Plots'
plt.savefig(f'{output_dir}/Comparison_of_both_models_NetLoad_PPO_MIP-DQN_MINLP.png')
plt.tight_layout()
plt.show()

"""**Plot Operation Cost for All Models**"""

sns.set_theme(style='whitegrid')
plt.rcParams["figure.figsize"] = (16, 9)
fig, axs = plt.subplots(1, 1)
plt.subplots_adjust(wspace=0.7, hspace=0.3)
plt.autoscale(tight=True)
fixed_date = datetime(2022, 1, 1)
period = [fixed_date + timedelta(hours=i) for i in range(24)]
axs.cla()
axs.bar(period, eval_data['operation_cost'], label = 'Operation Cost - MIP-DQN',
        width = 0.02, color='lightblue', align='edge')
axs.bar(period, eval_data_PPO['operation_cost'], label = 'Operation Cost - PPO',
              width = 0.02, color='lightcoral', align='center')
axs.bar(period, base_result['step_cost'], label = 'Operation Cost - MINLP',
              width = 0.02, color='gray', align='edge')
axs.xaxis_date()
axs.xaxis.set_major_formatter(plt.matplotlib.dates.DateFormatter('%H:%M'))
plt.title(f'Operation Cost Comparison For All Models {day}/{month}/2022',
          size=25, fontweight='bold')
plt.ylabel('£/MWh', size=25, fontweight='bold')
plt.xlabel('Time (Hour)', size=25, fontweight='bold')
plt.xticks(rotation=30, size=15, fontweight='bold')
plt.yticks(rotation=0, ha='right', size=15, fontweight='bold')
ax = plt.gca()
# Set linewidth and edge color for each spine
for spine in ax.spines.values():
    spine.set_linewidth(2)
    spine.set_edgecolor('black')
ax.xaxis.grid(False)  # Remove x-axis grid lines
ax.yaxis.grid(True)
axs.legend(loc='best', ncol=1, frameon=True, fontsize='15', labelspacing = 0.5,
           fancybox=True, framealpha=1, shadow=True, borderpad=2)
# Save the plot to a directory in Google Drive
output_dir = '/content/gdrive/MyDrive/Energy_System_RL_Modelling_Plots'
plt.savefig(f'{output_dir}/Cost_MIP_DQN_PPO_MINLP.png')
plt.tight_layout()
plt.show()

"""**Plot the Energy Price For All Models**"""

sns.set_theme(style='whitegrid')
plt.rcParams["figure.figsize"] = (16, 9)
fig, axs = plt.subplots(1, 1)
plt.subplots_adjust(wspace=0.7, hspace=0.3)
plt.autoscale(tight=True)
axs.cla()  # Use axs, not axs[0, 0]
fixed_date = datetime(2022, 1, 1)
period = [fixed_date + timedelta(hours=i) for i in range(24)]
axs.plot(period, eval_data['price'], drawstyle='steps-mid', label = 'Price - MIP-DQN',
         color = 'purple', linewidth=7.0, marker='o')
axs.plot(period, eval_data_PPO['price'], drawstyle='steps-mid',
               label='Price - PPO',  color = 'navy', linewidth=7.0)
axs.plot(period, base_result['price'], drawstyle='steps-mid',
               label='Price - MINLP',  color = 'crimson', linewidth=7.0)
axs.legend(loc='upper left', fontsize = 12, frameon = False, labelspacing = 0.5)
axs.xaxis_date()
axs.xaxis.set_major_formatter(plt.matplotlib.dates.DateFormatter('%H:%M'))
plt.title(f'Energy Price for All Models {day}/{month}/2022',
          size=25, fontweight='bold')
plt.ylabel('Price (£/MWh)', size=25, fontweight='bold')
plt.xlabel('Time (Hour)', size=25, fontweight='bold')
plt.xticks(rotation=30, size=15, fontweight='bold')
plt.yticks(rotation=0, ha='right', size=15, fontweight='bold')
ax = plt.gca()
# Set linewidth and edge color for each spine
for spine in ax.spines.values():
    spine.set_linewidth(2)
    spine.set_edgecolor('black')
ax.xaxis.grid(False)  # Remove x-axis grid lines
ax.yaxis.grid(True)
axs.legend(loc='best', ncol=1, frameon=True, fontsize='15', labelspacing = 0.5,
           fancybox=True, framealpha=1, shadow=True, borderpad=2)
# Save the plot to a directory in Google Drive
output_dir = '/content/gdrive/MyDrive/Energy_System_RL_Modelling_Plots'
plt.savefig(f'{output_dir}/Energy_Price_MIP-DQN_PPO_MINLP.png')
plt.tight_layout()
plt.show()

"""**State of Charge (SOC) for All Models**"""

sns.set_theme(style='whitegrid')
plt.rcParams["figure.figsize"] = (16, 9)
fig, axs = plt.subplots(1, 1)
plt.subplots_adjust(wspace=0.7, hspace=0.3)
plt.autoscale(tight=True)
axs.cla()  # Use axs, not axs[0, 0]
fixed_date = datetime(2022, 1, 1)
period = [fixed_date + timedelta(hours=i) for i in range(24)]
axs.plot(period, eval_data['soc'], drawstyle='steps-mid', label='SOC - MIP-DQN',
         color='green', linewidth=3.0)
axs.plot(period, eval_data_PPO['soc'], drawstyle='steps-mid', label='SOC - PPO',
         color='maroon', linewidth=3.0)
axs.plot(period, base_result['soc'], drawstyle='steps-mid', label='SOC - MINLP',
         color='blue', linewidth=3.0)
axs.legend(loc='upper left', fontsize = 12, frameon = False, labelspacing = 0.5)
axs.xaxis_date()
axs.xaxis.set_major_formatter(plt.matplotlib.dates.DateFormatter('%H:%M'))
plt.title(f'State of Charge (SOC) for All Models {day}/{month}/2022', size=25, fontweight='bold')
plt.xlabel('SOC', size=25, fontweight='bold')
plt.ylabel('Time (Hour)', size=25, fontweight='bold')
plt.xticks(rotation=30, size=15, fontweight='bold')
plt.yticks(rotation=0, ha='right', size=15, fontweight='bold')
ax = plt.gca()
# Set linewidth and edge color for each spine
for spine in ax.spines.values():
    spine.set_linewidth(2)
    spine.set_edgecolor('black')
ax.xaxis.grid(False)  # Remove x-axis grid lines
ax.yaxis.grid(True)
axs.legend(loc='best', ncol=1, frameon=True, fontsize='15', labelspacing = 0.5,
           fancybox=True, framealpha=1, shadow=True, borderpad=2)
# Save the plot to a directory in Google Drive
output_dir = '/content/gdrive/MyDrive/Energy_System_RL_Modelling_Plots'
plt.savefig(f'{output_dir}/SOC_MIP-DQN_PPO_MINLP.png')
plt.tight_layout()
plt.show()

"""**Compare the Different Cost Among All Models**"""

'''compare the different cost get from PPO and MIP-DQN'''
#ratio = sum(eval_data['operation_cost']) / sum(eval_data_PPO['operation_cost'])
MIP_DQN_Model_Op_Cost = sum(eval_data['operation_cost'])
PPO_Model_Op_Cost = sum(eval_data_PPO['operation_cost'])
MINLP_Model_Op_Cost = sum(base_result['step_cost'])
cheapest_model_cost, cheapest_model_name = min(
    (MIP_DQN_Model_Op_Cost, 'MIP-DQN'),
    (PPO_Model_Op_Cost, 'PPO'),
    (MINLP_Model_Op_Cost, 'MINLP'),
    key=lambda x: x[0]
)
print(f'Month is: {month}, Day is:{day}, Initial SOC is:{initial_soc:.3f}')
print(f'MIP-DQN Model Operation Cost is: {MIP_DQN_Model_Op_Cost:.3f}')
print(f'PPO Model Operation Cost is: {PPO_Model_Op_Cost:.3f}')
print(f'MINLP Model Operation Cost is: {MINLP_Model_Op_Cost:.3f}')
print(f'The Cheapest Model Operation Cost is: {cheapest_model_cost:.3f} (Model: {cheapest_model_name})')
#print(f'Ratio of MIP_DQN and PPO Model Operation Cost is: {ratio}')

"""#**Build the Soft Actor-Critic Policy optimization (SAC)**

**Build The Actor and Critic Modules for SAC Model**
"""

class ActorSAC(nn.Module):
    def __init__(self, mid_dim, state_dim, action_dim):
        super().__init__()
        self.net_state = nn.Sequential(nn.Linear(state_dim, mid_dim), nn.ReLU(),
                                       nn.Linear(mid_dim, mid_dim), nn.ReLU(), )
        self.net_a_avg = nn.Sequential(nn.Linear(mid_dim, mid_dim), nn.Hardswish(),
                                       nn.Linear(mid_dim, action_dim))  # the average of action
        self.net_a_std = nn.Sequential(nn.Linear(mid_dim, mid_dim), nn.Hardswish(),
                                       nn.Linear(mid_dim, action_dim))  # the log_std of action
        self.log_sqrt_2pi = np.log(np.sqrt(2 * np.pi))
    def forward(self, state):
        tmp = self.net_state(state)
        return self.net_a_avg(tmp).tanh()  # action
    def get_action(self, state):
        t_tmp = self.net_state(state)
        a_avg = self.net_a_avg(t_tmp)  # NOTICE! it is a_avg without .tanh()
        a_std = self.net_a_std(t_tmp).clamp(-20, 2).exp()
        return torch.normal(a_avg, a_std).tanh()  # re-parameterize
    def get_action_logprob(self, state):
        t_tmp = self.net_state(state)
        a_avg = self.net_a_avg(t_tmp)  # NOTICE! it needs a_avg.tanh()
        a_std_log = self.net_a_std(t_tmp).clamp(-20, 2)
        a_std = a_std_log.exp()
        noise = torch.randn_like(a_avg, requires_grad=True)
        a_tan = (a_avg + a_std * noise).tanh()  # action.tanh()
        log_prob = a_std_log + self.log_sqrt_2pi + noise.pow(2).__mul__(0.5)  # noise.pow(2) * 0.5
        log_prob = log_prob + (-a_tan.pow(2) + 1.000001).log()  # fix log_prob using the derivative of action.tanh()
        return a_tan, log_prob.sum(1, keepdim=True)
class CriticTwin(nn.Module):  # shared parameter
    def __init__(self, mid_dim, state_dim, action_dim):
        super().__init__()
        self.net_sa = nn.Sequential(nn.Linear(state_dim + action_dim, mid_dim), nn.ReLU(),
                                    nn.Linear(mid_dim, mid_dim), nn.ReLU())  # concat(state, action)
        self.net_q1 = nn.Sequential(nn.Linear(mid_dim, mid_dim), nn.Hardswish(),
                                    nn.Linear(mid_dim, 1))  # q1 value
        self.net_q2 = nn.Sequential(nn.Linear(mid_dim, mid_dim), nn.Hardswish(),
                                    nn.Linear(mid_dim, 1))  # q2 value
    def forward(self, state, action):
        tmp = self.net_sa(torch.cat((state, action), dim=1))
        return self.net_q1(tmp)  # one Q value
    def get_q1_q2(self, state, action):
        tmp = self.net_sa(torch.cat((state, action), dim=1))
        return self.net_q1(tmp), self.net_q2(tmp)  # two Q values

"""**Build the Agent's Base to Select Actions and Explore the Environment for SAC**"""

class AgentSAC(AgentBase):
    def __init__(self):
        super().__init__()
        self.ClassCri = CriticTwin
        self.ClassAct = ActorSAC
        self.if_use_cri_target = True
        self.if_use_act_target = False
        self.alpha_log = None
        self.alpha_optim = None
        self.target_entropy = None
    def init(self, net_dim, state_dim, action_dim, learning_rate=1e-4, _if_use_per=False, gpu_id=0, env_num=1):
        super().init(net_dim, state_dim, action_dim, learning_rate, _if_use_per, gpu_id)
        self.alpha_log = torch.tensor((-np.log(action_dim) * np.e,), dtype=torch.float32,
                                      requires_grad=True, device=self.device)  # trainable parameter
        self.alpha_optim = torch.optim.Adam((self.alpha_log,), lr=learning_rate)
        self.target_entropy = np.log(action_dim)
    def select_action(self, state):
        states = torch.as_tensor((state,), dtype=torch.float32, device=self.device)
        actions = self.act.get_action(states)
        return actions.detach().cpu().numpy()[0]
    def explore_env(self, env, target_step):
        trajectory = list()
        state = self.state
        for _ in range(target_step):
            action = self.select_action(state)
            state,next_state, reward, done, = env.step(action)
            trajectory.append((state, (reward, done, *action)))
            state = env.reset() if done else next_state
        self.state = state
        return trajectory
    def update_net(self, buffer, batch_size, repeat_times, soft_update_tau):
        buffer.update_now_len()
        alpha = self.alpha_log.exp().detach()
        obj_critic = obj_actor = None
        for _ in range(int(buffer.now_len * repeat_times / batch_size)):
            '''objective of critic (loss function of critic)'''
            with torch.no_grad():
                reward, mask, action, state, next_s = buffer.sample_batch(batch_size)
                next_a, next_log_prob = self.act_target.get_action_logprob(next_s)
                next_q = torch.min(*self.cri_target.get_q1_q2(next_s, next_a))
                q_label = reward + mask * (next_q + next_log_prob * alpha)
            q1, q2 = self.cri.get_q1_q2(state, action)
            obj_critic = self.criterion(q1, q_label) + self.criterion(q2, q_label)
            self.optim_update(self.cri_optim, obj_critic)
            self.soft_update(self.cri_target, self.cri, soft_update_tau)
            '''objective of alpha (temperature parameter automatic adjustment)'''
            action_pg, log_prob = self.act.get_action_logprob(state)  # policy gradient
            obj_alpha = (self.alpha_log * (log_prob - self.target_entropy).detach()).mean()
            self.optim_update(self.alpha_optim, obj_alpha)
            '''objective of actor'''
            alpha = self.alpha_log.exp().detach()
            with torch.no_grad():
                self.alpha_log[:] = self.alpha_log.clamp(-20, 2)
            obj_actor = -(torch.min(*self.cri_target.get_q1_q2(state, action_pg)) + log_prob * alpha).mean()
            self.optim_update(self.act_optim, obj_actor)
            self.soft_update(self.act_target, self.act, soft_update_tau)
        return obj_critic.item(), obj_actor.item(), alpha.item()

"""**Build the Agent's SAC Update Buffer**"""

def update_buffer(_trajectory):
    ten_state = torch.as_tensor([item[0] for item in _trajectory], dtype=torch.float32)
    ary_other = torch.as_tensor([item[1] for item in _trajectory])
    ary_other[:, 0] = ary_other[:, 0]   # ten_reward
    ary_other[:, 1] = (1.0 - ary_other[:, 1]) * gamma  # ten_mask = (1.0 - ary_done) * gamma
    buffer.extend_buffer(ten_state, ary_other)
    _steps = ten_state.shape[0]
    _r_exp = ary_other[:, 0].mean()  # other = (reward, mask, action)
    return _steps, _r_exp

"""**Test the SAC Function**"""

if __name__ == '__main__':
    args = Arguments()
    '''here record real unbalance'''
    reward_record = {'episode': [], 'steps': [], 'mean_episode_reward': [], 'unbalance': [],
                     'episode_operation_cost': []}
    loss_record = {'episode': [], 'steps': [], 'critic_loss': [], 'actor_loss': [], 'entropy_loss': []}
    args.visible_gpu = '0'
    args.save_network=True
    args.test_network= True
    args.save_test_data=True
    args.compare_with_pyomo=True
    for seed in args.random_seed_list:
        args.random_seed = seed
        # set different seed
        args.agent = AgentSAC()
        agent_name = f'{args.agent.__class__.__name__}'
        args.agent.cri_target = True
        args.env_RL = ESSEnv()
        args.init_before_training(if_main=True)
        '''init agent and environment'''
        agent = args.agent
        env_RL = args.env_RL
        agent.init(args.net_dim, env_RL.state_space.shape[0], env_RL.action_space.shape[0],
                   args.learning_rate, args._if_per_or_gae)
        '''init replay buffer'''
        buffer = ReplayBuffer(max_len=args.max_memo, state_dim=env_RL.state_space.shape[0],
                              action_dim= env_RL.action_space.shape[0])
        '''start training'''
        cwd = args.cwd
        gamma = args.gamma
        batch_size = args.batch_size  # how much data should be used to update net
        target_step = args.target_step  # how manysteps of one episode should stop
        repeat_times = args.repeat_times  # how many times should update for one batch size data
        soft_update_tau = args.soft_update_tau
        agent.state = env_RL.reset()
        '''collect data and train and update network'''
        num_episode = args.num_episode
        args.train=True
        args.save_network=True
        args.test_network= True
        args.save_test_data=True
        args.compare_with_pyomo=True
        if args.train:
            collect_data = True
            while collect_data:
                print(f'buffer:{buffer.now_len}')
                with torch.no_grad():
                    trajectory = agent.explore_env(env_RL, target_step)
                    steps, r_exp = update_buffer(trajectory)
                    buffer.update_now_len()
                if buffer.now_len >= 10000:
                    collect_data = False
            for i_episode in range(num_episode):
                critic_loss, actor_loss, entropy_loss = agent.update_net(
                    buffer, batch_size, repeat_times, soft_update_tau)
                loss_record['critic_loss'].append(critic_loss)
                loss_record['actor_loss'].append(actor_loss)
                loss_record['entropy_loss'].append(entropy_loss)
                with torch.no_grad():
                    episode_reward, episode_unbalance, episode_operation_cost = get_episode_return(
                        env_RL, agent.act, agent.device)
                    reward_record['mean_episode_reward'].append(episode_reward)
                    reward_record['unbalance'].append(episode_unbalance)
                    reward_record['episode_operation_cost'].append(episode_operation_cost)
                print(f'current epsiode is {i_episode}, reward:{episode_reward}, unbalance:{episode_unbalance}, buffer_length: {buffer.now_len}')
                if i_episode % 10==0:
                # target_step
                    with torch.no_grad():
                        trajectory = agent.explore_env(env_RL, target_step)
                        steps, r_exp = update_buffer(trajectory)
    if args.update_training_data:
        loss_record_path = f'{args.cwd}/loss_data_SAC.pkl'
        reward_record_path = f'{args.cwd}/reward_data_SAC.pkl'
        with open(loss_record_path, 'wb') as tf:
            pickle.dump(loss_record, tf)
        with open(reward_record_path, 'wb') as tf:
            pickle.dump(reward_record, tf)
    act_save_path = f'{args.cwd}/actor_SAC.pth'
    cri_save_path = f'{args.cwd}/critic_SAC.pth'
    print('training data have been saved')
    if args.save_network:
        torch.save(agent.act.state_dict(), act_save_path)
        torch.save(agent.cri.state_dict(), cri_save_path)
        print('training finished and actor and critic parameters have been saved')
    if args.test_network:
        args.cwd = agent_name
        agent.act.load_state_dict(torch.load(act_save_path))
        print('parameters have been reloaded and testing')
        record = test_one_episode(env_RL, agent.act, agent.device)
        eval_data_SAC = pd.DataFrame(record['information'])
        eval_data_SAC.columns = ['time_step', 'price', 'netload', 'action', 'real_action',
                             'soc', 'battery1', 'GAS', 'COAL', 'BIOMASS', 'NUCLEAR', 'HYDRO',
                             'IMPORTS', 'STORAGE', 'OTHER', 'unbalance', 'operation_cost']
    if args.save_test_data:
        test_data_save_path = f'{args.cwd}/test_data_SAC.pkl'
        with open(test_data_save_path, 'wb') as tf:
            pickle.dump(record, tf)
    '''compare with pyomo data and results'''
    if args.compare_with_pyomo:
        month = record['init_info'][0][0]
        day = record['init_info'][0][1]
        initial_soc = record['init_info'][0][3]
        print(initial_soc)

"""##**Results Analysis and Comparison For MINLP, MIP-DQN, PPO, and SAC**

**Obtain the Training Loss Data**
"""

datasource = '/content/AgentSAC/MIP_DQN_experiments/loss_data_SAC.pkl'
with open(datasource, 'rb') as tf:
    loss_data = pickle.load(tf)
print(loss_data.keys())
# Accessing data using keys
episode_value = loss_data['episode']
steps_value = loss_data['steps']
critic_loss_value = loss_data['critic_loss']
actor_loss_value = loss_data['actor_loss']
entropy_loss_value = loss_data['entropy_loss']
# Printing the values
print(f"Episode: {episode_value}")
print(f"Steps: {steps_value}")
print(f"Critic Loss: {critic_loss_value}")
print(f"Actor Loss: {actor_loss_value}")
print(f"Entropy Loss: {entropy_loss_value}")

"""**Plot the Critic and Actor Networks Loss Data**"""

plt.figure(figsize=(15, 9))
plt.gca().set_facecolor("white")
plt.gca().spines[['top', 'right']].set_visible(False)
episode_values = list(range(len(critic_loss_value)))
pd.Series(critic_loss_value, index=episode_values).plot(
    kind='line', color='sienna', label='Critic Network Loss', linewidth=3.0)
pd.Series(actor_loss_value, index=episode_values).plot(
    kind='line', color='violet', label='Actor Network Loss', linewidth=3.0)
plt.title('SAC Critic and Actor Networks Training Loss Data', size=25, fontweight='bold')
plt.xlabel('Episode', size=25, fontweight='bold')
plt.ylabel('Loss', size=25, fontweight='bold')
plt.xticks(rotation=30, size=15, fontweight='bold')
plt.yticks(rotation=0, ha='right', size=15, fontweight='bold')
ax = plt.gca()
# Set linewidth and edge color for each spine
for spine in ax.spines.values():
    spine.set_linewidth(2)
    spine.set_edgecolor('black')
ax.xaxis.grid(False)  # Remove x-axis grid lines
ax.yaxis.grid(True)
plt.legend(loc='best', ncol=1, frameon=True, fontsize='15',
           fancybox=True, framealpha=1, shadow=True, borderpad=2)
# Save the plot to a directory in Google Drive
output_dir = '/content/gdrive/MyDrive/Energy_System_RL_Modelling_Plots'
plt.savefig(f'{output_dir}/Critic_and_Actor_Networks_Training_Loss_Data_SAC.png')
plt.tight_layout()
plt.show()

"""**Obtain the Training Reward Data**"""

datasource = '/content/AgentSAC/MIP_DQN_experiments/reward_data_SAC.pkl'
with open(datasource, 'rb') as tf:
    reward_data = pickle.load(tf)
print(reward_data.keys())
# Accessing data using keys
episode_value = reward_data['episode']
steps_value = reward_data['steps']
mean_episode_reward_value = reward_data['mean_episode_reward']
unbalance_value = reward_data['unbalance']
episode_operation_cost_value = reward_data['episode_operation_cost']
# Printing the values
print(f"Episode: {episode_value}")
print(f"Steps: {steps_value}")
print(f"Mean Episode Reward: {mean_episode_reward_value}")
print(f"unbalance: {unbalance_value}")
print(f"Episode Operation Cost: {episode_operation_cost_value}")

"""**Plot the Training Mean Episode, Unbalance, and Operation Cost Loss Data**"""

plt.figure(figsize=(15, 9))
plt.gca().set_facecolor("white")
plt.gca().spines[['top', 'right']].set_visible(False)
episode_values = list(range(len(mean_episode_reward_value)))
pd.Series(mean_episode_reward_value, index=episode_values).plot(
    kind='line', color='maroon', label='Mean Episode Reward Loss', linewidth=3.0)
pd.Series(unbalance_value, index=episode_values).plot(
    kind='line', color='navy', label='Unbalance Loss', linewidth=3.0)
pd.Series(episode_operation_cost_value, index=episode_values).plot(
    kind='line', color='green', label='Episode Operation Cost Loss', linewidth=3.0)
plt.title('SAC Training Mean Episode, Unbalance, and Operation Cost Loss Data',
          size=25, fontweight='bold')
plt.xlabel('Episode', size=25, fontweight='bold')
plt.ylabel('Loss', size=25, fontweight='bold')
plt.xticks(rotation=30, size=15, fontweight='bold')
plt.yticks(rotation=0, ha='right', size=15, fontweight='bold')
plt.yscale('symlog')
ax = plt.gca()
# Set linewidth and edge color for each spine
for spine in ax.spines.values():
    spine.set_linewidth(2)
    spine.set_edgecolor('black')
ax.xaxis.grid(False)  # Remove x-axis grid lines
ax.yaxis.grid(True)
plt.legend(loc='best', ncol=1, frameon=True, fontsize='15',
           fancybox=True, framealpha=1, shadow=True, borderpad=2)
# Save the plot to a directory in Google Drive
output_dir = '/content/gdrive/MyDrive/Energy_System_RL_Modelling_Plots'
plt.savefig(f'{output_dir}/Training_Mean_Episode_Unbalance_and_Operation_Cost_Loss_Data_SAC.png')
plt.tight_layout()
plt.show()

"""**Obtain the Test Data Results**"""

datasource = '/content/AgentSAC/test_data_SAC.pkl'
with open(datasource, 'rb') as tf:
    test_data = pickle.load(tf)
print(test_data.keys())
# Accessing data using keys
init_info_value = test_data['init_info']
information_value = test_data['information']
state_value = test_data['state']
action_value = test_data['action']
reward_value = test_data['reward']
cost_value = test_data['cost']
unbalance_value = test_data['unbalance']
record_output_value = test_data['record_output']
# Printing the values
print(f"Initial Information: {init_info_value}")
print(f"Information: {information_value}")
print(f"State: {state_value}")
print(f"Action: {action_value}")
print(f"Reward: {reward_value}")
print(f"Cost: {cost_value}")
print(f"Unbalance: {unbalance_value}")
print(f"Record Output: {record_output_value}")
eval_data_SAC = pd.DataFrame(test_data['information'])
eval_data_SAC.columns = ['time_step', 'price', 'netload', 'action', 'real_action',
                     'soc', 'battery1', 'GAS', 'COAL', 'BIOMASS', 'NUCLEAR', 'HYDRO',
                     'IMPORTS', 'STORAGE', 'OTHER', 'unbalance', 'operation_cost']
eval_data_SAC

"""**Plot Unbalance of Plant Generation For PPO, MIP-DQN and SAC**"""

sns.set_theme(style='whitegrid')
plt.rcParams["figure.figsize"] = (16, 9)
fig, axs = plt.subplots(1, 1)
plt.subplots_adjust(wspace=0.7, hspace=0.3)
plt.autoscale(tight=True)
axs.cla()  # Use axs, not axs[0, 0]
axs.set_ylabel('Unbalance of Generation')
fixed_date = datetime(2022, 1, 1)
period = [fixed_date + timedelta(hours=i) for i in range(24)]
axs.bar(period, eval_data_PPO['unbalance'],
        label='Exchange with Grid-PPO', width=0.02, color = 'sienna', align = 'center')
axs.bar(period, eval_data['unbalance'],
        label='Exchange with Grid-MIP-DQN', width=0.02, color = 'gold', align = 'edge')
axs.bar(period, eval_data_SAC['unbalance'],
        label='Exchange with Grid-SAC', width=0.02, color = 'green', align = 'center')
plt.title(f'Unbalance of Generation - PPO, MIP-DQN, and SAC {day}/{month}/2022',
          size=25, fontweight='bold')
plt.ylabel('MW', size=25, fontweight='bold')
plt.xlabel('Time (Hour)', size=25, fontweight='bold')
plt.xticks(rotation=30, size=15, fontweight='bold')
plt.yticks(rotation=0, ha='right', size=15, fontweight='bold')
ax = plt.gca()
# Set linewidth and edge color for each spine
for spine in ax.spines.values():
    spine.set_linewidth(2)
    spine.set_edgecolor('black')
ax.xaxis.grid(False)  # Remove x-axis grid lines
ax.yaxis.grid(True)
plt.legend(loc='best', ncol=1, frameon=True, fontsize='15',
           fancybox=True, framealpha=1, shadow=True, borderpad=2)
# Save the plot to a directory in Google Drive
output_dir = '/content/gdrive/MyDrive/Energy_System_RL_Modelling_Plots'
plt.savefig(f'{output_dir}/Unbalance_of_Generation_PPO_MIP-DQN_SAC.png')
plt.tight_layout()
plt.show()

"""**Plot Net Load For All Models (MIP-DQN, PPO, MINLP, & SAC)**"""

sns.set_theme(style='whitegrid')
plt.rcParams["figure.figsize"] = (16, 9)
fig, axs = plt.subplots(1, 1)
plt.subplots_adjust(wspace=0.7, hspace=0.3)
plt.autoscale(tight=True)
axs.cla()  # Use axs, not axs[0, 0]
fixed_date = datetime(2022, 1, 1)
period = [fixed_date + timedelta(hours=i) for i in range(24)]
axs.bar(period, eval_data_PPO['netload'], label = 'Netload - PPO', width = 0.01,
        color='orange', align='edge')
axs.bar(period, eval_data['netload'], label='Netload - MIP-DQN',
         color = 'crimson', width=0.01, align='center')
axs.bar(period, base_result['netload'], label='Netload - MINLP',
         color = 'navy', width=0.01, align='edge')
axs.bar(period, eval_data_SAC['netload'], label='Netload - SAC',
         color = 'purple', width=0.01, align='center')
axs.xaxis_date()
axs.xaxis.set_major_formatter(plt.matplotlib.dates.DateFormatter('%H:%M'))
plt.title(f'Comparison of Net Load for All Models {day}/{month}/2022',
          size=35, fontweight='bold')
plt.ylabel('Netload (MWh)', size=25, fontweight='bold')
plt.xlabel('Time (Hour)', size=25, fontweight='bold')
plt.xticks(rotation=30, size=15, fontweight='bold')
plt.yticks(rotation=0, ha='right', size=15, fontweight='bold')
ax = plt.gca()
# Set linewidth and edge color for each spine
for spine in ax.spines.values():
    spine.set_linewidth(2)
    spine.set_edgecolor('black')
ax.xaxis.grid(False)  # Remove x-axis grid lines
ax.yaxis.grid(True)
axs.set_yscale("log")
axs.legend(loc='best', ncol=1, frameon=True, fontsize='15', labelspacing = 0.5,
           fancybox=True, framealpha=1, shadow=True, borderpad=2)
# Save the plot to a directory in Google Drive
output_dir = '/content/gdrive/MyDrive/Energy_System_RL_Modelling_Plots'
plt.savefig(f'{output_dir}/Comparison_of_both_models_NetLoad_PPO_MIP-DQN_MINLP_SAC.png')
plt.tight_layout()
plt.show()

"""**Plot Operation Cost for All Models**"""

sns.set_theme(style='whitegrid')
plt.rcParams["figure.figsize"] = (16, 9)
fig, axs = plt.subplots(1, 1)
plt.subplots_adjust(wspace=0.7, hspace=0.3)
plt.autoscale(tight=True)
fixed_date = datetime(2022, 1, 1)
period = [fixed_date + timedelta(hours=i) for i in range(24)]
axs.cla()
axs.bar(period, eval_data['operation_cost'], label = 'Operation Cost - MIP-DQN',
        width = 0.02, color='lightblue', align='edge')
axs.bar(period, eval_data_PPO['operation_cost'], label = 'Operation Cost - PPO',
              width = 0.02, color='lightcoral', align='center')
axs.bar(period, base_result['step_cost'], label = 'Operation Cost - MINLP',
              width = 0.02, color='gray', align='edge')
axs.bar(period, eval_data_SAC['operation_cost'], label = 'Operation Cost - SAC',
              width = 0.02, color='chocolate', align='center')
axs.xaxis_date()
axs.xaxis.set_major_formatter(plt.matplotlib.dates.DateFormatter('%H:%M'))
plt.title(f'Operation Cost Comparison For All Models {day}/{month}/2022',
          size=25, fontweight='bold')
plt.ylabel('£/MWh', size=25, fontweight='bold')
plt.xlabel('Time (Hour)', size=25, fontweight='bold')
plt.xticks(rotation=30, size=15, fontweight='bold')
plt.yticks(rotation=0, ha='right', size=15, fontweight='bold')
ax = plt.gca()
# Set linewidth and edge color for each spine
for spine in ax.spines.values():
    spine.set_linewidth(2)
    spine.set_edgecolor('black')
ax.xaxis.grid(False)  # Remove x-axis grid lines
ax.yaxis.grid(True)
axs.legend(loc='best', ncol=1, frameon=True, fontsize='15', labelspacing = 0.5,
           fancybox=True, framealpha=1, shadow=True, borderpad=2)
# Save the plot to a directory in Google Drive
output_dir = '/content/gdrive/MyDrive/Energy_System_RL_Modelling_Plots'
plt.savefig(f'{output_dir}/Cost_MIP_DQN_PPO_MINLP_SAC.png')
plt.tight_layout()
plt.show()

"""**Plot the Energy Price For All Models**"""

sns.set_theme(style='whitegrid')
plt.rcParams["figure.figsize"] = (16, 9)
fig, axs = plt.subplots(1, 1)
plt.subplots_adjust(wspace=0.7, hspace=0.3)
plt.autoscale(tight=True)
axs.cla()  # Use axs, not axs[0, 0]
fixed_date = datetime(2022, 1, 1)
period = [fixed_date + timedelta(hours=i) for i in range(24)]
axs.plot(period, eval_data['price'], drawstyle='steps-mid', label = 'Price - MIP-DQN',
         color = 'purple', linewidth=7.0, marker='o')
axs.plot(period, eval_data_PPO['price'], drawstyle='steps-mid',
               label='Price - PPO',  color = 'navy', linewidth=7.0)
axs.plot(period, base_result['price'], drawstyle='steps-mid',
               label='Price - MINLP',  color = 'crimson', linewidth=7.0)
axs.plot(period, eval_data_SAC['price'], drawstyle='steps-mid',
               label='Price - SAC',  color = 'gold', linewidth=7.0)
axs.legend(loc='upper left', fontsize = 12, frameon = False, labelspacing = 0.5)
axs.xaxis_date()
axs.xaxis.set_major_formatter(plt.matplotlib.dates.DateFormatter('%H:%M'))
plt.title(f'Energy Price for All Models {day}/{month}/2022',
          size=25, fontweight='bold')
plt.ylabel('Price (£/MWh)', size=25, fontweight='bold')
plt.xlabel('Time (Hour)', size=25, fontweight='bold')
plt.xticks(rotation=30, size=15, fontweight='bold')
plt.yticks(rotation=0, ha='right', size=15, fontweight='bold')
ax = plt.gca()
# Set linewidth and edge color for each spine
for spine in ax.spines.values():
    spine.set_linewidth(2)
    spine.set_edgecolor('black')
ax.xaxis.grid(False)  # Remove x-axis grid lines
ax.yaxis.grid(True)
axs.legend(loc='best', ncol=1, frameon=True, fontsize='15', labelspacing = 0.5,
           fancybox=True, framealpha=1, shadow=True, borderpad=2)
# Save the plot to a directory in Google Drive
output_dir = '/content/gdrive/MyDrive/Energy_System_RL_Modelling_Plots'
plt.savefig(f'{output_dir}/Energy_Price_MIP-DQN_PPO_MINLP_SAC.png')
plt.tight_layout()
plt.show()

"""**State of Charge (SOC) for All Models**"""

sns.set_theme(style='whitegrid')
plt.rcParams["figure.figsize"] = (16, 9)
fig, axs = plt.subplots(1, 1)
plt.subplots_adjust(wspace=0.7, hspace=0.3)
plt.autoscale(tight=True)
axs.cla()  # Use axs, not axs[0, 0]
fixed_date = datetime(2022, 1, 1)
period = [fixed_date + timedelta(hours=i) for i in range(24)]
axs.plot(period, eval_data['soc'], drawstyle='steps-mid', label='SOC - MIP-DQN',
         color='green', linewidth=3.0)
axs.plot(period, eval_data_PPO['soc'], drawstyle='steps-mid', label='SOC - PPO',
         color='maroon', linewidth=3.0)
axs.plot(period, base_result['soc'], drawstyle='steps-mid', label='SOC - MINLP',
         color='blue', linewidth=3.0)
axs.plot(period, eval_data_SAC['soc'], drawstyle='steps-mid', label='SOC - SAC',
         color='skyblue', linewidth=3.0)
axs.legend(loc='upper left', fontsize = 12, frameon = False, labelspacing = 0.5)
axs.xaxis_date()
axs.xaxis.set_major_formatter(plt.matplotlib.dates.DateFormatter('%H:%M'))
plt.title(f'State of Charge (SOC) for All Models {day}/{month}/2022', size=25, fontweight='bold')
plt.xlabel('SOC', size=25, fontweight='bold')
plt.ylabel('Time (Hour)', size=25, fontweight='bold')
plt.xticks(rotation=30, size=15, fontweight='bold')
plt.yticks(rotation=0, ha='right', size=15, fontweight='bold')
ax = plt.gca()
# Set linewidth and edge color for each spine
for spine in ax.spines.values():
    spine.set_linewidth(2)
    spine.set_edgecolor('black')
ax.xaxis.grid(False)  # Remove x-axis grid lines
ax.yaxis.grid(True)
axs.legend(loc='best', ncol=1, frameon=True, fontsize='15', labelspacing = 0.5,
           fancybox=True, framealpha=1, shadow=True, borderpad=2)
# Save the plot to a directory in Google Drive
output_dir = '/content/gdrive/MyDrive/Energy_System_RL_Modelling_Plots'
plt.savefig(f'{output_dir}/SOC_MIP-DQN_PPO_MINLP_SAC.png')
plt.tight_layout()
plt.show()

"""**Compare the Different Cost Among All Models**"""

'''compare the different cost get from PPO, MINLP, MIP-DQN, and SAC'''
#ratio = sum(eval_data['operation_cost']) / sum(eval_data_PPO['operation_cost'])
MIP_DQN_Model_Op_Cost = sum(eval_data['operation_cost'])
PPO_Model_Op_Cost = sum(eval_data_PPO['operation_cost'])
SAC_Model_Op_Cost = sum(eval_data_SAC['operation_cost'])
MINLP_Model_Op_Cost = sum(base_result['step_cost'])
cheapest_model_cost, cheapest_model_name = min(
    (MIP_DQN_Model_Op_Cost, 'MIP-DQN'),
    (PPO_Model_Op_Cost, 'PPO'),
    (MINLP_Model_Op_Cost, 'MINLP'),
    (SAC_Model_Op_Cost, 'SAC'),
    key=lambda x: x[0]
)
print(f'Month is: {month}, Day is:{day}, Initial SOC is:{initial_soc:.3f}')
print(f'MIP-DQN Model Operation Cost is: {MIP_DQN_Model_Op_Cost:.3f}')
print(f'PPO Model Operation Cost is: {PPO_Model_Op_Cost:.3f}')
print(f'SAC Model Operation Cost is: {SAC_Model_Op_Cost:.3f}')
print(f'MINLP Model Operation Cost is: {MINLP_Model_Op_Cost:.3f}')
print(f'The Cheapest Model Operation Cost is: {cheapest_model_cost:.3f} (Model: {cheapest_model_name})')
#print(f'Ratio of MIP_DQN and PPO Model Operation Cost is: {ratio}')

"""#**Build the Deep Deterministic Policy Gradient (DDPG)**

**Build The Actor and Critic Modules for DDPG Model**
"""

class Actor(nn.Module):
    def __init__(self, mid_dim, state_dim, action_dim):
        super().__init__()
        self.net = nn.Sequential(nn.Linear(state_dim, mid_dim), nn.ReLU(),
                                 nn.Linear(mid_dim, mid_dim), nn.ReLU(),
                                 nn.Linear(mid_dim, mid_dim), nn.Hardswish(),
                                 nn.Linear(mid_dim, action_dim))
    def forward(self, state):
        return self.net(state).tanh()  # action.tanh()
    def get_action(self, state, action_std):
        action = self.net(state).tanh()
        noise = (torch.randn_like(action) * action_std).clamp(-0.5, 0.5)
        return (action + noise).clamp(-1.0, 1.0)
class Critic(nn.Module):
    def __init__(self, mid_dim, state_dim, action_dim):
        super().__init__()
        self.net = nn.Sequential(nn.Linear(state_dim + action_dim, mid_dim), nn.ReLU(),
                                 nn.Linear(mid_dim, mid_dim), nn.ReLU(),
                                 nn.Linear(mid_dim, mid_dim), nn.Hardswish(),
                                 nn.Linear(mid_dim, 1))
    def forward(self, state, action):
        return self.net(torch.cat((state, action), dim=1))  # q value

"""**Build the Agent's Base to Select Actions and Explore the Environment for DDPG**"""

class AgentDDPG(AgentBase):
    def __init__(self):
        super().__init__()
        self.explore_noise = 0.1  # explore noise of action
        self.if_use_cri_target = self.if_use_act_target = True
        self.ClassCri = Critic
        self.ClassAct = Actor
    def update_net(self, buffer, batch_size, repeat_times, soft_update_tau) -> (float, float):
        buffer.update_now_len()
        obj_critic = obj_actor = None
        for _ in range(int(buffer.now_len / batch_size * repeat_times)):
            obj_critic, state = self.get_obj_critic(buffer, batch_size)#critic loss
            self.optim_update(self.cri_optim, obj_critic)
            self.soft_update(self.cri_target, self.cri, soft_update_tau)
            action_pg = self.act(state)  # policy gradient
            obj_actor = -self.cri(state, action_pg).mean()# actor loss, makes it bigger
            self.optim_update(self.act_optim, obj_actor)
            self.soft_update(self.act_target, self.act, soft_update_tau)
        return obj_actor.item(), obj_critic.item()
    def get_obj_critic(self, buffer, batch_size) -> (torch.Tensor, torch.Tensor):
        with torch.no_grad():
            reward, mask, action, state, next_s = buffer.sample_batch(batch_size)
            next_q = self.cri_target(next_s, self.act_target(next_s))
            q_label = reward + mask * next_q
        q_value = self.cri(state, action)
        obj_critic = self.criterion(q_value, q_label)
        return obj_critic, state

"""**Build the Agent's DDPG Update Buffer**"""

def update_buffer(_trajectory):
    ten_state = torch.as_tensor([item[0] for item in _trajectory], dtype=torch.float32)
    ary_other = torch.as_tensor([item[1] for item in _trajectory])
    ary_other[:, 0] = ary_other[:, 0]   # ten_reward
    ary_other[:, 1] = (1.0 - ary_other[:, 1]) * gamma  # ten_mask = (1.0 - ary_done) * gamma
    buffer.extend_buffer(ten_state, ary_other)
    _steps = ten_state.shape[0]
    _r_exp = ary_other[:, 0].mean()  # other = (reward, mask, action)
    return _steps, _r_exp

"""**Test the DDPG Function**"""

if __name__ == '__main__':
    args = Arguments()
    '''here record real unbalance'''
    reward_record = {'episode': [], 'steps': [], 'mean_episode_reward': [], 'unbalance': [],
                     'episode_operation_cost': []}
    loss_record = {'episode': [], 'steps': [], 'critic_loss': [], 'actor_loss': [], 'entropy_loss': []}
    args.visible_gpu = '0'
    args.save_network=True
    args.test_network= True
    args.save_test_data=True
    args.compare_with_pyomo=True
    for seed in args.random_seed_list:
        args.random_seed = seed
        # set different seed
        args.agent = AgentDDPG()
        agent_name = f'{args.agent.__class__.__name__}'
        args.agent.cri_target = True
        args.env_RL = ESSEnv()
        args.init_before_training(if_main=True)
        '''init agent and environment'''
        agent = args.agent
        env_RL = args.env_RL
        agent.init(args.net_dim, env_RL.state_space.shape[0], env_RL.action_space.shape[0],
                   args.learning_rate, args._if_per_or_gae)
        '''init replay buffer'''
        buffer = ReplayBuffer(max_len=args.max_memo, state_dim=env_RL.state_space.shape[0],
                              action_dim= env_RL.action_space.shape[0])
        '''start training'''
        cwd = args.cwd
        gamma = args.gamma
        batch_size = args.batch_size  # how much data should be used to update net
        target_step = args.target_step  # how manysteps of one episode should stop
        repeat_times = args.repeat_times  # how many times should update for one batch size data
        soft_update_tau = args.soft_update_tau
        agent.state = env_RL.reset()
        '''collect data and train and update network'''
        num_episode = args.num_episode
        args.train=True
        args.save_network=True
        args.test_network= True
        args.save_test_data=True
        args.compare_with_pyomo=True
        if args.train:
            collect_data = True
            while collect_data:
                print(f'buffer:{buffer.now_len}')
                with torch.no_grad():
                    trajectory = agent.explore_env(env_RL, target_step)
                    steps, r_exp = update_buffer(trajectory)
                    buffer.update_now_len()
                if buffer.now_len >= 10000:
                    collect_data = False
            for i_episode in range(num_episode):
                critic_loss, actor_loss = agent.update_net(
                    buffer, batch_size, repeat_times, soft_update_tau)
                loss_record['critic_loss'].append(critic_loss)
                loss_record['actor_loss'].append(actor_loss)
                with torch.no_grad():
                    episode_reward, episode_unbalance, episode_operation_cost = get_episode_return(
                        env_RL, agent.act, agent.device)
                    reward_record['mean_episode_reward'].append(episode_reward)
                    reward_record['unbalance'].append(episode_unbalance)
                    reward_record['episode_operation_cost'].append(episode_operation_cost)
                print(f'current epsiode is {i_episode}, reward:{episode_reward}, unbalance:{episode_unbalance}, buffer_length: {buffer.now_len}')
                if i_episode % 10==0:
                # target_step
                    with torch.no_grad():
                        trajectory = agent.explore_env(env_RL, target_step)
                        steps, r_exp = update_buffer(trajectory)
    if args.update_training_data:
        loss_record_path = f'{args.cwd}/loss_data_DDPG.pkl'
        reward_record_path = f'{args.cwd}/reward_data_DDPG.pkl'
        with open(loss_record_path, 'wb') as tf:
            pickle.dump(loss_record, tf)
        with open(reward_record_path, 'wb') as tf:
            pickle.dump(reward_record, tf)
    act_save_path = f'{args.cwd}/actor_DDPG.pth'
    cri_save_path = f'{args.cwd}/critic_DDPG.pth'
    print('training data have been saved')
    if args.save_network:
        torch.save(agent.act.state_dict(), act_save_path)
        torch.save(agent.cri.state_dict(), cri_save_path)
        print('training finished and actor and critic parameters have been saved')
    if args.test_network:
        args.cwd = agent_name
        agent.act.load_state_dict(torch.load(act_save_path))
        print('parameters have been reloaded and testing')
        record = test_one_episode(env_RL, agent.act, agent.device)
        eval_data_DDPG = pd.DataFrame(record['information'])
        eval_data_DDPG.columns = ['time_step', 'price', 'netload', 'action', 'real_action',
                             'soc', 'battery1', 'GAS', 'COAL', 'BIOMASS', 'NUCLEAR', 'HYDRO',
                             'IMPORTS', 'STORAGE', 'OTHER', 'unbalance', 'operation_cost']
    if args.save_test_data:
        test_data_save_path = f'{args.cwd}/test_data_DDPG.pkl'
        with open(test_data_save_path, 'wb') as tf:
            pickle.dump(record, tf)
    '''compare with pyomo data and results'''
    if args.compare_with_pyomo:
        month = record['init_info'][0][0]
        day = record['init_info'][0][1]
        initial_soc = record['init_info'][0][3]
        print(initial_soc)

"""##**Results Analysis and Comparison For MINLP, MIP-DQN, PPO, SAC, and DDPG**

**Obtain the Training Loss Data**
"""

datasource = '/content/AgentDDPG/MIP_DQN_experiments/loss_data_DDPG.pkl'
with open(datasource, 'rb') as tf:
    loss_data = pickle.load(tf)
print(loss_data.keys())
# Accessing data using keys
episode_value = loss_data['episode']
steps_value = loss_data['steps']
critic_loss_value = loss_data['critic_loss']
actor_loss_value = loss_data['actor_loss']
entropy_loss_value = loss_data['entropy_loss']
# Printing the values
print(f"Episode: {episode_value}")
print(f"Steps: {steps_value}")
print(f"Critic Loss: {critic_loss_value}")
print(f"Actor Loss: {actor_loss_value}")
print(f"Entropy Loss: {entropy_loss_value}")

"""**Plot the Critic and Actor Networks Loss Data**"""

plt.figure(figsize=(15, 9))
plt.gca().set_facecolor("white")
plt.gca().spines[['top', 'right']].set_visible(False)
episode_values = list(range(len(critic_loss_value)))
pd.Series(critic_loss_value, index=episode_values).plot(
    kind='line', color='sienna', label='Critic Network Loss', linewidth=3.0)
pd.Series(actor_loss_value, index=episode_values).plot(
    kind='line', color='violet', label='Actor Network Loss', linewidth=3.0)
plt.title('DDPG Critic and Actor Networks Training Loss Data', size=25, fontweight='bold')
plt.xlabel('Episode', size=25, fontweight='bold')
plt.ylabel('Loss', size=25, fontweight='bold')
plt.xticks(rotation=30, size=15, fontweight='bold')
plt.yticks(rotation=0, ha='right', size=15, fontweight='bold')
ax = plt.gca()
# Set linewidth and edge color for each spine
for spine in ax.spines.values():
    spine.set_linewidth(2)
    spine.set_edgecolor('black')
ax.xaxis.grid(False)  # Remove x-axis grid lines
ax.yaxis.grid(True)
plt.legend(loc='best', ncol=1, frameon=True, fontsize='15',
           fancybox=True, framealpha=1, shadow=True, borderpad=2)
# Save the plot to a directory in Google Drive
output_dir = '/content/gdrive/MyDrive/Energy_System_RL_Modelling_Plots'
plt.savefig(f'{output_dir}/Critic_and_Actor_Networks_Training_Loss_Data_DDPG.png')
plt.tight_layout()
plt.show()

"""**Obtain the Training Reward Data**"""

datasource = '/content/AgentDDPG/MIP_DQN_experiments/reward_data_DDPG.pkl'
with open(datasource, 'rb') as tf:
    reward_data = pickle.load(tf)
print(reward_data.keys())
# Accessing data using keys
episode_value = reward_data['episode']
steps_value = reward_data['steps']
mean_episode_reward_value = reward_data['mean_episode_reward']
unbalance_value = reward_data['unbalance']
episode_operation_cost_value = reward_data['episode_operation_cost']
# Printing the values
print(f"Episode: {episode_value}")
print(f"Steps: {steps_value}")
print(f"Mean Episode Reward: {mean_episode_reward_value}")
print(f"unbalance: {unbalance_value}")
print(f"Episode Operation Cost: {episode_operation_cost_value}")

"""**Plot the Training Mean Episode, Unbalance, and Operation Cost Loss Data**"""

plt.figure(figsize=(15, 9))
plt.gca().set_facecolor("white")
plt.gca().spines[['top', 'right']].set_visible(False)
episode_values = list(range(len(mean_episode_reward_value)))
pd.Series(mean_episode_reward_value, index=episode_values).plot(
    kind='line', color='maroon', label='Mean Episode Reward Loss', linewidth=3.0)
pd.Series(unbalance_value, index=episode_values).plot(
    kind='line', color='navy', label='Unbalance Loss', linewidth=3.0)
pd.Series(episode_operation_cost_value, index=episode_values).plot(
    kind='line', color='green', label='Episode Operation Cost Loss', linewidth=3.0)
plt.title('DDPG Training Mean Episode, Unbalance, and Operation Cost Loss Data',
          size=25, fontweight='bold')
plt.xlabel('Episode', size=25, fontweight='bold')
plt.ylabel('Loss', size=25, fontweight='bold')
plt.xticks(rotation=30, size=15, fontweight='bold')
plt.yticks(rotation=0, ha='right', size=15, fontweight='bold')
plt.yscale('symlog')
ax = plt.gca()
# Set linewidth and edge color for each spine
for spine in ax.spines.values():
    spine.set_linewidth(2)
    spine.set_edgecolor('black')
ax.xaxis.grid(False)  # Remove x-axis grid lines
ax.yaxis.grid(True)
plt.legend(loc='best', ncol=1, frameon=True, fontsize='15',
           fancybox=True, framealpha=1, shadow=True, borderpad=2)
# Save the plot to a directory in Google Drive
output_dir = '/content/gdrive/MyDrive/Energy_System_RL_Modelling_Plots'
plt.savefig(f'{output_dir}/Training_Mean_Episode_Unbalance_and_Operation_Cost_Loss_Data_DDPG.png')
plt.tight_layout()
plt.show()

"""**Obtain the Test Data Results**"""

datasource = '/content/AgentDDPG/test_data_DDPG.pkl'
with open(datasource, 'rb') as tf:
    test_data = pickle.load(tf)
print(test_data.keys())
# Accessing data using keys
init_info_value = test_data['init_info']
information_value = test_data['information']
state_value = test_data['state']
action_value = test_data['action']
reward_value = test_data['reward']
cost_value = test_data['cost']
unbalance_value = test_data['unbalance']
record_output_value = test_data['record_output']
# Printing the values
print(f"Initial Information: {init_info_value}")
print(f"Information: {information_value}")
print(f"State: {state_value}")
print(f"Action: {action_value}")
print(f"Reward: {reward_value}")
print(f"Cost: {cost_value}")
print(f"Unbalance: {unbalance_value}")
print(f"Record Output: {record_output_value}")
eval_data_DDPG = pd.DataFrame(test_data['information'])
eval_data_DDPG.columns = ['time_step', 'price', 'netload', 'action', 'real_action',
                     'soc', 'battery1', 'GAS', 'COAL', 'BIOMASS', 'NUCLEAR', 'HYDRO',
                     'IMPORTS', 'STORAGE', 'OTHER', 'unbalance', 'operation_cost']
eval_data_DDPG

"""**Plot Unbalance of Plant Generation For PPO, MIP-DQN, SAC and DDPG**"""

sns.set_theme(style='whitegrid')
plt.rcParams["figure.figsize"] = (16, 9)
fig, axs = plt.subplots(1, 1)
plt.subplots_adjust(wspace=0.7, hspace=0.3)
plt.autoscale(tight=True)
axs.cla()  # Use axs, not axs[0, 0]
axs.set_ylabel('Unbalance of Generation')
fixed_date = datetime(2022, 1, 1)
period = [fixed_date + timedelta(hours=i) for i in range(24)]
axs.bar(period, eval_data_PPO['unbalance'],
        label='Exchange with Grid-PPO', width=0.02, color = 'sienna', align = 'center')
axs.bar(period, eval_data['unbalance'],
        label='Exchange with Grid-MIP-DQN', width=0.02, color = 'gold', align = 'edge')
axs.bar(period, eval_data_SAC['unbalance'],
        label='Exchange with Grid-SAC', width=0.02, color = 'green', align = 'center')
axs.bar(period, eval_data_DDPG['unbalance'],
        label='Exchange with Grid-DDPG', width=0.02, color = 'maroon', align = 'edge')
plt.title(f'Unbalance of Generation - PPO, MIP-DQN, SAC, and DDPG {day}/{month}/2022',
          size=25, fontweight='bold')
plt.ylabel('MW', size=25, fontweight='bold')
plt.xlabel('Time (Hour)', size=25, fontweight='bold')
plt.xticks(rotation=30, size=15, fontweight='bold')
plt.yticks(rotation=0, ha='right', size=15, fontweight='bold')
ax = plt.gca()
# Set linewidth and edge color for each spine
for spine in ax.spines.values():
    spine.set_linewidth(2)
    spine.set_edgecolor('black')
ax.xaxis.grid(False)  # Remove x-axis grid lines
ax.yaxis.grid(True)
plt.legend(loc='best', ncol=1, frameon=True, fontsize='15',
           fancybox=True, framealpha=1, shadow=True, borderpad=2)
# Save the plot to a directory in Google Drive
output_dir = '/content/gdrive/MyDrive/Energy_System_RL_Modelling_Plots'
plt.savefig(f'{output_dir}/Unbalance_of_Generation_PPO_MIP-DQN_SAC_DDPG.png')
plt.tight_layout()
plt.show()

"""**Plot Net Load For All Models (MIP-DQN, PPO, MINLP, SAC, and DDPG)**"""

sns.set_theme(style='whitegrid')
plt.rcParams["figure.figsize"] = (16, 9)
fig, axs = plt.subplots(1, 1)
plt.subplots_adjust(wspace=0.7, hspace=0.3)
plt.autoscale(tight=True)
axs.cla()  # Use axs, not axs[0, 0]
fixed_date = datetime(2022, 1, 1)
period = [fixed_date + timedelta(hours=i) for i in range(24)]
axs.bar(period, eval_data_PPO['netload'], label = 'Netload - PPO', width = 0.01,
        color='orange', align='edge')
axs.bar(period, eval_data['netload'], label='Netload - MIP-DQN',
         color = 'crimson', width=0.01, align='center')
axs.bar(period, base_result['netload'], label='Netload - MINLP',
         color = 'navy', width=0.01, align='edge')
axs.bar(period, eval_data_SAC['netload'], label='Netload - SAC',
         color = 'purple', width=0.01, align='center')
axs.bar(period, eval_data_DDPG['netload'], label='Netload - DDPG',
         color = 'violet', width=0.01, align='edge')
axs.xaxis_date()
axs.xaxis.set_major_formatter(plt.matplotlib.dates.DateFormatter('%H:%M'))
plt.title(f'Comparison of Net Load for All Models {day}/{month}/2022',
          size=35, fontweight='bold')
plt.ylabel('Netload (MWh)', size=25, fontweight='bold')
plt.xlabel('Time (Hour)', size=25, fontweight='bold')
plt.xticks(rotation=30, size=15, fontweight='bold')
plt.yticks(rotation=0, ha='right', size=15, fontweight='bold')
ax = plt.gca()
# Set linewidth and edge color for each spine
for spine in ax.spines.values():
    spine.set_linewidth(2)
    spine.set_edgecolor('black')
ax.xaxis.grid(False)  # Remove x-axis grid lines
ax.yaxis.grid(True)
axs.set_yscale("log")
axs.legend(loc='best', ncol=1, frameon=True, fontsize='15', labelspacing = 0.5,
           fancybox=True, framealpha=1, shadow=True, borderpad=2)
# Save the plot to a directory in Google Drive
output_dir = '/content/gdrive/MyDrive/Energy_System_RL_Modelling_Plots'
plt.savefig(f'{output_dir}/Comparison_of_both_models_NetLoad_PPO_MIP-DQN_MINLP_SAC_DDPG.png')
plt.tight_layout()
plt.show()

"""**Plot Operation Cost for All Models**"""

sns.set_theme(style='whitegrid')
plt.rcParams["figure.figsize"] = (16, 9)
fig, axs = plt.subplots(1, 1)
plt.subplots_adjust(wspace=0.7, hspace=0.3)
plt.autoscale(tight=True)
fixed_date = datetime(2022, 1, 1)
period = [fixed_date + timedelta(hours=i) for i in range(24)]
axs.cla()
axs.bar(period, eval_data['operation_cost'], label = 'Operation Cost - MIP-DQN',
        width = 0.02, color='lightblue', align='edge')
axs.bar(period, eval_data_PPO['operation_cost'], label = 'Operation Cost - PPO',
              width = 0.02, color='lightcoral', align='center')
axs.bar(period, base_result['step_cost'], label = 'Operation Cost - MINLP',
              width = 0.02, color='gray', align='edge')
axs.bar(period, eval_data_SAC['operation_cost'], label = 'Operation Cost - SAC',
              width = 0.02, color='chocolate', align='center')
axs.bar(period, eval_data_DDPG['operation_cost'], label = 'Operation Cost - DDPG',
              width = 0.02, color='cyan', align='edge')
axs.xaxis_date()
axs.xaxis.set_major_formatter(plt.matplotlib.dates.DateFormatter('%H:%M'))
plt.title(f'Operation Cost Comparison For All Models {day}/{month}/2022',
          size=25, fontweight='bold')
plt.ylabel('£/MWh', size=25, fontweight='bold')
plt.xlabel('Time (Hour)', size=25, fontweight='bold')
plt.xticks(rotation=30, size=15, fontweight='bold')
plt.yticks(rotation=0, ha='right', size=15, fontweight='bold')
ax = plt.gca()
# Set linewidth and edge color for each spine
for spine in ax.spines.values():
    spine.set_linewidth(2)
    spine.set_edgecolor('black')
ax.xaxis.grid(False)  # Remove x-axis grid lines
ax.yaxis.grid(True)
axs.legend(loc='best', ncol=1, frameon=True, fontsize='15', labelspacing = 0.5,
           fancybox=True, framealpha=1, shadow=True, borderpad=2)
# Save the plot to a directory in Google Drive
output_dir = '/content/gdrive/MyDrive/Energy_System_RL_Modelling_Plots'
plt.savefig(f'{output_dir}/Cost_MIP_DQN_PPO_MINLP_SAC_DDPG.png')
plt.tight_layout()
plt.show()

"""**Plot the Energy Price For All Models**"""

sns.set_theme(style='whitegrid')
plt.rcParams["figure.figsize"] = (16, 9)
fig, axs = plt.subplots(1, 1)
plt.subplots_adjust(wspace=0.7, hspace=0.3)
plt.autoscale(tight=True)
axs.cla()  # Use axs, not axs[0, 0]
fixed_date = datetime(2022, 1, 1)
period = [fixed_date + timedelta(hours=i) for i in range(24)]
axs.plot(period, eval_data['price'], drawstyle='steps-mid', label = 'Price - MIP-DQN',
         color = 'purple', linewidth=7.0, marker='o')
axs.plot(period, eval_data_PPO['price'], drawstyle='steps-mid',
               label='Price - PPO',  color = 'navy', linewidth=7.0)
axs.plot(period, base_result['price'], drawstyle='steps-mid',
               label='Price - MINLP',  color = 'crimson', linewidth=7.0)
axs.plot(period, eval_data_SAC['price'], drawstyle='steps-mid',
               label='Price - SAC',  color = 'gold', linewidth=7.0)
axs.plot(period, eval_data_DDPG['price'], drawstyle='steps-mid',
               label='Price - DDPG',  color = 'olive', linewidth=7.0)
axs.legend(loc='upper left', fontsize = 12, frameon = False, labelspacing = 0.5)
axs.xaxis_date()
axs.xaxis.set_major_formatter(plt.matplotlib.dates.DateFormatter('%H:%M'))
plt.title(f'Energy Price for All Models {day}/{month}/2022',
          size=25, fontweight='bold')
plt.ylabel('Price (£/MWh)', size=25, fontweight='bold')
plt.xlabel('Time (Hour)', size=25, fontweight='bold')
plt.xticks(rotation=30, size=15, fontweight='bold')
plt.yticks(rotation=0, ha='right', size=15, fontweight='bold')
ax = plt.gca()
# Set linewidth and edge color for each spine
for spine in ax.spines.values():
    spine.set_linewidth(2)
    spine.set_edgecolor('black')
ax.xaxis.grid(False)  # Remove x-axis grid lines
ax.yaxis.grid(True)
axs.legend(loc='best', ncol=1, frameon=True, fontsize='15', labelspacing = 0.5,
           fancybox=True, framealpha=1, shadow=True, borderpad=2)
# Save the plot to a directory in Google Drive
output_dir = '/content/gdrive/MyDrive/Energy_System_RL_Modelling_Plots'
plt.savefig(f'{output_dir}/Energy_Price_MIP-DQN_PPO_MINLP_SAC_DDPG.png')
plt.tight_layout()
plt.show()

"""**State of Charge (SOC) for All Models**"""

sns.set_theme(style='whitegrid')
plt.rcParams["figure.figsize"] = (16, 9)
fig, axs = plt.subplots(1, 1)
plt.subplots_adjust(wspace=0.7, hspace=0.3)
plt.autoscale(tight=True)
axs.cla()  # Use axs, not axs[0, 0]
fixed_date = datetime(2022, 1, 1)
period = [fixed_date + timedelta(hours=i) for i in range(24)]
axs.plot(period, eval_data['soc'], drawstyle='steps-mid', label='SOC - MIP-DQN',
         color='green', linewidth=3.0)
axs.plot(period, eval_data_PPO['soc'], drawstyle='steps-mid', label='SOC - PPO',
         color='maroon', linewidth=3.0)
axs.plot(period, base_result['soc'], drawstyle='steps-mid', label='SOC - MINLP',
         color='blue', linewidth=3.0)
axs.plot(period, eval_data_SAC['soc'], drawstyle='steps-mid', label='SOC - SAC',
         color='skyblue', linewidth=3.0)
axs.plot(period, eval_data_DDPG['soc'], drawstyle='steps-mid', label='SOC - DDPG',
         color='peru', linewidth=3.0)
axs.legend(loc='upper left', fontsize = 12, frameon = False, labelspacing = 0.5)
axs.xaxis_date()
axs.xaxis.set_major_formatter(plt.matplotlib.dates.DateFormatter('%H:%M'))
plt.title(f'State of Charge (SOC) for All Models {day}/{month}/2022', size=25, fontweight='bold')
plt.xlabel('SOC', size=25, fontweight='bold')
plt.ylabel('Time (Hour)', size=25, fontweight='bold')
plt.xticks(rotation=30, size=15, fontweight='bold')
plt.yticks(rotation=0, ha='right', size=15, fontweight='bold')
ax = plt.gca()
# Set linewidth and edge color for each spine
for spine in ax.spines.values():
    spine.set_linewidth(2)
    spine.set_edgecolor('black')
ax.xaxis.grid(False)  # Remove x-axis grid lines
ax.yaxis.grid(True)
axs.legend(loc='best', ncol=1, frameon=True, fontsize='15', labelspacing = 0.5,
           fancybox=True, framealpha=1, shadow=True, borderpad=2)
# Save the plot to a directory in Google Drive
output_dir = '/content/gdrive/MyDrive/Energy_System_RL_Modelling_Plots'
plt.savefig(f'{output_dir}/SOC_MIP-DQN_PPO_MINLP_SAC_DDPG.png')
plt.tight_layout()
plt.show()

"""**Compare the Different Cost Among All Models**"""

'''compare the different cost Obtained from PPO, MINLP, MIP-DQN, SAC, and DDPG'''
#ratio = sum(eval_data['operation_cost']) / sum(eval_data_PPO['operation_cost'])
MIP_DQN_Model_Op_Cost = sum(eval_data['operation_cost'])
PPO_Model_Op_Cost = sum(eval_data_PPO['operation_cost'])
SAC_Model_Op_Cost = sum(eval_data_SAC['operation_cost'])
DDPG_Model_Op_Cost = sum(eval_data_DDPG['operation_cost'])
MINLP_Model_Op_Cost = sum(base_result['step_cost'])
cheapest_model_cost, cheapest_model_name = min(
    (MIP_DQN_Model_Op_Cost, 'MIP-DQN'),
    (PPO_Model_Op_Cost, 'PPO'),
    (MINLP_Model_Op_Cost, 'MINLP'),
    (SAC_Model_Op_Cost, 'SAC'),
    (DDPG_Model_Op_Cost, 'DDPG'),
    key=lambda x: x[0]
)
print(f'Month is: {month}, Day is:{day}, Initial SOC is:{initial_soc:.3f}')
print(f'MIP-DQN Model Operation Cost is: {MIP_DQN_Model_Op_Cost:.3f}')
print(f'PPO Model Operation Cost is: {PPO_Model_Op_Cost:.3f}')
print(f'SAC Model Operation Cost is: {SAC_Model_Op_Cost:.3f}')
print(f'DDPG Model Operation Cost is: {DDPG_Model_Op_Cost:.3f}')
print(f'MINLP Model Operation Cost is: {MINLP_Model_Op_Cost:.3f}')
print(f'The Cheapest Model Operation Cost is: {cheapest_model_cost:.3f} (Model: {cheapest_model_name})')
#print(f'Ratio of MIP_DQN and PPO Model Operation Cost is: {ratio}')

"""#**Build the Twin Delayed Deep Deterministic (TD3)**

**Build The Actor and Critic Modules for TD3 Model**
"""

class Actor(nn.Module):
    def __init__(self, mid_dim, state_dim, action_dim):
        super().__init__()
        self.net = nn.Sequential(nn.Linear(state_dim, mid_dim), nn.ReLU(),
                                 nn.Linear(mid_dim, mid_dim), nn.ReLU(),
                                 nn.Linear(mid_dim, mid_dim), nn.Hardswish(),
                                 nn.Linear(mid_dim, action_dim))
    def forward(self, state):
        return self.net(state).tanh()  # action.tanh()
    def get_action(self, state, action_std):
        action = self.net(state).tanh()
        noise = (torch.randn_like(action) * action_std).clamp(-0.5, 0.5)
        return (action + noise).clamp(-1.0, 1.0)
class CriticTwin(nn.Module):  # shared parameter
    def __init__(self, mid_dim, state_dim, action_dim):
        super().__init__()
        self.net_sa = nn.Sequential(nn.Linear(state_dim + action_dim, mid_dim), nn.ReLU(),
                                    nn.Linear(mid_dim, mid_dim), nn.ReLU())  # concat(state, action)
        self.net_q1 = nn.Sequential(nn.Linear(mid_dim, mid_dim), nn.Hardswish(),
                                    nn.Linear(mid_dim, 1))  # q1 value
        self.net_q2 = nn.Sequential(nn.Linear(mid_dim, mid_dim), nn.Hardswish(),
                                    nn.Linear(mid_dim, 1))  # q2 value
    def forward(self, state, action):
        tmp = self.net_sa(torch.cat((state, action), dim=1))
        return self.net_q1(tmp)  # one Q value
    def get_q1_q2(self, state, action):
        tmp = self.net_sa(torch.cat((state, action), dim=1))
        return self.net_q1(tmp), self.net_q2(tmp)  # two Q values

"""**Build the Agent's Base to Select Actions and Explore the Environment for TD3**"""

class AgentTD3(AgentBase):
    def __init__(self):
        super().__init__()
        self.explore_noise = 0.1  # standard deviation of exploration noise
        self.policy_noise = 0.2  # standard deviation of policy noise
        self.update_freq = 2  # delay update frequency
        self.if_use_cri_target = self.if_use_act_target = True
        self.ClassCri = CriticTwin
        self.ClassAct = Actor
    def update_net(self, buffer, batch_size, repeat_times, soft_update_tau) -> tuple:
        buffer.update_now_len()
        obj_critic = obj_actor = None
        for update_c in range(int(buffer.now_len / batch_size * repeat_times)):
            obj_critic, state = self.get_obj_critic(buffer, batch_size)
            self.optim_update(self.cri_optim, obj_critic)
            action_pg = self.act(state)  # policy gradient
            obj_actor = -self.cri_target(state, action_pg).mean()  # use cri_target instead of cri for stable training
            self.optim_update(self.act_optim, obj_actor)
            if update_c % self.update_freq == 0:  # delay update
                self.soft_update(self.cri_target, self.cri, soft_update_tau)
                self.soft_update(self.act_target, self.act, soft_update_tau)
        return obj_critic.item() / 2, obj_actor.item()
    def get_obj_critic(self, buffer, batch_size) -> (torch.Tensor, torch.Tensor):
        with torch.no_grad():
            reward, mask, action, state, next_s = buffer.sample_batch(batch_size)
            next_a = self.act_target.get_action(next_s, self.policy_noise)  # policy noise
            next_q = torch.min(*self.cri_target.get_q1_q2(next_s, next_a))  # twin critics
            q_label = reward + mask * next_q
        q1, q2 = self.cri.get_q1_q2(state, action)
        obj_critic = self.criterion(q1, q_label) + self.criterion(q2, q_label)  # twin critics
        return obj_critic, state

"""**Build the Agent's TD3 Update Buffer**"""

def update_buffer(_trajectory):
    ten_state = torch.as_tensor([item[0] for item in _trajectory], dtype=torch.float32)
    ary_other = torch.as_tensor([item[1] for item in _trajectory])
    ary_other[:, 0] = ary_other[:, 0]   # ten_reward
    ary_other[:, 1] = (1.0 - ary_other[:, 1]) * gamma  # ten_mask = (1.0 - ary_done) * gamma
    buffer.extend_buffer(ten_state, ary_other)
    _steps = ten_state.shape[0]
    _r_exp = ary_other[:, 0].mean()  # other = (reward, mask, action)
    return _steps, _r_exp

"""**Test the TD3 Function**"""

if __name__ == '__main__':
    args = Arguments()
    '''here record real unbalance'''
    reward_record = {'episode': [], 'steps': [], 'mean_episode_reward': [], 'unbalance': [],
                     'episode_operation_cost': []}
    loss_record = {'episode': [], 'steps': [], 'critic_loss': [], 'actor_loss': [], 'entropy_loss': []}
    args.visible_gpu = '0'
    args.save_network=True
    args.test_network= True
    args.save_test_data=True
    args.compare_with_pyomo=True
    for seed in args.random_seed_list:
        args.random_seed = seed
        # set different seed
        args.agent = AgentTD3()
        agent_name = f'{args.agent.__class__.__name__}'
        args.agent.cri_target = True
        args.env_RL = ESSEnv()
        args.init_before_training(if_main=True)
        '''init agent and environment'''
        agent = args.agent
        env_RL = args.env_RL
        agent.init(args.net_dim, env_RL.state_space.shape[0], env_RL.action_space.shape[0],
                   args.learning_rate, args._if_per_or_gae)
        '''init replay buffer'''
        buffer = ReplayBuffer(max_len=args.max_memo, state_dim=env_RL.state_space.shape[0],
                              action_dim= env_RL.action_space.shape[0])
        '''start training'''
        cwd = args.cwd
        gamma = args.gamma
        batch_size = args.batch_size  # how much data should be used to update net
        target_step = args.target_step  # how manysteps of one episode should stop
        repeat_times = args.repeat_times  # how many times should update for one batch size data
        soft_update_tau = args.soft_update_tau
        agent.state = env_RL.reset()
        '''collect data and train and update network'''
        num_episode = args.num_episode
        args.train=True
        args.save_network=True
        args.test_network= True
        args.save_test_data=True
        args.compare_with_pyomo=True
        if args.train:
            collect_data = True
            while collect_data:
                print(f'buffer:{buffer.now_len}')
                with torch.no_grad():
                    trajectory = agent.explore_env(env_RL, target_step)
                    steps, r_exp = update_buffer(trajectory)
                    buffer.update_now_len()
                if buffer.now_len >= 10000:
                    collect_data = False
            for i_episode in range(num_episode):
                critic_loss, actor_loss = agent.update_net(
                    buffer, batch_size, repeat_times, soft_update_tau)
                loss_record['critic_loss'].append(critic_loss)
                loss_record['actor_loss'].append(actor_loss)
                with torch.no_grad():
                    episode_reward, episode_unbalance, episode_operation_cost = get_episode_return(
                        env_RL, agent.act, agent.device)
                    reward_record['mean_episode_reward'].append(episode_reward)
                    reward_record['unbalance'].append(episode_unbalance)
                    reward_record['episode_operation_cost'].append(episode_operation_cost)
                print(f'current epsiode is {i_episode}, reward:{episode_reward}, unbalance:{episode_unbalance}, buffer_length: {buffer.now_len}')
                if i_episode % 10==0:
                # target_step
                    with torch.no_grad():
                        trajectory = agent.explore_env(env_RL, target_step)
                        steps, r_exp = update_buffer(trajectory)
    if args.update_training_data:
        loss_record_path = f'{args.cwd}/loss_data_TD3.pkl'
        reward_record_path = f'{args.cwd}/reward_data_TD3.pkl'
        with open(loss_record_path, 'wb') as tf:
            pickle.dump(loss_record, tf)
        with open(reward_record_path, 'wb') as tf:
            pickle.dump(reward_record, tf)
    act_save_path = f'{args.cwd}/actor_TD3.pth'
    cri_save_path = f'{args.cwd}/critic_TD3.pth'
    print('training data have been saved')
    if args.save_network:
        torch.save(agent.act.state_dict(), act_save_path)
        torch.save(agent.cri.state_dict(), cri_save_path)
        print('training finished and actor and critic parameters have been saved')
    if args.test_network:
        args.cwd = agent_name
        agent.act.load_state_dict(torch.load(act_save_path))
        print('parameters have been reloaded and testing')
        record = test_one_episode(env_RL, agent.act, agent.device)
        eval_data_TD3 = pd.DataFrame(record['information'])
        eval_data_TD3.columns = ['time_step', 'price', 'netload', 'action', 'real_action',
                             'soc', 'battery1', 'GAS', 'COAL', 'BIOMASS', 'NUCLEAR', 'HYDRO',
                             'IMPORTS', 'STORAGE', 'OTHER', 'unbalance', 'operation_cost']
    if args.save_test_data:
        test_data_save_path = f'{args.cwd}/test_data_TD3.pkl'
        with open(test_data_save_path, 'wb') as tf:
            pickle.dump(record, tf)
    '''compare with pyomo data and results'''
    if args.compare_with_pyomo:
        month = record['init_info'][0][0]
        day = record['init_info'][0][1]
        initial_soc = record['init_info'][0][3]
        print(initial_soc)

"""##**Results Analysis and Comparison For MINLP, MIP-DQN, PPO, SAC, DDPG, and TD3**

**Obtain the Training Loss Data**
"""

datasource = '/content/AgentTD3/MIP_DQN_experiments/loss_data_TD3.pkl'
with open(datasource, 'rb') as tf:
    loss_data = pickle.load(tf)
print(loss_data.keys())
# Accessing data using keys
episode_value = loss_data['episode']
steps_value = loss_data['steps']
critic_loss_value = loss_data['critic_loss']
actor_loss_value = loss_data['actor_loss']
entropy_loss_value = loss_data['entropy_loss']
# Printing the values
print(f"Episode: {episode_value}")
print(f"Steps: {steps_value}")
print(f"Critic Loss: {critic_loss_value}")
print(f"Actor Loss: {actor_loss_value}")
print(f"Entropy Loss: {entropy_loss_value}")

"""**Plot the Critic and Actor Networks Loss Data**"""

plt.figure(figsize=(15, 9))
plt.gca().set_facecolor("white")
plt.gca().spines[['top', 'right']].set_visible(False)
episode_values = list(range(len(critic_loss_value)))
pd.Series(critic_loss_value, index=episode_values).plot(
    kind='line', color='sienna', label='Critic Network Loss', linewidth=3.0)
pd.Series(actor_loss_value, index=episode_values).plot(
    kind='line', color='violet', label='Actor Network Loss', linewidth=3.0)
plt.title('TD3 Critic and Actor Networks Training Loss Data', size=25, fontweight='bold')
plt.xlabel('Episode', size=25, fontweight='bold')
plt.ylabel('Loss', size=25, fontweight='bold')
plt.xticks(rotation=30, size=15, fontweight='bold')
plt.yticks(rotation=0, ha='right', size=15, fontweight='bold')
ax = plt.gca()
# Set linewidth and edge color for each spine
for spine in ax.spines.values():
    spine.set_linewidth(2)
    spine.set_edgecolor('black')
ax.xaxis.grid(False)  # Remove x-axis grid lines
ax.yaxis.grid(True)
plt.legend(loc='best', ncol=1, frameon=True, fontsize='15',
           fancybox=True, framealpha=1, shadow=True, borderpad=2)
# Save the plot to a directory in Google Drive
output_dir = '/content/gdrive/MyDrive/Energy_System_RL_Modelling_Plots'
plt.savefig(f'{output_dir}/Critic_and_Actor_Networks_Training_Loss_Data_DDPG_TD3.png')
plt.tight_layout()
plt.show()

"""**Obtain the Training Reward Data**"""

datasource = '/content/AgentTD3/MIP_DQN_experiments/reward_data_TD3.pkl'
with open(datasource, 'rb') as tf:
    reward_data = pickle.load(tf)
print(reward_data.keys())
# Accessing data using keys
episode_value = reward_data['episode']
steps_value = reward_data['steps']
mean_episode_reward_value = reward_data['mean_episode_reward']
unbalance_value = reward_data['unbalance']
episode_operation_cost_value = reward_data['episode_operation_cost']
# Printing the values
print(f"Episode: {episode_value}")
print(f"Steps: {steps_value}")
print(f"Mean Episode Reward: {mean_episode_reward_value}")
print(f"unbalance: {unbalance_value}")
print(f"Episode Operation Cost: {episode_operation_cost_value}")

"""**Plot the Training Mean Episode, Unbalance, and Operation Cost Loss Data**"""

plt.figure(figsize=(15, 9))
plt.gca().set_facecolor("white")
plt.gca().spines[['top', 'right']].set_visible(False)
episode_values = list(range(len(mean_episode_reward_value)))
pd.Series(mean_episode_reward_value, index=episode_values).plot(
    kind='line', color='maroon', label='Mean Episode Reward Loss', linewidth=3.0)
pd.Series(unbalance_value, index=episode_values).plot(
    kind='line', color='navy', label='Unbalance Loss', linewidth=3.0)
pd.Series(episode_operation_cost_value, index=episode_values).plot(
    kind='line', color='green', label='Episode Operation Cost Loss', linewidth=3.0)
plt.title('TD3 Training Mean Episode, Unbalance, and Operation Cost Loss Data',
          size=25, fontweight='bold')
plt.xlabel('Episode', size=25, fontweight='bold')
plt.ylabel('Loss', size=25, fontweight='bold')
plt.xticks(rotation=30, size=15, fontweight='bold')
plt.yticks(rotation=0, ha='right', size=15, fontweight='bold')
plt.yscale('symlog')
ax = plt.gca()
# Set linewidth and edge color for each spine
for spine in ax.spines.values():
    spine.set_linewidth(2)
    spine.set_edgecolor('black')
ax.xaxis.grid(False)  # Remove x-axis grid lines
ax.yaxis.grid(True)
plt.legend(loc='best', ncol=1, frameon=True, fontsize='15',
           fancybox=True, framealpha=1, shadow=True, borderpad=2)
# Save the plot to a directory in Google Drive
output_dir = '/content/gdrive/MyDrive/Energy_System_RL_Modelling_Plots'
plt.savefig(f'{output_dir}/Training_Mean_Episode_Unbalance_and_Operation_Cost_Loss_Data_DDPG_TD3.png')
plt.tight_layout()
plt.show()

"""**Obtain the Test Data Results**"""

datasource = '/content/AgentTD3/test_data_TD3.pkl'
with open(datasource, 'rb') as tf:
    test_data = pickle.load(tf)
print(test_data.keys())
# Accessing data using keys
init_info_value = test_data['init_info']
information_value = test_data['information']
state_value = test_data['state']
action_value = test_data['action']
reward_value = test_data['reward']
cost_value = test_data['cost']
unbalance_value = test_data['unbalance']
record_output_value = test_data['record_output']
# Printing the values
print(f"Initial Information: {init_info_value}")
print(f"Information: {information_value}")
print(f"State: {state_value}")
print(f"Action: {action_value}")
print(f"Reward: {reward_value}")
print(f"Cost: {cost_value}")
print(f"Unbalance: {unbalance_value}")
print(f"Record Output: {record_output_value}")
eval_data_TD3 = pd.DataFrame(test_data['information'])
eval_data_TD3.columns = ['time_step', 'price', 'netload', 'action', 'real_action',
                     'soc', 'battery1', 'GAS', 'COAL', 'BIOMASS', 'NUCLEAR', 'HYDRO',
                     'IMPORTS', 'STORAGE', 'OTHER', 'unbalance', 'operation_cost']
eval_data_TD3

"""**Plot Unbalance of Plant Generation For PPO, MIP-DQN, SAC, DDPG, and TD3**"""

sns.set_theme(style='whitegrid')
plt.rcParams["figure.figsize"] = (16, 9)
fig, axs = plt.subplots(1, 1)
plt.subplots_adjust(wspace=0.7, hspace=0.3)
plt.autoscale(tight=True)
axs.cla()  # Use axs, not axs[0, 0]
axs.set_ylabel('Unbalance of Generation')
fixed_date = datetime(2022, 1, 1)
period = [fixed_date + timedelta(hours=i) for i in range(24)]
axs.bar(period, eval_data_PPO['unbalance'],
        label='Exchange with Grid-PPO', width=0.02, color = 'sienna', align = 'center')
axs.bar(period, eval_data['unbalance'],
        label='Exchange with Grid-MIP-DQN', width=0.02, color = 'gold', align = 'edge')
axs.bar(period, eval_data_SAC['unbalance'],
        label='Exchange with Grid-SAC', width=0.02, color = 'green', align = 'center')
axs.bar(period, eval_data_DDPG['unbalance'],
        label='Exchange with Grid-DDPG', width=0.02, color = 'maroon', align = 'edge')
axs.bar(period, eval_data_TD3['unbalance'],
        label='Exchange with Grid-TD3', width=0.02, color = 'olive', align = 'center')
plt.title(f'Unbalance of Generation - PPO, MIP-DQN, SAC, DDPG, and TD3 For {day}/{month}/2022',
          size=25, fontweight='bold')
plt.ylabel('MW', size=25, fontweight='bold')
plt.xlabel('Time (Hour)', size=25, fontweight='bold')
plt.xticks(rotation=30, size=15, fontweight='bold')
plt.yticks(rotation=0, ha='right', size=15, fontweight='bold')
ax = plt.gca()
# Set linewidth and edge color for each spine
for spine in ax.spines.values():
    spine.set_linewidth(2)
    spine.set_edgecolor('black')
ax.xaxis.grid(False)  # Remove x-axis grid lines
ax.yaxis.grid(True)
plt.legend(loc='best', ncol=1, frameon=True, fontsize='15',
           fancybox=True, framealpha=1, shadow=True, borderpad=2)
# Save the plot to a directory in Google Drive
output_dir = '/content/gdrive/MyDrive/Energy_System_RL_Modelling_Plots'
plt.savefig(f'{output_dir}/Unbalance_of_Generation_PPO_MIP-DQN_SAC_DDPG_TD3.png')
plt.tight_layout()
plt.show()

"""**Plot Net Load For All Models (MIP-DQN, PPO, MINLP, SAC, DDPG, and TD3)**"""

sns.set_theme(style='whitegrid')
plt.rcParams["figure.figsize"] = (16, 9)
fig, axs = plt.subplots(1, 1)
plt.subplots_adjust(wspace=0.7, hspace=0.3)
plt.autoscale(tight=True)
axs.cla()  # Use axs, not axs[0, 0]
fixed_date = datetime(2022, 1, 1)
period = [fixed_date + timedelta(hours=i) for i in range(24)]
axs.bar(period, eval_data_PPO['netload'], label = 'Netload - PPO', width = 0.01,
        color='orange', align='edge')
axs.bar(period, eval_data['netload'], label='Netload - MIP-DQN',
         color = 'crimson', width=0.01, align='center')
axs.bar(period, base_result['netload'], label='Netload - MINLP',
         color = 'navy', width=0.01, align='edge')
axs.bar(period, eval_data_SAC['netload'], label='Netload - SAC',
         color = 'purple', width=0.01, align='center')
axs.bar(period, eval_data_DDPG['netload'], label='Netload - DDPG',
         color = 'violet', width=0.01, align='edge')
axs.bar(period, eval_data_TD3['netload'], label='Netload - TD3',
         color = 'goldenrod', width=0.01, align='center')
axs.xaxis_date()
axs.xaxis.set_major_formatter(plt.matplotlib.dates.DateFormatter('%H:%M'))
plt.title(f'Comparison of Net Load for All Models For {day}/{month}/2022',
          size=35, fontweight='bold')
plt.ylabel('Netload (MWh)', size=25, fontweight='bold')
plt.xlabel('Time (Hour)', size=25, fontweight='bold')
plt.xticks(rotation=30, size=15, fontweight='bold')
plt.yticks(rotation=0, ha='right', size=15, fontweight='bold')
ax = plt.gca()
# Set linewidth and edge color for each spine
for spine in ax.spines.values():
    spine.set_linewidth(2)
    spine.set_edgecolor('black')
ax.xaxis.grid(False)  # Remove x-axis grid lines
ax.yaxis.grid(True)
axs.set_yscale("log")
axs.legend(loc='best', ncol=1, frameon=True, fontsize='15', labelspacing = 0.5,
           fancybox=True, framealpha=1, shadow=True, borderpad=2)
# Save the plot to a directory in Google Drive
output_dir = '/content/gdrive/MyDrive/Energy_System_RL_Modelling_Plots'
plt.savefig(f'{output_dir}/Comparison_of_both_models_NetLoad_PPO_MIP-DQN_MINLP_SAC_DDPG_TD3.png')
plt.tight_layout()
plt.show()

"""**Plot Operation Cost for All Models**"""

sns.set_theme(style='whitegrid')
plt.rcParams["figure.figsize"] = (16, 9)
fig, axs = plt.subplots(1, 1)
plt.subplots_adjust(wspace=0.7, hspace=0.3)
plt.autoscale(tight=True)
fixed_date = datetime(2022, 1, 1)
period = [fixed_date + timedelta(hours=i) for i in range(24)]
axs.cla()
axs.bar(period, eval_data['operation_cost'], label = 'Operation Cost - MIP-DQN',
        width = 0.02, color='lightblue', align='edge')
axs.bar(period, eval_data_PPO['operation_cost'], label = 'Operation Cost - PPO',
              width = 0.02, color='lightcoral', align='center')
axs.bar(period, base_result['step_cost'], label = 'Operation Cost - MINLP',
              width = 0.02, color='gray', align='edge')
axs.bar(period, eval_data_SAC['operation_cost'], label = 'Operation Cost - SAC',
              width = 0.02, color='chocolate', align='center')
axs.bar(period, eval_data_DDPG['operation_cost'], label = 'Operation Cost - DDPG',
              width = 0.02, color='cyan', align='edge')
axs.bar(period, eval_data_TD3['operation_cost'], label = 'Operation Cost - TD3',
              width = 0.02, color='darkseagreen', align='center')
axs.xaxis_date()
axs.xaxis.set_major_formatter(plt.matplotlib.dates.DateFormatter('%H:%M'))
plt.title(f'Operation Cost Comparison For All Models For {day}/{month}/2022',
          size=25, fontweight='bold')
plt.ylabel('£/MWh', size=25, fontweight='bold')
plt.xlabel('Time (Hour)', size=25, fontweight='bold')
plt.xticks(rotation=30, size=15, fontweight='bold')
plt.yticks(rotation=0, ha='right', size=15, fontweight='bold')
ax = plt.gca()
# Set linewidth and edge color for each spine
for spine in ax.spines.values():
    spine.set_linewidth(2)
    spine.set_edgecolor('black')
ax.xaxis.grid(False)  # Remove x-axis grid lines
ax.yaxis.grid(True)
axs.legend(loc='best', ncol=1, frameon=True, fontsize='15', labelspacing = 0.5,
           fancybox=True, framealpha=1, shadow=True, borderpad=2)
# Save the plot to a directory in Google Drive
output_dir = '/content/gdrive/MyDrive/Energy_System_RL_Modelling_Plots'
plt.savefig(f'{output_dir}/Cost_MIP_DQN_PPO_MINLP_SAC_DDPG_TD3.png')
plt.tight_layout()
plt.show()

"""**Plot the Energy Price For All Models**"""

sns.set_theme(style='whitegrid')
plt.rcParams["figure.figsize"] = (16, 9)
fig, axs = plt.subplots(1, 1)
plt.subplots_adjust(wspace=0.7, hspace=0.3)
plt.autoscale(tight=True)
axs.cla()  # Use axs, not axs[0, 0]
fixed_date = datetime(2022, 1, 1)
period = [fixed_date + timedelta(hours=i) for i in range(24)]
axs.plot(period, eval_data['price'], drawstyle='steps-mid', label = 'Price - MIP-DQN',
         color = 'purple', linewidth=7.0, marker='o')
axs.plot(period, eval_data_PPO['price'], drawstyle='steps-mid',
               label='Price - PPO',  color = 'navy', linewidth=7.0)
axs.plot(period, base_result['price'], drawstyle='steps-mid',
               label='Price - MINLP',  color = 'crimson', linewidth=7.0)
axs.plot(period, eval_data_SAC['price'], drawstyle='steps-mid',
               label='Price - SAC',  color = 'gold', linewidth=7.0)
axs.plot(period, eval_data_DDPG['price'], drawstyle='steps-mid',
               label='Price - DDPG',  color = 'olive', linewidth=7.0)
axs.plot(period, eval_data_TD3['price'], drawstyle='steps-mid',
               label='Price - TD3',  color = 'plum', linewidth=7.0)
axs.legend(loc='upper left', fontsize = 12, frameon = False, labelspacing = 0.5)
axs.xaxis_date()
axs.xaxis.set_major_formatter(plt.matplotlib.dates.DateFormatter('%H:%M'))
plt.title(f'Energy Price for All Models For {day}/{month}/2022',
          size=25, fontweight='bold')
plt.ylabel('Price (£/MWh)', size=25, fontweight='bold')
plt.xlabel('Time (Hour)', size=25, fontweight='bold')
plt.xticks(rotation=30, size=15, fontweight='bold')
plt.yticks(rotation=0, ha='right', size=15, fontweight='bold')
ax = plt.gca()
# Set linewidth and edge color for each spine
for spine in ax.spines.values():
    spine.set_linewidth(2)
    spine.set_edgecolor('black')
ax.xaxis.grid(False)  # Remove x-axis grid lines
ax.yaxis.grid(True)
axs.legend(loc='best', ncol=1, frameon=True, fontsize='15', labelspacing = 0.5,
           fancybox=True, framealpha=1, shadow=True, borderpad=2)
# Save the plot to a directory in Google Drive
output_dir = '/content/gdrive/MyDrive/Energy_System_RL_Modelling_Plots'
plt.savefig(f'{output_dir}/Energy_Price_MIP-DQN_PPO_MINLP_SAC_DDPG_TD3.png')
plt.tight_layout()
plt.show()

"""**State of Charge (SOC) for All Models**"""

sns.set_theme(style='whitegrid')
plt.rcParams["figure.figsize"] = (16, 9)
fig, axs = plt.subplots(1, 1)
plt.subplots_adjust(wspace=0.7, hspace=0.3)
plt.autoscale(tight=True)
axs.cla()  # Use axs, not axs[0, 0]
fixed_date = datetime(2022, 1, 1)
period = [fixed_date + timedelta(hours=i) for i in range(24)]
axs.plot(period, eval_data['soc'], drawstyle='steps-mid', label='SOC - MIP-DQN',
         color='green', linewidth=3.0)
axs.plot(period, eval_data_PPO['soc'], drawstyle='steps-mid', label='SOC - PPO',
         color='maroon', linewidth=3.0)
axs.plot(period, base_result['soc'], drawstyle='steps-mid', label='SOC - MINLP',
         color='blue', linewidth=3.0)
axs.plot(period, eval_data_SAC['soc'], drawstyle='steps-mid', label='SOC - SAC',
         color='skyblue', linewidth=3.0)
axs.plot(period, eval_data_DDPG['soc'], drawstyle='steps-mid', label='SOC - DDPG',
         color='peru', linewidth=3.0)
axs.plot(period, eval_data_TD3['soc'], drawstyle='steps-mid', label='SOC - TD3',
         color='olive', linewidth=3.0)
axs.legend(loc='upper left', fontsize = 12, frameon = False, labelspacing = 0.5)
axs.xaxis_date()
axs.xaxis.set_major_formatter(plt.matplotlib.dates.DateFormatter('%H:%M'))
plt.title(f'State of Charge (SOC) for All Models - {day}/{month}/2022', size=25, fontweight='bold')
plt.xlabel('SOC', size=25, fontweight='bold')
plt.ylabel('Time (Hour)', size=25, fontweight='bold')
plt.xticks(rotation=30, size=15, fontweight='bold')
plt.yticks(rotation=0, ha='right', size=15, fontweight='bold')
ax = plt.gca()
# Set linewidth and edge color for each spine
for spine in ax.spines.values():
    spine.set_linewidth(2)
    spine.set_edgecolor('black')
ax.xaxis.grid(False)  # Remove x-axis grid lines
ax.yaxis.grid(True)
axs.legend(loc='best', ncol=1, frameon=True, fontsize='15', labelspacing = 0.5,
           fancybox=True, framealpha=1, shadow=True, borderpad=2)
# Save the plot to a directory in Google Drive
output_dir = '/content/gdrive/MyDrive/Energy_System_RL_Modelling_Plots'
plt.savefig(f'{output_dir}/SOC_MIP-DQN_PPO_MINLP_SAC_DDPG_TD3.png')
plt.tight_layout()
plt.show()

"""**Compare the Different Cost Among All Models**"""

'''compare the different cost Obtained from PPO, MINLP, MIP-DQN, SAC, DDPG, and TD3'''
#ratio = sum(eval_data['operation_cost']) / sum(eval_data_PPO['operation_cost'])
MIP_DQN_Model_Op_Cost = sum(eval_data['operation_cost'])
PPO_Model_Op_Cost = sum(eval_data_PPO['operation_cost'])
SAC_Model_Op_Cost = sum(eval_data_SAC['operation_cost'])
DDPG_Model_Op_Cost = sum(eval_data_DDPG['operation_cost'])
TD3_Model_Op_Cost = sum(eval_data_TD3['operation_cost'])
MINLP_Model_Op_Cost = sum(base_result['step_cost'])
cheapest_model_cost, cheapest_model_name = min(
    (MIP_DQN_Model_Op_Cost, 'MIP-DQN'),
    (PPO_Model_Op_Cost, 'PPO'),
    (MINLP_Model_Op_Cost, 'MINLP'),
    (SAC_Model_Op_Cost, 'SAC'),
    (DDPG_Model_Op_Cost, 'DDPG'),
    (TD3_Model_Op_Cost, 'TD3'),
    key=lambda x: x[0]
)
print(f'Month is: {month}, Day is:{day}, Initial SOC is:{initial_soc:.3f}')
print(f'MIP-DQN Model Operation Cost is: {MIP_DQN_Model_Op_Cost:.3f}')
print(f'PPO Model Operation Cost is: {PPO_Model_Op_Cost:.3f}')
print(f'SAC Model Operation Cost is: {SAC_Model_Op_Cost:.3f}')
print(f'DDPG Model Operation Cost is: {DDPG_Model_Op_Cost:.3f}')
print(f'TD3 Model Operation Cost is: {TD3_Model_Op_Cost:.3f}')
print(f'MINLP Model Operation Cost is: {MINLP_Model_Op_Cost:.3f}')
print(f'The Cheapest Model Operation Cost is: {cheapest_model_cost:.3f} (Model: {cheapest_model_name})')
#print(f'Ratio of MIP_DQN and PPO Model Operation Cost is: {ratio}')

"""#**Build the Deep Q Network (DQN)**

**Build The Actor and Critic Modules for DQN Model**
"""

class Actor(nn.Module):
    def __init__(self, mid_dim, state_dim, action_dim):
        super().__init__()
        self.net = nn.Sequential(nn.Linear(state_dim, mid_dim), nn.ReLU(),
                               nn.Linear(mid_dim, mid_dim), nn.ReLU(),
                               nn.Linear(mid_dim, mid_dim), nn.ReLU(),
                               nn.Linear(mid_dim, action_dim))
    def forward(self, state):
        return self.net(state).tanh()# make the data from -1 to 1
    def get_action(self, state, action_std):#
        action = self.net(state).tanh()
        noise = (torch.randn_like(action)*action_std).clamp(-0.5, 0.5)#
        return (action + noise).clamp(-1.0, 1.0)
class CriticQ(nn.Module):
    def __init__(self, mid_dim, state_dim, action_dim):
        super().__init__()
        self.net_head = nn.Sequential(nn.Linear(state_dim + action_dim, mid_dim), nn.ReLU(),
                                    nn.Linear(mid_dim, mid_dim), nn.ReLU())
        self.net_q1 = nn.Sequential(nn.Linear(mid_dim, mid_dim), nn.ReLU(),
                                  nn.Linear(mid_dim, 1))# we get q1 value
        self.net_q2 = nn.Sequential(nn.Linear(mid_dim, mid_dim), nn.ReLU(),
                                  nn.Linear(mid_dim, 1))# we get q2 value
    def forward(self, value):
        mid = self.net_head(value)
        return self.net_q1(mid)
    def get_q1_q2(self, value):
        mid = self.net_head(value)
        return self.net_q1(mid), self.net_q2(mid)

"""**Build the Agent's Base to Select Actions and Explore the Environment for DQN**"""

class AgentDQN(AgentBase):
    def __init__(self):
        super().__init__()
        self.explore_noise = 0.5  # standard deviation of exploration noise
        self.policy_noise = 0.2  # standard deviation of policy noise
        self.update_freq = 2  # delay update frequency
        self.if_use_cri_target = self.if_use_act_target = True
        self.ClassCri = CriticQ
        self.ClassAct = Actor
    def update_net(self, buffer, batch_size, repeat_times, soft_update_tau) -> tuple:
        buffer.update_now_len()
        obj_critic = obj_actor = None
        for update_c in range(int(buffer.now_len / batch_size * repeat_times)):# we update too much time?
            obj_critic, state = self.get_obj_critic(buffer, batch_size)
            self.optim_update(self.cri_optim, obj_critic)
            action_pg = self.act(state)  # policy gradient
            obj_actor = -self.cri_target(torch.cat(
                (state, action_pg), dim=-1)).mean()  # use cri_target instead of cri for stable training
            self.optim_update(self.act_optim, obj_actor)
            if update_c % self.update_freq == 0:  # delay update
                self.soft_update(self.cri_target, self.cri, soft_update_tau)
                self.soft_update(self.act_target, self.act, soft_update_tau)
        return obj_critic.item() / 2, obj_actor.item()
    def get_obj_critic(self, buffer, batch_size):
        with torch.no_grad():
            reward, mask, action, state, next_s = buffer.sample_batch(batch_size)
            next_a = self.act_target.get_action(next_s, self.policy_noise)  # policy noise,
            next_q = torch.min(*self.cri_target.get_q1_q2(
                torch.cat((next_s, next_a), dim=-1)))  # twin critics
            q_label = reward + mask * next_q
        q1, q2 = self.cri.get_q1_q2(torch.cat((state, action), dim=-1))
        obj_critic = self.criterion(q1, q_label) + self.criterion(q2, q_label)  # twin critics
        return obj_critic, state

"""**Build the Agent's DQN Update Buffer**"""

def update_buffer(_trajectory):
    ten_state = torch.as_tensor([item[0] for item in _trajectory],
                                dtype=torch.float32)
    ary_other = torch.as_tensor([item[1] for item in _trajectory])
    ary_other[:, 0] = ary_other[:, 0]   # ten_reward
    ary_other[:, 1] = (1.0 - ary_other[:, 1]) * gamma  # ten_mask = (1.0 - ary_done) * gamma
    buffer.extend_buffer(ten_state, ary_other)
    _steps = ten_state.shape[0]
    _r_exp = ary_other[:, 0].mean()  # other = (reward, mask, action)
    return _steps, _r_exp

"""**Test the DQN Function**"""

if __name__ == '__main__':
    args = Arguments()
    '''here record real unbalance'''
    reward_record = {'episode': [], 'steps': [], 'mean_episode_reward': [], 'unbalance': [],
                     'episode_operation_cost': []}
    loss_record = {'episode': [], 'steps': [], 'critic_loss': [], 'actor_loss': [], 'entropy_loss': []}
    args.visible_gpu = '0'
    args.save_network=True
    args.test_network= True
    args.save_test_data=True
    args.compare_with_pyomo=True
    for seed in args.random_seed_list:
        args.random_seed = seed
        # set different seed
        args.agent = AgentDQN()
        agent_name = f'{args.agent.__class__.__name__}'
        args.agent.cri_target = True
        args.env_RL = ESSEnv()
        args.init_before_training(if_main=True)
        '''init agent and environment'''
        agent = args.agent
        env_RL = args.env_RL
        agent.init(args.net_dim, env_RL.state_space.shape[0], env_RL.action_space.shape[0],
                   args.learning_rate, args._if_per_or_gae)
        '''init replay buffer'''
        buffer = ReplayBuffer(max_len=args.max_memo, state_dim=env_RL.state_space.shape[0],
                              action_dim=env_RL.action_space.shape[0])
        '''start training'''
        cwd = args.cwd
        gamma = args.gamma
        batch_size = args.batch_size  # how much data should be used to update net
        target_step = args.target_step  # how manysteps of one episode should stop
        repeat_times = args.repeat_times  # how many times should update for one batch size data
        soft_update_tau = args.soft_update_tau
        agent.state = env_RL.reset()
        '''collect data and train and update network'''
        num_episode = args.num_episode
        args.train=True
        args.save_network=True
        args.test_network= True
        args.save_test_data=True
        args.compare_with_pyomo=True
        if args.train:
            collect_data = True
            while collect_data:
                print(f'buffer:{buffer.now_len}')
                with torch.no_grad():
                    trajectory = agent.explore_env(env_RL, target_step)
                    steps, r_exp = update_buffer(trajectory)
                    buffer.update_now_len()
                if buffer.now_len >= 10000:
                    collect_data = False
            for i_episode in range(num_episode):
                critic_loss, actor_loss = agent.update_net(
                    buffer, batch_size, repeat_times, soft_update_tau)
                loss_record['critic_loss'].append(critic_loss)
                loss_record['actor_loss'].append(actor_loss)
                with torch.no_grad():
                    episode_reward, episode_unbalance, episode_operation_cost = get_episode_return(
                        env_RL, agent.act, agent.device)
                    reward_record['mean_episode_reward'].append(episode_reward)
                    reward_record['unbalance'].append(episode_unbalance)
                    reward_record['episode_operation_cost'].append(episode_operation_cost)
                print(
                    f'current episode is {i_episode}, reward:{episode_reward}, unbalance:{episode_unbalance}, buffer_length: {buffer.now_len}')
                if i_episode % 10 == 0:
                    # target_step
                    with torch.no_grad():
                        agent._update_exploration_rate(args.explorate_decay, args.explorate_min)
                        trajectory = agent.explore_env(env_RL, target_step)
                        steps, r_exp = update_buffer(trajectory)
    if args.update_training_data:
        loss_record_path = f'{args.cwd}/loss_data_DQN.pkl'
        reward_record_path = f'{args.cwd}/reward_data_DQN.pkl'
        with open(loss_record_path, 'wb') as tf:
            pickle.dump(loss_record, tf)
        with open(reward_record_path, 'wb') as tf:
            pickle.dump(reward_record, tf)
    act_save_path = f'{args.cwd}/actor_DQN.pth'
    cri_save_path = f'{args.cwd}/critic_DQN.pth'
    print('training data have been saved')
    if args.save_network:
        torch.save(agent.act.state_dict(), act_save_path)
        torch.save(agent.cri.state_dict(), cri_save_path)
        print('training finished and actor and critic parameters have been saved')
    if args.test_network:
        args.cwd = agent_name
        agent.act.load_state_dict(torch.load(act_save_path))
        print('parameters have been reloaded and testing')
        record = test_one_episode(env_RL, agent.act, agent.device)
        eval_data_DQN = pd.DataFrame(record['information'])
        eval_data_DQN.columns = ['time_step', 'price', 'netload', 'action', 'real_action',
                             'soc', 'battery1', 'GAS', 'COAL', 'BIOMASS', 'NUCLEAR', 'HYDRO',
                             'IMPORTS', 'STORAGE', 'OTHER', 'unbalance', 'operation_cost']
    if args.save_test_data:
        test_data_save_path = f'{args.cwd}/test_data_DQN.pkl'
        with open(test_data_save_path, 'wb') as tf:
            pickle.dump(record, tf)
    '''compare with pyomo data and results'''
    if args.compare_with_pyomo:
        month = record['init_info'][0][0]
        day = record['init_info'][0][1]
        initial_soc = record['init_info'][0][3]
        print(initial_soc)

"""##**Results Analysis and Comparison For MINLP, MIP-DQN, PPO, SAC, DDPG, TD3, and DQN**

**Obtain the Training Loss Data**
"""

datasource = '/content/AgentDQN/MIP_DQN_experiments/loss_data_DQN.pkl'
with open(datasource, 'rb') as tf:
    loss_data = pickle.load(tf)
print(loss_data.keys())
# Accessing data using keys
episode_value = loss_data['episode']
steps_value = loss_data['steps']
critic_loss_value = loss_data['critic_loss']
actor_loss_value = loss_data['actor_loss']
entropy_loss_value = loss_data['entropy_loss']
# Printing the values
print(f"Episode: {episode_value}")
print(f"Steps: {steps_value}")
print(f"Critic Loss: {critic_loss_value}")
print(f"Actor Loss: {actor_loss_value}")
print(f"Entropy Loss: {entropy_loss_value}")

"""**Plot the Critic and Actor Networks Loss Data**"""

plt.figure(figsize=(15, 9))
plt.gca().set_facecolor("white")
plt.gca().spines[['top', 'right']].set_visible(False)
episode_values = list(range(len(critic_loss_value)))
pd.Series(critic_loss_value, index=episode_values).plot(
    kind='line', color='sienna', label='Critic Network Loss', linewidth=3.0)
pd.Series(actor_loss_value, index=episode_values).plot(
    kind='line', color='violet', label='Actor Network Loss', linewidth=3.0)
plt.title('DQN Critic and Actor Networks Training Loss Data', size=25, fontweight='bold')
plt.xlabel('Episode', size=25, fontweight='bold')
plt.ylabel('Loss', size=25, fontweight='bold')
plt.xticks(rotation=30, size=15, fontweight='bold')
plt.yticks(rotation=0, ha='right', size=15, fontweight='bold')
ax = plt.gca()
# Set linewidth and edge color for each spine
for spine in ax.spines.values():
    spine.set_linewidth(2)
    spine.set_edgecolor('black')
ax.xaxis.grid(False)  # Remove x-axis grid lines
ax.yaxis.grid(True)
plt.legend(loc='best', ncol=1, frameon=True, fontsize='15',
           fancybox=True, framealpha=1, shadow=True, borderpad=2)
# Save the plot to a directory in Google Drive
output_dir = '/content/gdrive/MyDrive/Energy_System_RL_Modelling_Plots'
plt.savefig(f'{output_dir}/Critic_and_Actor_Networks_Training_Loss_Data_DDPG_TD3_DQN.png')
plt.tight_layout()
plt.show()

"""**Obtain the Training Reward Data**"""

datasource = '/content/AgentDQN/MIP_DQN_experiments/reward_data_DQN.pkl'
with open(datasource, 'rb') as tf:
    reward_data = pickle.load(tf)
print(reward_data.keys())
# Accessing data using keys
episode_value = reward_data['episode']
steps_value = reward_data['steps']
mean_episode_reward_value = reward_data['mean_episode_reward']
unbalance_value = reward_data['unbalance']
episode_operation_cost_value = reward_data['episode_operation_cost']
# Printing the values
print(f"Episode: {episode_value}")
print(f"Steps: {steps_value}")
print(f"Mean Episode Reward: {mean_episode_reward_value}")
print(f"unbalance: {unbalance_value}")
print(f"Episode Operation Cost: {episode_operation_cost_value}")

"""**Plot the Training Mean Episode, Unbalance, and Operation Cost Loss Data**"""

plt.figure(figsize=(15, 9))
plt.gca().set_facecolor("white")
plt.gca().spines[['top', 'right']].set_visible(False)
episode_values = list(range(len(mean_episode_reward_value)))
pd.Series(mean_episode_reward_value, index=episode_values).plot(
    kind='line', color='maroon', label='Mean Episode Reward Loss', linewidth=3.0)
pd.Series(unbalance_value, index=episode_values).plot(
    kind='line', color='navy', label='Unbalance Loss', linewidth=3.0)
pd.Series(episode_operation_cost_value, index=episode_values).plot(
    kind='line', color='green', label='Episode Operation Cost Loss', linewidth=3.0)
plt.title('DQN Training Mean Episode, Unbalance, and Operation Cost Loss Data',
          size=25, fontweight='bold')
plt.xlabel('Episode', size=25, fontweight='bold')
plt.ylabel('Loss', size=25, fontweight='bold')
plt.xticks(rotation=30, size=15, fontweight='bold')
plt.yticks(rotation=0, ha='right', size=15, fontweight='bold')
plt.yscale('symlog')
ax = plt.gca()
# Set linewidth and edge color for each spine
for spine in ax.spines.values():
    spine.set_linewidth(2)
    spine.set_edgecolor('black')
ax.xaxis.grid(False)  # Remove x-axis grid lines
ax.yaxis.grid(True)
plt.legend(loc='best', ncol=1, frameon=True, fontsize='15',
           fancybox=True, framealpha=1, shadow=True, borderpad=2)
# Save the plot to a directory in Google Drive
output_dir = '/content/gdrive/MyDrive/Energy_System_RL_Modelling_Plots'
plt.savefig(f'{output_dir}/Training_Mean_Episode_Unbalance_and_Operation_Cost_Loss_Data_DDPG_TD3_DQN.png')
plt.tight_layout()
plt.show()

"""**Obtain the Test Data Results**"""

datasource = '/content/AgentDQN/test_data_DQN.pkl'
with open(datasource, 'rb') as tf:
    test_data = pickle.load(tf)
print(test_data.keys())
# Accessing data using keys
init_info_value = test_data['init_info']
information_value = test_data['information']
state_value = test_data['state']
action_value = test_data['action']
reward_value = test_data['reward']
cost_value = test_data['cost']
unbalance_value = test_data['unbalance']
record_output_value = test_data['record_output']
# Printing the values
print(f"Initial Information: {init_info_value}")
print(f"Information: {information_value}")
print(f"State: {state_value}")
print(f"Action: {action_value}")
print(f"Reward: {reward_value}")
print(f"Cost: {cost_value}")
print(f"Unbalance: {unbalance_value}")
print(f"Record Output: {record_output_value}")
eval_data_DQN = pd.DataFrame(test_data['information'])
eval_data_DQN.columns = ['time_step', 'price', 'netload', 'action', 'real_action',
                     'soc', 'battery1', 'GAS', 'COAL', 'BIOMASS', 'NUCLEAR', 'HYDRO',
                     'IMPORTS', 'STORAGE', 'OTHER', 'unbalance', 'operation_cost']
eval_data_DQN

"""**Plot Unbalance of Plant Generation For PPO, MIP-DQN, SAC, DDPG, TD3, and TD3**"""

sns.set_theme(style='whitegrid')
plt.rcParams["figure.figsize"] = (16, 9)
fig, axs = plt.subplots(1, 1)
plt.subplots_adjust(wspace=0.7, hspace=0.3)
plt.autoscale(tight=True)
axs.cla()  # Use axs, not axs[0, 0]
axs.set_ylabel('Unbalance of Generation')
fixed_date = datetime(2022, 1, 1)
period = [fixed_date + timedelta(hours=i) for i in range(24)]
axs.bar(period, eval_data_PPO['unbalance'],
        label='Exchange with Grid-PPO', width=0.02, color = 'sienna', align = 'center')
axs.bar(period, eval_data['unbalance'],
        label='Exchange with Grid-MIP-DQN', width=0.02, color = 'gold', align = 'edge')
axs.bar(period, eval_data_SAC['unbalance'],
        label='Exchange with Grid-SAC', width=0.02, color = 'green', align = 'center')
axs.bar(period, eval_data_DDPG['unbalance'],
        label='Exchange with Grid-DDPG', width=0.02, color = 'maroon', align = 'edge')
axs.bar(period, eval_data_TD3['unbalance'],
        label='Exchange with Grid-TD3', width=0.02, color = 'olive', align = 'center')
axs.bar(period, eval_data_DQN['unbalance'],
        label='Exchange with Grid-DQN', width=0.02, color = 'plum', align = 'edge')
plt.title(f'Unbalance of Generation - PPO, MIP-DQN, SAC, DDPG, TD3, and DQN For {day}/{month}/2022',
          size=25, fontweight='bold')
plt.ylabel('MW', size=25, fontweight='bold')
plt.xlabel('Time (Hour)', size=25, fontweight='bold')
plt.xticks(rotation=30, size=15, fontweight='bold')
plt.yticks(rotation=0, ha='right', size=15, fontweight='bold')
ax = plt.gca()
# Set linewidth and edge color for each spine
for spine in ax.spines.values():
    spine.set_linewidth(2)
    spine.set_edgecolor('black')
ax.xaxis.grid(False)  # Remove x-axis grid lines
ax.yaxis.grid(True)
plt.legend(loc='best', ncol=1, frameon=True, fontsize='15',
           fancybox=True, framealpha=1, shadow=True, borderpad=2)
# Save the plot to a directory in Google Drive
output_dir = '/content/gdrive/MyDrive/Energy_System_RL_Modelling_Plots'
plt.savefig(f'{output_dir}/Unbalance_of_Generation_PPO_MIP-DQN_SAC_DDPG_TD3_DQN.png')
plt.tight_layout()
plt.show()

"""**Plot Net Load For All Models (MIP-DQN, PPO, MINLP, SAC, DDPG, TD3, and DQN)**"""

sns.set_theme(style='whitegrid')
plt.rcParams["figure.figsize"] = (16, 9)
fig, axs = plt.subplots(1, 1)
plt.subplots_adjust(wspace=0.7, hspace=0.3)
plt.autoscale(tight=True)
axs.cla()  # Use axs, not axs[0, 0]
fixed_date = datetime(2022, 1, 1)
period = [fixed_date + timedelta(hours=i) for i in range(24)]
axs.bar(period, eval_data_PPO['netload'], label = 'Netload - PPO', width = 0.01,
        color='orange', align='edge')
axs.bar(period, eval_data['netload'], label='Netload - MIP-DQN',
         color = 'crimson', width=0.01, align='center')
axs.bar(period, base_result['netload'], label='Netload - MINLP',
         color = 'navy', width=0.01, align='edge')
axs.bar(period, eval_data_SAC['netload'], label='Netload - SAC',
         color = 'purple', width=0.01, align='center')
axs.bar(period, eval_data_DDPG['netload'], label='Netload - DDPG',
         color = 'violet', width=0.01, align='edge')
axs.bar(period, eval_data_TD3['netload'], label='Netload - TD3',
         color = 'goldenrod', width=0.01, align='center')
axs.bar(period, eval_data_DQN['netload'], label='Netload - DQN',
         color = 'lime', width=0.01, align='edge')
axs.xaxis_date()
axs.xaxis.set_major_formatter(plt.matplotlib.dates.DateFormatter('%H:%M'))
plt.title(f'Comparison of Net Load for All Models For {day}/{month}/2022',
          size=35, fontweight='bold')
plt.ylabel('Netload (MWh)', size=25, fontweight='bold')
plt.xlabel('Time (Hour)', size=25, fontweight='bold')
plt.xticks(rotation=30, size=15, fontweight='bold')
plt.yticks(rotation=0, ha='right', size=15, fontweight='bold')
ax = plt.gca()
# Set linewidth and edge color for each spine
for spine in ax.spines.values():
    spine.set_linewidth(2)
    spine.set_edgecolor('black')
ax.xaxis.grid(False)  # Remove x-axis grid lines
ax.yaxis.grid(True)
axs.set_yscale("log")
axs.legend(loc='best', ncol=1, frameon=True, fontsize='15', labelspacing = 0.5,
           fancybox=True, framealpha=1, shadow=True, borderpad=2)
# Save the plot to a directory in Google Drive
output_dir = '/content/gdrive/MyDrive/Energy_System_RL_Modelling_Plots'
plt.savefig(f'{output_dir}/Comparison_of_both_models_NetLoad_PPO_MIP-DQN_MINLP_SAC_DDPG_TD3_DQN.png')
plt.tight_layout()
plt.show()

"""**Plot Operation Cost for All Models**"""

sns.set_theme(style='whitegrid')
plt.rcParams["figure.figsize"] = (16, 9)
fig, axs = plt.subplots(1, 1)
plt.subplots_adjust(wspace=0.7, hspace=0.3)
plt.autoscale(tight=True)
fixed_date = datetime(2022, 1, 1)
period = [fixed_date + timedelta(hours=i) for i in range(24)]
axs.cla()
axs.bar(period, eval_data['operation_cost'], label = 'Operation Cost - MIP-DQN',
        width = 0.02, color='lightblue', align='edge')
axs.bar(period, eval_data_PPO['operation_cost'], label = 'Operation Cost - PPO',
              width = 0.02, color='lightcoral', align='center')
axs.bar(period, base_result['step_cost'], label = 'Operation Cost - MINLP',
              width = 0.02, color='gray', align='edge')
axs.bar(period, eval_data_SAC['operation_cost'], label = 'Operation Cost - SAC',
              width = 0.02, color='chocolate', align='center')
axs.bar(period, eval_data_DDPG['operation_cost'], label = 'Operation Cost - DDPG',
              width = 0.02, color='cyan', align='edge')
axs.bar(period, eval_data_TD3['operation_cost'], label = 'Operation Cost - TD3',
              width = 0.02, color='darkseagreen', align='center')
axs.bar(period, eval_data_DQN['operation_cost'], label = 'Operation Cost - DQN',
              width = 0.02, color='yellowgreen', align='edge')
axs.xaxis_date()
axs.xaxis.set_major_formatter(plt.matplotlib.dates.DateFormatter('%H:%M'))
plt.title(f'Operation Cost Comparison For All Models For {day}/{month}/2022',
          size=25, fontweight='bold')
plt.ylabel('£/MWh', size=25, fontweight='bold')
plt.xlabel('Time (Hour)', size=25, fontweight='bold')
plt.xticks(rotation=30, size=15, fontweight='bold')
plt.yticks(rotation=0, ha='right', size=15, fontweight='bold')
ax = plt.gca()
# Set linewidth and edge color for each spine
for spine in ax.spines.values():
    spine.set_linewidth(2)
    spine.set_edgecolor('black')
ax.xaxis.grid(False)  # Remove x-axis grid lines
ax.yaxis.grid(True)
axs.legend(loc='best', ncol=1, frameon=True, fontsize='15', labelspacing = 0.5,
           fancybox=True, framealpha=1, shadow=True, borderpad=2)
# Save the plot to a directory in Google Drive
output_dir = '/content/gdrive/MyDrive/Energy_System_RL_Modelling_Plots'
plt.savefig(f'{output_dir}/Cost_MIP_DQN_PPO_MINLP_SAC_DDPG_TD3_DQN.png')
plt.tight_layout()
plt.show()

"""**Plot the Energy Price For All Models**"""

sns.set_theme(style='whitegrid')
plt.rcParams["figure.figsize"] = (16, 9)
fig, axs = plt.subplots(1, 1)
plt.subplots_adjust(wspace=0.7, hspace=0.3)
plt.autoscale(tight=True)
axs.cla()  # Use axs, not axs[0, 0]
fixed_date = datetime(2022, 1, 1)
period = [fixed_date + timedelta(hours=i) for i in range(24)]
axs.plot(period, eval_data['price'], drawstyle='steps-mid', label = 'Price - MIP-DQN',
         color = 'purple', linewidth=7.0, marker='o')
axs.plot(period, eval_data_PPO['price'], drawstyle='steps-mid',
               label='Price - PPO',  color = 'navy', linewidth=7.0)
axs.plot(period, base_result['price'], drawstyle='steps-mid',
               label='Price - MINLP',  color = 'crimson', linewidth=7.0)
axs.plot(period, eval_data_SAC['price'], drawstyle='steps-mid',
               label='Price - SAC',  color = 'gold', linewidth=7.0)
axs.plot(period, eval_data_DDPG['price'], drawstyle='steps-mid',
               label='Price - DDPG',  color = 'olive', linewidth=7.0)
axs.plot(period, eval_data_TD3['price'], drawstyle='steps-mid',
               label='Price - TD3',  color = 'plum', linewidth=7.0)
axs.plot(period, eval_data_DQN['price'], drawstyle='steps-mid',
               label='Price - DQN',  color = 'salmon', linewidth=7.0)
axs.legend(loc='upper left', fontsize = 12, frameon = False, labelspacing = 0.5)
axs.xaxis_date()
axs.xaxis.set_major_formatter(plt.matplotlib.dates.DateFormatter('%H:%M'))
plt.title(f'Energy Price for All Models For {day}/{month}/2022',
          size=25, fontweight='bold')
plt.ylabel('Price (£/MWh)', size=25, fontweight='bold')
plt.xlabel('Time (Hour)', size=25, fontweight='bold')
plt.xticks(rotation=30, size=15, fontweight='bold')
plt.yticks(rotation=0, ha='right', size=15, fontweight='bold')
ax = plt.gca()
# Set linewidth and edge color for each spine
for spine in ax.spines.values():
    spine.set_linewidth(2)
    spine.set_edgecolor('black')
ax.xaxis.grid(False)  # Remove x-axis grid lines
ax.yaxis.grid(True)
axs.legend(loc='best', ncol=1, frameon=True, fontsize='15', labelspacing = 0.5,
           fancybox=True, framealpha=1, shadow=True, borderpad=2)
# Save the plot to a directory in Google Drive
output_dir = '/content/gdrive/MyDrive/Energy_System_RL_Modelling_Plots'
plt.savefig(f'{output_dir}/Energy_Price_MIP-DQN_PPO_MINLP_SAC_DDPG_TD3_DQN.png')
plt.tight_layout()
plt.show()

"""**State of Charge (SOC) for All Models**"""

sns.set_theme(style='whitegrid')
plt.rcParams["figure.figsize"] = (16, 9)
fig, axs = plt.subplots(1, 1)
plt.subplots_adjust(wspace=0.7, hspace=0.3)
plt.autoscale(tight=True)
axs.cla()  # Use axs, not axs[0, 0]
fixed_date = datetime(2022, 1, 1)
period = [fixed_date + timedelta(hours=i) for i in range(24)]
axs.plot(period, eval_data['soc'], drawstyle='steps-mid', label='SOC - MIP-DQN',
         color='green', linewidth=3.0)
axs.plot(period, eval_data_PPO['soc'], drawstyle='steps-mid', label='SOC - PPO',
         color='maroon', linewidth=3.0)
axs.plot(period, base_result['soc'], drawstyle='steps-mid', label='SOC - MINLP',
         color='blue', linewidth=3.0)
axs.plot(period, eval_data_SAC['soc'], drawstyle='steps-mid', label='SOC - SAC',
         color='skyblue', linewidth=3.0)
axs.plot(period, eval_data_DDPG['soc'], drawstyle='steps-mid', label='SOC - DDPG',
         color='peru', linewidth=3.0)
axs.plot(period, eval_data_TD3['soc'], drawstyle='steps-mid', label='SOC - TD3',
         color='olive', linewidth=3.0)
axs.plot(period, eval_data_DQN['soc'], drawstyle='steps-mid', label='SOC - DQN',
         color='yellowgreen', linewidth=3.0)
axs.legend(loc='upper left', fontsize = 12, frameon = False, labelspacing = 0.5)
axs.xaxis_date()
axs.xaxis.set_major_formatter(plt.matplotlib.dates.DateFormatter('%H:%M'))
plt.title(f'State of Charge (SOC) for All Models - {day}/{month}/2022', size=25, fontweight='bold')
plt.xlabel('SOC', size=25, fontweight='bold')
plt.ylabel('Time (Hour)', size=25, fontweight='bold')
plt.xticks(rotation=30, size=15, fontweight='bold')
plt.yticks(rotation=0, ha='right', size=15, fontweight='bold')
ax = plt.gca()
# Set linewidth and edge color for each spine
for spine in ax.spines.values():
    spine.set_linewidth(2)
    spine.set_edgecolor('black')
ax.xaxis.grid(False)  # Remove x-axis grid lines
ax.yaxis.grid(True)
axs.legend(loc='best', ncol=1, frameon=True, fontsize='15', labelspacing = 0.5,
           fancybox=True, framealpha=1, shadow=True, borderpad=2)
# Save the plot to a directory in Google Drive
output_dir = '/content/gdrive/MyDrive/Energy_System_RL_Modelling_Plots'
plt.savefig(f'{output_dir}/SOC_MIP-DQN_PPO_MINLP_SAC_DDPG_TD3_DQN.png')
plt.tight_layout()
plt.show()

"""**Compare the Different Cost Among All Models**"""

'''compare the different cost Obtained from PPO, MINLP, MIP-DQN, SAC, DDPG, TD3, and DQN'''
#ratio = sum(eval_data['operation_cost']) / sum(eval_data_PPO['operation_cost'])
MIP_DQN_Model_Op_Cost = sum(eval_data['operation_cost'])
PPO_Model_Op_Cost = sum(eval_data_PPO['operation_cost'])
SAC_Model_Op_Cost = sum(eval_data_SAC['operation_cost'])
DDPG_Model_Op_Cost = sum(eval_data_DDPG['operation_cost'])
TD3_Model_Op_Cost = sum(eval_data_TD3['operation_cost'])
DQN_Model_Op_Cost = sum(eval_data_DQN['operation_cost'])
MINLP_Model_Op_Cost = sum(base_result['step_cost'])
cheapest_model_cost, cheapest_model_name = min(
    (MIP_DQN_Model_Op_Cost, 'MIP-DQN'),
    (PPO_Model_Op_Cost, 'PPO'),
    (MINLP_Model_Op_Cost, 'MINLP'),
    (SAC_Model_Op_Cost, 'SAC'),
    (DDPG_Model_Op_Cost, 'DDPG'),
    (TD3_Model_Op_Cost, 'TD3'),
    (DQN_Model_Op_Cost, 'DQN'),
    key=lambda x: x[0]
)
print('===================================================================')
print(f'Month is: {month}, Day is:{day}, Initial SOC is:{initial_soc:.3f}')
print('===================================================================')
print(f'MIP-DQN Model Operation Cost is: {MIP_DQN_Model_Op_Cost:.3f}')
print('===================================================================')
print(f'PPO Model Operation Cost is: {PPO_Model_Op_Cost:.3f}')
print('===================================================================')
print(f'SAC Model Operation Cost is: {SAC_Model_Op_Cost:.3f}')
print('===================================================================')
print(f'DDPG Model Operation Cost is: {DDPG_Model_Op_Cost:.3f}')
print('===================================================================')
print(f'TD3 Model Operation Cost is: {TD3_Model_Op_Cost:.3f}')
print('===================================================================')
print(f'DQN Model Operation Cost is: {DQN_Model_Op_Cost:.3f}')
print('===================================================================')
print(f'MINLP Model Operation Cost is: {MINLP_Model_Op_Cost:.3f}')
print('===================================================================')
print(f'The Cheapest Model Operation Cost is: {cheapest_model_cost:.3f} (Model: {cheapest_model_name})')
#print(f'Ratio of MIP_DQN and PPO Model Operation Cost is: {ratio}')
print('===================================================================')

def ordinal(n):
    suffix = 'th' if 11 <= n <= 13 else {1: 'st', 2: 'nd', 3: 'rd'}.get(n % 10, 'th')
    return f'{n}{suffix}'
models_costs = [
    ('MIP-DQN', MIP_DQN_Model_Op_Cost),
    ('PPO', PPO_Model_Op_Cost),
    ('MINLP', MINLP_Model_Op_Cost),
    ('SAC', SAC_Model_Op_Cost),
    ('DDPG', DDPG_Model_Op_Cost),
    ('TD3', TD3_Model_Op_Cost),
    ('DQN', DQN_Model_Op_Cost),
]
# Sort the models based on their operation costs in ascending order
sorted_models = sorted(models_costs, key=lambda x: x[1])
# Display the results
print('===================================================================')
for i, (model_name, model_cost) in enumerate(sorted_models):
    print(f'{model_name} Model Operation Cost is: {model_cost:.3f}')
    print('===================================================================')
    if i == 0:
        print(
            f'The Best Performing Model Operation Cost is: {model_cost:.3f} (Model: {model_name})')
    elif i == 1:
        print(
            f'The Second Best Model Operation Cost is: {model_cost:.3f} (Model: {model_name})')
    elif i == 2:
        print(
            f'The Third Best Model Operation Cost is: {model_cost:.3f} (Model: {model_name})')
    else:
        print(
            f'{ordinal(i + 1)} Best Model Operation Cost is: {model_cost:.3f} (Model: {model_name})')
print('===================================================================')

"""#**Proposed Variations**"""

